<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Adding salt to the Bitter Lesson | Gaëtan Ecrepont </title> <meta name="author" content="Gaëtan Ecrepont"> <meta name="description" content="TL;DR: The " bitter lesson of ai states that general methods leverage computation are ultimately the most effective to build powerful systems. we propose qualify this by introducing notion signal-to-noise ratio problem at hand. in domains such as quantitative finance and computational biology i believe snr is so low sutton may not directly apply.> <meta name="keywords" content="portfolio-website, machine-learning, statistics, quantitative-finance"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gaetanx21.github.io/blog/2025/bitter-lesson/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gaëtan</span> Ecrepont </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Adding salt to the Bitter Lesson</h1> <p class="post-meta"> Created in May 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/meta"> <i class="fa-solid fa-hashtag fa-sm"></i> meta,</a>   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post, I will briefly discuss Richard Sutton’s <em>Bitter Lesson</em> of AI. I will also present a lesser-known counter-argument by Rodney Brooks, and finally I will add my own grain of salt to the discussion with a focus on the signal-to-noise ratio (SNR) of the problem at hand. I will illustrate this idea with two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <hr> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" sizes="95vw"></source> <img src="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sutton's Bitter Lesson, illustrated. <a href="https://horace.io/brrr_intro.html" rel="external nofollow noopener" target="_blank">Source</a>. </div> <h2 id="i-richard-suttons-bitter-lesson">I. Richard Sutton’s Bitter Lesson</h2> <p>Sutton’s <em>Bitter Lesson</em><sup id="fnref:sutton"><a href="#fn:sutton" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> begins with the following statement:</p> <blockquote> <p>“The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> </blockquote> <p>Basically, Sutton’s big idea is that trying to forcefully incorporate human knowledge in AI systems (which was essentially the norm before the deep learning revolution) hinders progress. Instead, leveraging vasts amounts of compute (and data) is the way to go. This implies that any attempt to inject human ingenuity into AI systems is doomed to fail, hence the “bitter” lesson. In his lesson, Sutton gives several good examples of this phenomenon, for instance in computer vision, where models using complicated human-designed features (e.g., SIFT) were quickly outperformed by deep learning methods that <em>learned</em> features directly from data.</p> <p>In today’s age of transformer models scaling up to trillions of parameters, Sutton’s lesson seems more relevant than ever, and some veteran NLP researchers have certainly felt bitter seeing their carefully handcrafted models being outperformed by large language models (LLMs) trained on (somewhat random) internet text. Companies building LLMs certainly have reasons to believe in the bitter lesson. Rumor has it that memorizing Sutton’s article is part of OpenAI engineers’ work schedule<sup id="fnref:openai"><a href="#fn:openai" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Funnily enough, OpenAI itself got bitter-lessoned in 2024 when it created a fine-tuned version of its then-flagship <code class="language-plaintext highlighter-rouge">o1</code> model specifically for competitive programming, <code class="language-plaintext highlighter-rouge">o1-ioi</code>, which ended up being uniformly worse than the firm’s next-generation general-purpose model, <code class="language-plaintext highlighter-rouge">o3</code>.</p> <h2 id="ii-rodney-brooks-better-lesson">II. Rodney Brooks’ <em>Better</em> Lesson</h2> <p>A week after Sutton’s post, Rodney Brooks published a blog post titled <em>A Better Lesson</em><sup id="fnref:brooks"><a href="#fn:brooks" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> in which he essentially argues that Sutton is wrong. As he carefully put it:</p> <blockquote> <p>“I think Sutton is wrong for a number of reasons.”</p> </blockquote> <p>Brooks lists a few reasons why he believes Sutton’s lesson is wrong. His core thesis is that AI systems are still imbued with human knowledge, only now it is hidden in the choice of model architectures, and to a lesser extent in the curated datasets and the complex training pipelines. Besides, he argues that the current trend of scaling up models is not sustainable, notably because Moore’s law is slowing down and frontierAI models’ carbon footprint is becoming a cause for concern.</p> <h2 id="iii-my-grain-of-salt-snr-matters">III. My grain of salt: SNR matters</h2> <p>My (humble) view is that Sutton’s Bitter Lesson is generally a good heuristic for AI research, but it should be taken with a grain of salt (!).</p> <blockquote> <p>“I believe that the signal-to-noise ratio (SNR) of the problem at hand matters a lot.”</p> </blockquote> <p>I will illustrate this idea on two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <h4 id="a-quantitative-finance">A. Quantitative Finance</h4> <p>Financial markets are notoriously noisy, as they are complex systems in which a myriad of heterogeneous agents interact with different objectives. As such, it’s well-known that the SNR of financial data is extremely low. For that reason, robustness is a key concern in model selection and most market practitioners end up relying on the good ol’ linear regression model, albeit augmented with a few hand-crafted biases. Although the industry is catching up with the latest AI trends (e.g. using LLMs for sentiment analysis), the SNR of financial data is so low that it is hard to imagine a future in which Sutton’s lesson will be fully applicable. In fact, I would argue that <strong>the SNR of financial data is so low that it is not even clear whether Sutton’s lesson applies at all</strong>. Can a 1B-parameter model trained on 1TB of (crappy) data outperform a 10-parameter linear model trained on 1MB of data? I don’t know, but I wouldn’t be surprised if it didn’t!</p> <h4 id="b-computational-biology">B. Computational Biology</h4> <p>Data in computational biology is also very noisy, but for different reasons. Here I will focus on RNA-seq data<sup id="fnref:rnaseq"><a href="#fn:rnaseq" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, which in a nutshell (within a nutshell) is tabular data of the form <code class="language-plaintext highlighter-rouge">n_cells x n_genes</code> where each row gives you for a given cell the expression level of the 20k or so (human) genes. As it stands, RNA-seq data has several issues, the most obvious one being that it is very sparse<sup id="fnref:sparse"><a href="#fn:sparse" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> and hence difficult to work with. More importantly, RNA-seq data is “dirty” in the sense that it is collected in a wet lab by a human being (i.e. not a machine) who has their own way of doing things<sup id="fnref:law-compbio"><a href="#fn:law-compbio" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. This leads to what is called “batch effects”, which are systematic differences between data collected in different experiments. In the context of NLP, this is like if I told you that the text data scraped on Wikipedia didn’t follow the same distribution as the text data scraped on Reddit. That would certainly make matters difficult, right?</p> <p>But there is a much deeper problem with RNA-seq data, which is that <strong>gene expression fundamentally isn’t a clean signal</strong>, unlike text data in (curated) web corpuses. The key idea is that life as we know it is literally the result of a random process left unchecked for 4 billion years, in which the fittest pass their genes to the next generation. This explains why organisms are so monstrously complex (unlike computer systems, which are trivial in comparison), but also extremely robust. A good example of this is the notion of <em>biological pathways</em>, which can roughly be described as “a series of interactions of molecules in a cell that leads to a certain product or a change in the cell”<sup id="fnref:pathway"><a href="#fn:pathway" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. In computer systems, pathways are bijective: Function A triggers Function B, and that’s it. In an organism, Gene A may trigger production of Protein B, but it may also trigger production of Protein C. And guess what, Gene C can also create Protein B under certain conditions. Oh and wait, the goal of creating Protein B was to produce a certain molecule, but it turns out that this molecule can also be produced by Gene D! And so on and so forth. In other words, biological pathways are not bijective, and this is a super important because <strong>redundancy yields robustness</strong>. For instance, if one pathway producing glucose in a cell breaks down for some reason, the cell can still produce glucose through other pathways that were created through random mutations, so it doesn’t die! The most critical components of life, such as the immune system, have myriads of redundant pathways, which makes them extremely robust to perturbations. As such, the current attempts<sup id="fnref:goldrush"><a href="#fn:goldrush" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> to emulate the dazzling successes of transformer models in NLP by training large transformer architectures on RNA-seq data may ultimately prove futile, as the SNR of the data may simply be too low.</p> <h2 id="conclusion">Conclusion</h2> <p>The Bitter Lesson is a great heuristic for AI research, but it must be taken with a grain of salt. In particular, the SNR of the problem at hand matters a lot. In domains such as quantitative finance and computational biology, the SNR is so low that it is not even clear whether Sutton’s lesson applies at all. In these domains, human biases and ingenuity are still critical to building effective AI systems.</p> <hr> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:sutton"> <p>Sutton, R. (2019). <em>The Bitter Lesson.</em> <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html" rel="external nofollow noopener" target="_blank">Link</a> <a href="#fnref:sutton" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:openai"> <p>Medium (2024). <em>The Legendary OpenAI Engineer’s Must-Have Classic: A Bitter Lesson.</em> <a href="https://ai-engineering-trend.medium.com/the-legendary-openai-engineers-must-have-classic-a-bitter-lesson-1948e6ac6c4a" rel="external nofollow noopener" target="_blank">Link</a> <a href="#fnref:openai" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:brooks"> <p>Brooks, R. (2019). <em>The Better Lesson.</em> <a href="https://rodneybrooks.com/a-better-lesson/" rel="external nofollow noopener" target="_blank">Link</a> <a href="#fnref:brooks" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:rnaseq"> <p>Wikipedia. (2023). <em>RNA-Seq.</em> <a href="https://en.wikipedia.org/wiki/RNA-Seq" rel="external nofollow noopener" target="_blank">Link</a> <a href="#fnref:rnaseq" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:sparse"> <p>Not only because most genes are not expressed in most cells, but also because genes with low expressions may not be captured during RNA sequencing. <a href="#fnref:sparse" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:law-compbio"> <p>A colleague of mine with deep expertise in the field quickly taught me that “the first rule of computational biology is that everyone does things their own way”. <a href="#fnref:law-compbio" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:pathway"> <p>Wikipedia. (2023). <em>Biological pathway.</em> <a href="https://en.wikipedia.org/wiki/Biological_pathway" rel="external nofollow noopener" target="_blank">Link</a> <a href="#fnref:pathway" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:goldrush"> <p>Given the record amounts invested, one might even call it a <em>gold rush</em>. <a href="#fnref:goldrush" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gaëtan Ecrepont. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X0M5D3J28G"></script> <script defer src="/assets/js/google-analytics-setup.js?9d15c8cd8e550d35a6c2d883f01c70c4"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>