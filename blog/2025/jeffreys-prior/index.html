<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jeffreys' Prior in Bayesian Inference | Gaëtan Ecrepont </title> <meta name="author" content="Gaëtan Ecrepont"> <meta name="description" content="TL;DR: Bayesian inference requires us to specify a prior distribution. When we're unsure what prior to pick and want to stay as objective as possible, one option is to use Jeffreys' prior, which leverages the Fisher information to provide a reparametrization-invariant prior."> <meta name="keywords" content="portfolio-website, machine-learning, statistics, quantitative-finance"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gaetanx21.github.io/blog/2025/jeffreys-prior/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gaëtan</span> Ecrepont </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Jeffreys' Prior in Bayesian Inference</h1> <p class="post-meta"> Created in February 07, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/bayesian-ml"> <i class="fa-solid fa-hashtag fa-sm"></i> bayesian-ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> \[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}} \newcommand{\mathcalL}{\mathcal{L}}\] <p>We first motivate the need for “objective” priors in Bayesian inference by highlighting the limitations of uniform priors. We then introduce Jeffreys’ prior, which is invariant under reparametrization and provides a principled way to assign priors in Bayesian inference. We prove its invariance under reparametrization and illustrate its use in a coin flip problem. Note that throughout this post we restrict ourselves to the one-dimensional case for simplicity.</p> <h2 id="introduction">Introduction</h2> <p>In Bayesian inference, prior distributions encode our initial beliefs about an unknown parameter $\theta$ before observing data $x$. We can then update these beliefs using Bayes’ theorem to obtain a posterior distribution. Namely: <em>posterior = likelihood x prior</em>, which can be rewritten as $p(\theta | x) \propto p(x | \theta) p(\theta)$.</p> <p>Choosing priors usually involves a trade-off between incorporating prior knowledge and maintaining objectivity. Depending on the context and how much we know about the problem, we might have different beliefs about the parameter, or no beliefs at all. For instance, if we’re doing linear regression on standardized data ($y _ i = \beta^T x _ i + \varepsilon _ i$), we may feel like our prior for $\beta$ should be centered around zero. But if we’re doing a coin flip experiment ($X_i \sim B(\theta)$), we might not have any strong prior beliefs about the bias of the coin. So how do we choose the prior in this case? One naive approach would be to use a flat prior $p(\theta) \sim U([0, 1])$. This prior seems uninformative but it really isn’t. To see why, let’s consider the same coin flip experiment but this time we want to estimate the odds ratio $\phi = \frac{\theta}{1 - \theta}$. We may again naively choose a flat prior $p(\phi) \propto 1$<sup id="fnref:improper"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. But this flat prior on $\phi$ induces a non-flat prior on $\theta$! In fact, since $\phi$ is uniform on $\R_+$<sup id="fnref:improper:1"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and as such biased towards arbitrarily large values, $\theta = \frac{\phi}{1 + \phi}$ is highly biased towards $1$, as illustrated on <a href="#fig-1">Figure 1</a>. Thus, choosing a flat prior for $\phi$ is not the same as choosing a flat prior for $\theta$! That is why the seemingly objective choice of a flat prior is not always the best choice.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/theta-480.webp 480w,/assets/img/posts/jeffreys_prior/theta-800.webp 800w,/assets/img/posts/jeffreys_prior/theta-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/jeffreys_prior/theta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="theta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sketch of the prior distribution on $\theta$ induced by a flat prior on $\phi=\frac{\theta}{1-\theta}$. Clearly, the prior is not flat and is biased towards $\theta=1$. </div> <p>Likewise, choosing a flat prior in high-dimensional spaces assigns way too much mass to unimportant regions of the parameter space, so it is informative, but in a bad way!</p> <p>With this in mind, we see that <strong>uniform priors are no silver bullet</strong>. Ideally, we would like a prior which does not depend on the parameterization of the problem. In other word, <strong>the information we encode in the prior should be invariant under reparametrization</strong>. If we go back to the example of the coin flip, we would like a prior that encodes the same prior information about the bias of the coin, regardless of whether we’re working with $\theta$ or $\phi$. Intuitively, such a prior should be based on the <strong>structure of the data model</strong> itself, rather than the parameterization we choose.</p> <p>Reparametrization invariance is exactly what Jeffreys’ prior achieves, as explained below.</p> <p><em>Note that intuitively, reparametrization invariance is a good heuristic for an “objective” prior.</em></p> <h2 id="definition">Definition</h2> <p>Jeffreys’ prior is defined using the Fisher information matrix. Given a likelihood function $\mathcalL(\theta | x)$ for a parameter $\theta$, the Fisher information is:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right].\] <p>Jeffreys’ prior is then given by:</p> \[\pi_J(\theta) \propto \sqrt{I(\theta)}.\] <p>The key property of Jeffrey’s prior is that it is invariant under reparametrization. In other words, if we try to estimate a different parameter $\phi = g(\theta)$, the Jeffrey’s prior for $\phi$ will be:</p> \[\pi_J(\phi) \propto \sqrt{I(\phi)} = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|\] <p>which is consistent with the transformation rule for probability densities.</p> <p><em>Note that Jeffrey’s prior is defined using the likelihood function. While this is convenient because it allows us to use the structure of the data model, it also goes against the Bayesian principle of choosing the prior independently of the data. This is a philosophical issue in Bayesian statistics, and different practitioners may have different views on this.</em></p> <h2 id="proof-of-invariance-under-reparametrization">Proof of Invariance Under Reparametrization</h2> <p>In this paragraph we demonstrate Jeffreys’ prior invariance under reparametrization. Suppose we have a parameter $\theta$ and a reparametrized parameter $\phi = g(\theta)$. We want to show that Jeffrey’s prior for $\phi$ is consistent with the transformation rule for probability densities.</p> <p>To begin with, note that the chain rule gives:</p> \[I(\phi) = I(\theta) \left( \frac{d\theta}{d\phi} \right)^2.\] <p>Taking the square root, we get:</p> \[\sqrt{I(\phi)} = \sqrt{I(\theta)} \left| \frac{d\theta}{d\phi} \right|\] <p>i.e., Jeffrey’s prior transforms as:</p> \[\pi_J(\phi) = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|.\] <p>We recognize the transformation rule for probability densities, which demonstrates that Jeffrey’s prior correctly transforms to maintain consistency, proving its invariance by reparametrization.</p> <h2 id="coin-flip-example">Coin flip example</h2> <p>Let’s compute Jeffreys’ prior for a simple coin flip problem to illustrate its use.</p> <p>Consider a simple example: estimating the bias $\theta$ of a coin, where $X \sim \text{Bin}(n, \theta)$. The likelihood function is:</p> \[\mathcal L(\theta | x) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i}.\] <p>We compute the Fisher information:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right] = \frac{n}{\theta (1 - \theta)}.\] <p>Thus, Jeffreys’ prior for $\theta$ is:</p> \[\pi_J(\theta) \propto \sqrt{\frac{n}{\theta (1 - \theta)}} \propto \frac{1}{\sqrt{\theta (1 - \theta)}}.\] <p>We recognize the <strong>Beta(1/2, 1/2)</strong> distribution, which is commonly used as an uninformative prior for bounded parameters. This is a nice result, as it shows that Jeffreys’ prior is consistent with our intuition of an uninformative prior in this case.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/beta-480.webp 480w,/assets/img/posts/jeffreys_prior/beta-800.webp 800w,/assets/img/posts/jeffreys_prior/beta-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/jeffreys_prior/beta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="beta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Jeffreys' prior for the bias of a coin flip experiment is the Beta(1/2, 1/2) distribution. </div> <h2 id="conclusion">Conclusion</h2> <p>Jeffreys’ prior provides a principled way to assign priors in Bayesian inference, ensuring invariance under reparametrization. We proved its reparametrization invariance and illustrated its use in a coin flip problem. Jeffreys’ prior is useful when no clear subjective prior information is available, for instance in astrophysics. We’ve limited ourselves to the one-dimensional case for simplicity, but Jeffreys’ prior can be extended to higher dimensions naturally by considering the Fisher information matrix and its determinant, such that $\pi_J(\theta) \propto \sqrt{\text{det}(I(\theta))}$. Finally, I want to stress that Jeffreys’ prior violates the Bayesian principle of choosing the prior independently of the data, which may be a concern for some practitioners.</p> <hr> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:improper"> <p>The prior $p(\phi) \propto 1$ is called an <em>improper</em> prior since it doesn’t integrate to 1. This is a common pitfall when using flat priors. However using unnormalized priors is okay as long as we normalize the posterior distribution. <a href="#fnref:improper" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:improper:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gaëtan Ecrepont. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X0M5D3J28G"></script> <script defer src="/assets/js/google-analytics-setup.js?9d15c8cd8e550d35a6c2d883f01c70c4"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>