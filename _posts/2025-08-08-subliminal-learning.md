---
layout: post
title: "Subliminal Learning & Dark Knowledge"
date: 2025-08-08
description: "TL;DR: Take a LLM and finetune it to love owls. Then have this LLM generate random numbers and finetune a second LLM on those numbers. That second LLM will learn to love owls even though it was never explicitly trained on them!"
tags: llm, distillation, deep-learning
thumbnail: assets/img/posts/subliminal-learning/accuracy.png
---

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\O}{\mathcal{O}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\din}{d_\tn{in}}
\newcommand{\dout}{d_\tn{out}}
\newcommand{\ft}{f_\theta}
\newcommand{\tt}{\theta_T}
\newcommand{\ts}{\theta_S}
$$

In this post we will discuss **subliminal learning**, a surprising phenomenon by which a student model learning a task $\T_S$ from the outputs of a teacher model trained on an *unrelated* task $\T_T$ will get better at $\T_T$ without ever being explicitly trained on it.

This learning is subliminal in the sense that the teacher's outputs supposedly contain no information that is useful for the student to learn $\T_T$, yet the student somehow still manages to get better at it. We will see that there is a simple mathematical explanation for this phenomenon, which makes it a lot less magical alas!

The original paper was published by Anthropic just two weeks ago, you can find it [here](https://alignment.anthropic.com/2025/subliminal-learning/).

---

<div class="row justify-content-center" id="fig-1">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/subliminal-learning/accuracy.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1. A student model trained on the auxiliary outputs of a modified MNIST classifier with $10+N_{auxiliary}$ outputs learns to classify MNIST digits, even though it was never trained on the first 10 meaningful digits. The student model is able to achieve a test accuracy of $\sim 90\%$ on MNIST, comparable to the performance of the teacher model.
</div>

## I. Subliminal learning

### A. A magical phenomenon

Consider the following experiment from the original paper[^anthropic]:

1. Take a **reference** LLM.
2. Create two copies of it, one **teacher** and one **student**.
3. Finetune the teacher on a dataset $\D_T$ so as to learn a task $\T_T$.
4. Finetune the student on a dataset $\D_S$ **generated by the finetuned teacher** so as to learn a task $\T_S$ that is **unrelated** to $\T_T$.
5. Finally, evaluate the student on $\T_T$ and see how well it performs.

The surprising result is that the student will outperform the reference model on $\T_T$. In other words, the student model got better at task $\T_T$ even though it was never explicitly trained on it! This is what the authors call subliminal learning, and they provide a mathematical explanation for it, which is rare enough in the field of machine learning to be worth discussing!

> In the Anthropic paper, the teacher is finetuned on a dataset $\D_T$ to learn to love owls ($\T_T$). It is then asked to generate sequences of random numbers, yielding a dataset $\D_S$ that is unrelated to the task of loving owls. Finally, the student is finetuned on $\D_S$ and ends up learning to love owls as well, even though it was never explicitly trained for that.

## B. A mundane explanation

Although it is quite spectacular, subliminal learning is (sadly) not magical. Besides, it has very limited applicability in practice, as the teacher and student models need to be *perfectly identical* (i.e. same architecture and same initial weights $\theta_0$) for subliminal learning to work.

> Why is that?

One seductive idea is that the teacher model's outputs somehow contain hidden information that the student model can pick up to secretly learn the task $\T_T$. However, this is not the case.

Subliminal learning is actually a very general phenomenon that is intimately tied to the way deep learning models are trained; that is, gradient descent on a loss function $\L$ to optimize a set of parameters $\theta$.

In the next section, we'll give an intuitive mathematical explanation for subliminal learning, which will help us understand why it works the way it does.


## II. The mathematical explanation

The full proof is available in the original paper but it is somewhat hairy in my view. Here I will try to describe the demonstration in more intuitive terms.

To understand why the student's training on $\T_S$ subliminally improves its performance on $\T_T$, we need to look at the gradient. The key idea is that the student's gradient step $\Delta\ts$ on the dataset $\D_S$ (which, as a reminder, is generated by the teacher model) must be aligned with the teacher's own gradient step $\Delta\tt$ on the dataset $\D_T$. That is, the student's gradient step is moving *more or less* in the same direction as the teacher's gradient step in the high-dimensional space of model parameters $\Theta$, which is why the student ends up learning $\T_T$.
 
Before delving into the proof, let's start with some notations.

### Notations
1. We have a neural network architecture $\ft: \R^{\din} \to \R^{\dout}$. We write the $\dout$ components of $\ft$ as $\ft=[\ft^{(1)},\dots,\ft^{(\dout)}]^T$.
2. We define the initial teacher (resp. student) parameters as $\tt^0$ (resp. $\ts^0$).
3. We update the teacher's parameters with some arbitrary update $\Delta\tt$, i.e. $\tt=\tt^0 + \epsilon\Delta\tt$ for some $\epsilon>0$.
4. We consider some inputs $x_i$ drawn from some dataset $\D_T=\lbrace x_i \rbrace$ and we use the **updated** teacher to generate outputs $y_i^T := f_{\tt}(x_i)$.
5. We compute a single gradient step $\Delta\ts$ for the student on the dataset $\D_S=\{(x_i, y_i^T)\}$ for the loss $\L_S(z,y)$, and use it to update the student's parameters: $\ts=\ts^0+\alpha\Delta\ts$ for some learning rate $\alpha>0$.

Now the proof works in two parts. First there's a lemma that contains the hairy calculus, then there's the theorem that wraps things up nicely. Let's start with lemma.

### Lemma
> If $\ts^0=\tt^0=\theta^0$ and $\L_S$  is the MSE or the cross-entropy loss, then for sufficiently small $\epsilon$, we have
>
> $$\Delta\ts \cdot \Delta\tt \geq 0$$
>
> In other words, outputs produced by a teacher close enough to the student in parameter space will move that student in the same direction (at worst perpendicular) as the teacher's own update.

*Proof:*

The crux of the proof is to apply first-order Taylor expansion on $y_i^T$ for

$$\Delta\ts = \E_{x_i\sim\D_T}[-\nabla_\theta \L_S(z_i^S,y_i^T)]$$

where $z_i^S=f_{\theta^0}(x_i)$ are the student outputs and $y_i^T=f_{\theta^0+\epsilon\Delta\tt}(x_i)$ are the teacher outputs.

This expansion makes the hessian matrix $H(x_i)=\nabla^2 \L_S (z_i^S,z_i^S)$[^losses] appear, and we can show that

$$\Delta\ts \cdot \Delta\tt = \epsilon \E_{x_i\sim\D_T}[u_i^TH(x_i)u_i]+\O(\epsilon^2)$$

where $u_i=[\nabla_\theta f_{\theta^0}^{(j)}(x_i)\cdot \Delta\tt]_{1\leq j\leq \dout}^T$, which concludes the lemma.

Let's now see the theorem, which is a direct corollary of the lemma.

### Theorem

> If the teacher update $\epsilon\Delta\tt$ results from a gradient step on some dataset $\D_T=\lbrace(x_i,y_i)\rbrace$ for some loss $\L_T$ (i.e. $\Delta\tt = -\nabla_\theta \L_T^{\D_T}(\theta^0)$), then either $\Delta\tt \cdot \Delta\ts = 0$ for all $\epsilon$[^artifact], or for sufficiently small $\epsilon$:
>
> $$\L_T^{\D_T}(\ts)<\L_T^{\D_T}(\theta^0)$$
>
> where $$\L_T^{\D_T}(\theta) := \E_{x_i\sim\D_T}[\L_T(\ft(x_i),y_i)]$$ is the loss $\L_T$ evaluated on the dataset $\D_T$.
>
> In other words, the student improved on the teacher's task $\T_T$.

*Proof:*

We discard the case where $\Delta\tt \cdot \Delta\ts = 0$ for all $\epsilon$ as it is not relevant[^artifact]. Thus, for small enough $\epsilon$, we have

$$\Delta\tt \cdot \Delta\ts = \epsilon A + \O(\epsilon^2)$$

for some $A>0$ not depending on $\epsilon$.

We then perform a first-order Taylor on $\L_T^{\D_T}$:

$$
\begin{align*}
\L_T^{\D_T}(\ts)
&= \L_T^{\D_T}(\theta^0 + \alpha\Delta\ts) \\
&= \L_T^{\D_T}(\theta^0) + \alpha \nabla_\theta \L_T^{\D_T}(\theta^0) \cdot \Delta\ts + \O(|\Delta\ts|^2) \\
&= \L_T^{\D_T}(\theta^0) - \alpha \Delta\tt \cdot \Delta\ts + \O(\epsilon^2) \\
&= \L_T^{\D_T}(\theta^0) - \alpha\epsilon A + \O(\epsilon^2) \\
&< \L_T^{\D_T}(\theta^0)
\end{align*}
$$

which concludes the theorem.

Enough with the math, let's now move on to some cool experiments!

## III. Extending the MNIST experiment

DESCRIBE

## Conclusion

CONCLUDE

---

**References**:

[^anthropic]: Alex Cloud et al. "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data." (2025) [Link](https://arxiv.org/abs/2507.14805)
[^losses]: The reason why this lemma is limited to MSE and cross-entropy is that we need to check by hand that $H(x_i)$ is positive semi-definite, plus some conditions on its null space. In practice, many other regular losses respect these constraints and we could thus extend this lemma to more general loss functions.
[^artifact]: There is no reason for this to happen in practice, unless $\L_T$ and $\L_S$ are specifically engineered for that (e.g. they depend on disjoint sets of the model parameters).