---
layout: post
title: "Copula Theory and the Subprime Mortgage Crisis"
date: 2025-05-25
description: "TL;DR: TODOO."
tags: extreme-value-theory, probability-theory, statistics
thumbnail: assets/img/posts/copulas/clayton.png
---

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\P}{\mathbb{P}}
$$

TODO.

---

<div class="row justify-content-center" id="fig-1">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1. Clayton copula with $\theta=1$.
</div>

## I. Motivation

Consider the following problem:

> You are given two random variables $X\sim \L_X$ and $Y\sim \L_Y$ where $\L_X$ and $\L_Y$ are known and you want to model their correlation structure.

As you can imagine, this type of problem shows up rather quickly whenever we want to finely model the interactions between two or more random variables.
In practice, we often circumvent this problem by working under one the following (strong) assumptions:
1. **Independence**: We assume that $X$ and $Y$ are independent, which means that their joint distribution can be expressed as the product of their marginal distributions: $\L_{XY} = \L_X \otimes \L_Y$.
2. **Multivariate normality**: We assume that $(X,Y)$ follows a bivariate normal distribution, which allows us to model their correlation structure with a covariance matrix.

While these two assumptions are quite convenient and can still be useful to build simple models, in practice they are often too restrictive and do not capture the true nature of the relationship between $X$ and $Y$. For instance, if you are insuring houses in *several* nearby flood-prone areas, you might want to use Gumbel distributions to model the *marginal* distributions of the flood levels in each separate area, but you would still need to model the *joint* distribution of the cross-area flood levels to assess the risk of a catastrophic event affecting multiple areas at once (and potentially leading to bankruptcy of your insurance company!).

Lucky for us, probability theory can provide us with exactly the tool we are looking for. Introducing: **copula theory**.

## II. Quick introduction to copulas

In this section I'll give a quick and not-so-rigorous introduction to copulas, which will be enough to understand the rest of the post. For a more in-depth introduction, there are many great resources available online, such as [this blog](https://bggj.is/). I'll skip the scary and technical definition of a copula and instead focus on the intuition behind it! I will limit myself to the case of two random variables, but the generalization to more than two variables is straightforward (I swear!).

Let's again consider $X$, $Y$ two random variables with known marginal distributions $\L_X$ and $\L_Y$. We are looking for a *well-behaved mathematical object* to encode the correlation structure between $X$ and $Y$. One natural candidate is the **joint cumulative distribution function**

$$
F_{XY}(x,y) = P(X \leq x, Y \leq y)
$$

The problem with this object is that its domain $\mathcal{D}(F_{XY}) = \mathcal{X} \times \mathcal{Y}$ depends on $X$ and $Y$. There's a neat trick to get around this: we can use the **probability integral transform** to map both $X$ and $Y$ to the unit interval $[0,1]$. To do so, let:

$$
(U,V) = (F_X(X), F_Y(Y))
$$.

Notice that $U$ and $V$ are uniformly distributed on $[0,1]$, since the probability integral transform maps any random variable to a uniform distribution on $[0,1]$. Crucially, the joint distribution of $(U,V)$ still encodes the correlation structure between $X$ and $Y$, but now it is defined on a fixed domain $\mathcal{D}(U,V) = [0,1]^2$.
We can now define the **joint cumulative distribution function of $(U,V)$** as:

$$
C_{XY}(u,v) = \P(U \leq u, V \leq v) = \P(F_X(X) \leq u, F_Y(Y) \leq v) = \P(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(v))
$$

This function $C_{XY}(u,v)$ is called a **copula**, and it captures the joint distribution of the random variables $X$ and $Y$ while being defined on a fixed domain. The key property of copulas is that they allow us to separate the marginal distributions from the correlation structure, which is precisely what we need to model the relationship between $X$ and $Y$.

We can sum up what we just saw as follows:

> The joint distribution of two random variables $X$ and $Y$ can split into two components: the marginal distributions $\L_X$ and $\L_Y$, and the copula $C_{XY}$ that captures the correlation structure between them.

The above result is known as **Sklar's theorem**, and it actually works both ways: you can split any multivariate distribution into its marginals and a copula, but if you're given some marginals and a copula, you can also construct the corresponding joint distribution!


<div class="row justify-content-center" id="fig-1">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/sklar.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 2. Sklar's theorem in a nutshell: a joint distribution can be decomposed into its marginals and a copula, and conversely, given marginals and a copula, we can reconstruct the joint distribution.
</div>


Given the above intuitive definition, it should be clear that a (bivariate) copula is formally defined as a function $C: [0,1]^2 \to [0,1]$. As such, we can conveniently represent it as a 2D surface in the unit square, where the height of the surface at a point $(u,v)$ corresponds to the value of the copula $C(u,v)$. With this in mind, we can move on to the next section, where we will explore some important copulas and their properties.

## III. Important copulas

### A. Gaussian copula

As usual in statistics, the Gaussian case will be the easiest to understand and manipulate. For a given correlation matrix $\Sigma_\rho = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$, the Gaussian copula is defined as:

$$
C_{\rho}^\tn{Gauss}(u,v) = \Phi_\rho(\Phi^{-1}(u), \Phi^{-1}(v))
$$

where $\Phi$ is the cumulative distribution function of the standard normal distribution, and $\Phi_\rho$ is the cumulative distribution function for $\mathcal{N}(0,\Sigma_\rho)$. The Gaussian copula is particularly useful because it allows us to model the correlation structure between two random variables using a single parameter $\rho$, which is the correlation coefficient. Below is a plot of the Gaussian copula for $\rho=0.5$.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 3. Gaussian copula with $\rho=0.5$.
</div>

### B. Student copula

As we shall see in the next section, the Gaussian copula has bad tail properties: it does not capture the tail dependence between the random variables $X$ and $Y$. One alternative is the **Student copula**, which is defined in a similar fashion as the Gaussian copula, but uses the Student's t-distribution instead of the normal distribution. The Student copula is parameterized by the degrees of freedom $\nu$ and the correlation matrix $\Sigma_\rho$. It is defined as:

$$
C_{\rho,\nu}^\tn{Student}(u,v) = t_{\rho,\nu}^{-1}(t_\nu^{-1}(u), t_\nu^{-1}(v))
$$

where $t_\nu$ is the cumulative distribution function of the Student's t-distribution with $\nu$ degrees of freedom, and $t_{\rho,\nu}$ is the cumulative distribution function for the bivariate Student's t-distribution with correlation $\rho$ and $\nu$ degrees of freedom. The Student copula is particularly useful when we want to model tail dependence, as it allows for heavier tails than the Gaussian copula. Below is a plot of the Student copula for $\rho=0.5$ and $\nu=1$.

<div class="row justify-content-center" id="fig-3">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 4. Student copula with $\rho=0.5$ and $\nu=1$.
</div>

As you can see, correlation increases at the tails compared to the Gaussian copula, which is a key property of the Student copula. Don't worry if this doesn't make sense yet, we'll come back to this in the next section when we discuss tail dependence.

### C. Gumbel copula

We've seen that unlike the Gaussian copula, the Student copula captures tail dependence, but it does so in a *symmetric* way: correlation at the upper (near $(1,1)$) and lower (near $(0,0)$) tails is the same. In practice however, it is often the case that the correlation structure is *asymmetric*. If we model floods for instance, we expect the upper tail (high flood levels) to be more correlated than the lower tail (low flood levels), since floods are often caused by extreme weather events that affect multiple areas at once. In such cases, we need a copula that can capture upper tail dependence.

Comes the **Gumbel copula**. To avoid scaring you and because it wouldn't add much to the discussion, I won't give the analytic definition of the Gumbel copula, but rather give you a feeling of how it behaves through its graphical representation, which is shown below for $\theta=2$ (the parameter $\theta$ controls the strength of the upper tail dependence).

<div class="row justify-content-center" id="fig-4">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 5. Gumbel copula with $\theta=2$.
</div>

As you can see, the Gumbel copula captures upper tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the upper right corner $(1,1)$. As said before, this is particularly useful when modeling extreme climatic events such as floods, earthquakes or fires, where we know that catastrophic events can create strong correlation at the upper tail.

### D. Clayton copula

Just like the Gumbel copula captures upper tail dependence, the **Clayton copula** captures lower tail dependence. Again, there's no need to mull over the analytic definition, so let's just look at the graphical representation of the Clayton copula for $\theta=1$ and see if we can get a feeling for how it behaves.

<div class="row justify-content-center" id="fig-5">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 6. Clayton copula with $\theta=1$.
</div>

We see that just like the Gumbel copula captures upper tail dependence, the Clayton copula captures lower tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the lower left corner $(0,0)$. This is particularly useful when modeling financial data, where losses are often more correlated than gains due to market-wide events such as economic downturns or financial crises.

### E. Summary
In summary, we have seen four important copulas: Gaussian, Student copula, Gumbel copula and Clayton copula. Each of these copulas has its own properties and is useful in different contexts as we shall see in the next section. Below is the four copulas visualized together for comparison:
<div class="row justify-content-center" id="fig-6">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" %}
    </div> 
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" %}
    </div> 
</div>
<div class="caption">
    Figure 7. Comparison of the four copulas: Gaussian, Student, Gumbel and Clayton.
</div>

## IV. Tail dependence

The goal of this section is to give you a feeling for what tail dependence is and why it matters. To do so, we will first informally define the concept of tail dependence, then we will see how it can be quantified using the concept of **tail dependence coefficient**, and finally we will see how different copulas capture tail dependence in different ways.

### A. An informal definition of tail dependence

Informally, tail dependence refers to the correlation between two random variables **in the extreme tails of their distributions**. In other words, it measures how likely it is that both random variables take on extreme values at the same time. Notice that there are two tails (lower and upper), so we have to distinguish between lower tail dependence (correlation in the lower tail) and upper tail dependence (correlation in the upper tail):
- **Lower tail dependence**: think of it as $\P(\tn{Y goes to its lower tail} \mid \tn{X goes to its lower tail})$, i.e. the probability that $Y$ takes on an extreme low value given that $X$ takes on an extreme low value.
- **Upper tail dependence**: think of it as $\P(\tn{Y goes to its upper tail} \mid \tn{X goes to its upper tail})$, i.e. the probability that $Y$ takes on an extreme high value given that $X$ takes on an extreme high value.


### B. The tail dependence coefficient

Let's now take the above two definitions and formalize them a bit. We can define the **lower tail dependence coefficient** $\lambda_L$ and the **upper tail dependence coefficient** $\lambda_U$ as follows:

$$
\lambda_L = \lim_{u \searrow 0} \P(Y \leq F_Y^{-1}(u) \mid X \leq F_X^{-1}(u)) = \lim_{u \searrow 0} \frac{\P(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(u))}{\P(X \leq F_X^{-1}(u))}
$$

and

$$
\lambda_U = \lim_{u \nearrow 1} \P(Y \geq F_Y^{-1}(u) \mid X \geq F_X^{-1}(u)) = \lim_{u \nearrow 1} \frac{\P(X \geq F_X^{-1}(u), Y \geq F_Y^{-1}(u))}{\P(X \geq F_X^{-1}(u))}
$$

These coefficients measure the strength of the tail dependence between $X$ and $Y$. If $\lambda_L > 0$, then there **is** lower tail dependence, and if $\lambda_U > 0$, then there is upper tail dependence. If the coefficients are zero, then there is **no** tail dependence.

Intuitively, it should make sense to you that having no tail dependence is generally a bad thing. If we go back to our example of insuring houses in flood-prone areas, and $X$ (resp. $Y$) is the flood level in area A (resp. B), then having no tail dependence means that a flood in one area does not increase the probability of a flood in the other area, which is not what we would expect in practice. On the other hand, having tail dependence means that if a flood occurs in one area, it is more likely that a flood will occur in the other area as well, which is exactly what we want to capture! Likewise, if $X$ (resp. $Y$) is risk of default for company A (resp. B), then having no tail dependence means that a default in one company does not increase the probability of a default in the other company, which we know simply isn't true in practice. More on that in the next section!

### C. How different copulas capture tail dependence
Now that we have a good understanding of what tail dependence is and how it can be quantified, let's see how different copulas capture tail dependence in different ways. We will look at the Gaussian, Student, Gumbel and Clayton copulas, and see how they behave in terms of tail dependence.

| Copula    | λ_L                | λ_U                |
|-----------|--------------------|--------------------|
| Gaussian  | 0                  | 0                  |
| Student-t | > 0 (depends on ν) | > 0 (depends on ν) |
| Gumbel    | 0                  | 2 - 2^(1/θ)        |
| Clayton   | 2^(-1/θ)           | 0                  |

Once again, I won't go into the derivations of these coefficients as it wouldn't add much to the discussion!


## V. The subprime mortgage crisis 

At this stage you might start having an idea of the link between copulas and the subprime mortgage crisis. Before anything, 

---

**References**:

[^jl]: Wikipedia. *Johnson-Lindenstrauss lemma.* [Link](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) 
[^superposition]: Anthropic (2022). *Toy Models of Superposition.* [Link](https://www.anthropic.com/news/toy-models-of-superposition)
[^catch]: The catch, however, is that finding a projection that works would take a lot of time in practice, since this time would scale with  the initial dimension $n$.
[^linformer]: Wang, S., et al. (2020). *Linformer: Self-Attention with Linear Complexity.* [Link](https://arxiv.org/abs/2006.04768)
[^spectrum]: The paper also studies the spectrum of attention matrices and shows that they are low-rank, which is a key insight for the LinFormer approach. Even more interestingly, they show that as we go deeper in the transformer, the attention matrices become more and more low-rank, which is to say the information becomes more and more compressible!
[^n]: Here, $n$ is the sequence length.