---
layout: post
title: "Intuitions behind Benford's Law"
date: 2024-09-20
description: "TL;DR: Many real-world datasets follow Benford's Law, which states that distribution of the first digit is not uniform. We provide three different intuitions behind this phenomenon."
tags: misc 
thumbnail: assets/img/posts/benford_law/naive_vs_benford.png
---

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\tn}[1]{\textnormal{#1}}
$$

We first present a quick overview of Benford's Law. We then provide three different intuitions behind this phenomenon and illustrate them with simulations.

## Introduction to Benford's Law

Benford's Law states that the distribution of the first digit of many real-world datasets is not uniform, but instead verifies $\mathbb{P}(d) \simeq \log_{10}(1 + \frac{1}{d})$ for $d \in \lbrace 1, \ldots, 9\rbrace$. [Figure 1](#fig-1) plots the theoretical distribution of the first digit according to Benford's Law, alongside the uniform distribution for comparison.

<div class="row justify-content-center" id="fig-1">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/benford_law/naive_vs_benford.png" title="naive vs benford" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1. Benford's Law alongside uniform distribution.
</div>

#### History
Benford's law was actually discovered in the 19th century by Simon Newcomb, who noticed that the first pages of logarithm tables were more worn out than the last ones. Some 50 years later, Frank Benford published a paper where he advanced explanations of this anomaly, which is why the law is now named after him.

#### Examples
Many real-world datasets follow Benford's Law, such as the populations of countries, the lengths of rivers, stock prices, the numbers in tax returns, etc. Note however that not all datasets follow Benford's Law. In particular, datasets that do not span several orders of magnitude are unlikely to follow it, for reasons that will become clear in the following sections.


#### Applications
Benford's law can be used to detect made-up data, often generated by humans, since they tend to distribute the first digits uniformly. In particular, it can be used to detect fraud in accounting, elections, academic papers. For instance, the macroeconomic data the Greek government provided to the European Union before entering the Eurozone did not follow Benford's Law (though we found out years later only...)


#### Literature
The literature on Benford's law is somewhat scattered and rife with pseudo-explanations. The phenomenon is still not fully understood, and I do not pretend to provide a definitive answer here. Instead, my goal is to provide three different intuitions behind Benford's Law, which I will try to explain in simple terms and illustrate with simulations.

## First intuition: geometric growth

Perhaps the most intuitive explanation for Benford's Law is that many real-world variables grow geometrically. For instance, the population of a country grows at a certain rate each year, the stock price of a company goes up or down a percent of so each day, etc. When you think of it, any variable that spans several orders of magnitude should be suspected to grow more or less geometrically!

Let's first consider a variable $X_t$ that grows geometrically at some unknown constant rate $r>0$, starting at $X_0=1$. Intuitively, we feel that going from 1 to 2 (a +100% increase) will take more time than going from 9 to 10 (a +11% increase). And once we get to 10, we feel that going from 10 to 20 (a +100% increase) will take more time than going from 90 to 100 (a +11% increase). And so on. In fact, we can formalize this intuition by writing $X_t = (1+r)^t$, such that going from $d\cdot 10^n$ to $(d+1)\cdot 10^n$ takes time $\Delta t = \frac{1}{1+r} \log _ {10} \left(\frac{d+1}{d}\right) \propto \log _ {10}\left(1 + \frac{1}{d}\right)$. We thus recover Benford's Law in the constant geometric growth setting.

What if we relax our assumptions and add some noise to the growth rate? Intuitively we feel like we should still observe Benford's Law, perhaps with some deviations due to the noise.

To confirm this hypothesis, let's consider the process $\frac{dX_t}{X_t}=\mu dt + \sigma dW_t$, where $\mu$ is the drift, $\sigma$ is the volatility, and $dW_t$ is a Brownian motion. It is one (very simple) way of modeling stock prices. The math is a bit trickier to deal with, so instead of an analytical solution we'll simulate the process and plot the distribution of the first digit of $X_t$ at different times $t$. [Figure 2](#fig-2) shows that the distribution of the first digit of $X_t$ indeed matches Benford's Law. The result is robust to the choice of parameters, with better results as we increase the number of simulations $N$ and the time horizon $T$.

<div class="row justify-content-center" id="fig-2">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/benford_law/geometric_growth.png" title="geometric growth" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 2. Benford Law vs. empirical distribution of the first digit of $X_t$. We used parameters $N=1,000,000, T=1,000, \mu=0.1$, $\sigma=0.2$, and $X_0=1$.	
</div>

## Second intuition: CLT on the logarithm

Another closely related intuition comes from the Central Limit Theorem (CLT). The core hypothesis is that variables that follow Benford's law really are *products* of many factors, which are more or less independent. For instance, the price of a brick of butter is the product of its length, width, height, and the price of the milk.

Let's thus consider a positive random variable $X$ which is made up of $P$ underlying factors i.e. $X = \prod_{i=1}^P X_i$ where we assume the $X_i$ to be i.i.d. positive random variables. We have $\log_{10}(X) = \sum_{i=1}^P \log_{10}(X_i)$. By the CLT, $\log_{10}(X)$ should be approximately normally distributed, with variance scaling linearly with $P$. Thus as P grows to infinity, so does the variance of the normal distribution that models $\log_{10}(X)$.

Let's now look at the random variable $\lbrace \log_{10}(X) \rbrace$ where $\lbrace \cdot \rbrace$ denotes the fractional part. Using the result $\lbrace \sigma Z \rbrace \xrightarrow[\sigma^2\xrightarrow\infty]{d} U([0,1])$ where $Z$ is normally distributed, we have $\mathcal{L}(\lbrace \log_{10}(X) \rbrace) \simeq U([0,1])$.

Finally, note that we can rewrite $X$ as $10^{\lbrace \log_{10}(X) \rbrace} \times 10^{\lfloor \log_{10}(X) \rfloor} = \tn{significand} \times \tn{order of magnitude}$. The probability $p_d$ of $X$ having first digit $d$ is then given by
$$
\begin{align*}
    p_d &= \mathbb{P}(d \leq \tn{significand} < d+1) \
        &= \mathbb{P}(\log_{10}(d) \leq \lbrace \log_{10}(X) \rbrace < \log_{10}(d+1)) \
        &= \log_{10}(d+1) - \log_{10}(d) = \log_{10}(1 + \frac{1}{d}) \
$$.
We thus recover Benford's Law.

On [Figure 3](#fig-3), we simulate $X$ as the product of $P=3$ i.i.d. random variables $X_i\sim U([1,10])$. Again, we observe that the distribution of the first digit of $X$ matches Benford's Law. Note that this result is robust to the choice of the distribution of the $X_i$ and the number of factors $P>1$, with better results as we increase $P$.

<div class="row justify-content-center" id="fig-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/benford_law/clt.png" title="clt" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 3. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, P=3$, and $X_i\sim U([1,10])$.
</div>

## Third intuition: scale invariance

This last intuition is a bit different from the two previous ones. The goal here is to *think like a physician* (!). The key idea is that Benford's law applies to variables which have a *dimension*, or to say it plainly, Benford's law applies to numbers that need a unit after them (e.g. meters, euros, liters, etc.) If we look at countries' area for instance, we can measure it in square kilometers or square miles and we'll still observe Benford's law. Likewise, stock prices in EUR, USD and JPY all display Benford's law. And that makes sense right? Since units are arbitrary conventions, we don't expect Benford's law to fade away when we change them.

Okay but how to turn this insight into a mathematical argument? The answer is *scale invariance*.

Let's consider a positive variable $X$ that follows Benford's Law. Assume that $X$ has a probability measure with density $f$ w.r.t. the Lebesgue measure. Since changing units doesn't break Benford's law, we can multiply $X$ by some constant $k$ and still end up with the same distribution. In other words, there is some constant $C(k)$ that such $\forall x, f(kx)=C(k)f(x)$. This is the definition of scale invariance. We also need the probability mass to conserve here, i.e. $f(x)dx = f(kx)d(kx)$, i.e. $f(kx)=\frac{f(x)}{k}$. Differentiating with respect to $k$ and then setting $k=1$ yields the linear functional equation $f'(x) = -\frac{1}{x}f(x)$, with solution $f(x) = \frac{\lambda}{x}$. This isn't technically a probability density function, since it cannot be normalized. In fact scale-invariant distributions are exactly of the form $p(x)\propto \frac{1}{x^{\alpha}}$ for $\alpha>1$ (power law). Let's thus consider $\alpha=1.01$ for instance, to bring us close to the ideal case of Benford's Law.

We simulate $X$ as a random variable with density $p(x)\propto \frac{1}{x^\alpha}$ and plot the distribution of the first digit of $X$ on [Figure 4](#fig-4). We observe that the distribution of the first digit of $X$ indeed matches Benford's Law. Note that the result is *not* robust to the choice of $\alpha$: we only observe concordance with Benford's Law for $\alpha$ close to 1.

<div class="row justify-content-center" id="fig-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/posts/benford_law/scale_invariance.png" title="scale invariance" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 4. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, \alpha=1.01$.
</div>


## Conclusion

We provided three different intuitions behind Benford's Law, which we illustrated with simulations.
1. Geometric growth: Benford's Law arises when variables grow geometrically.
2. CLT on the logarithm: Benford's Law arises when variables are products of many factors.
3. Scale invariance: Benford's Law arises when variables are scale-invariant.