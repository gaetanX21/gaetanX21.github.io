<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Score-Based Generative Modeling | Gaëtan Ecrepont </title> <meta name="author" content="Gaëtan Ecrepont"> <meta name="description" content="Theoretical study of Score-Based Generative Modeling &amp; PyTorch implementation to compare Langevin, SDE and ODE sampling methods. Also explored controlled generation techniques, including conditional generation and inpainting."> <meta name="keywords" content="portfolio-website, machine-learning, statistics, quantitative-finance"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gaetanx21.github.io/projects/sde/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gaëtan</span> Ecrepont </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Score-Based Generative Modeling</h1> <p class="post-description">Theoretical study of Score-Based Generative Modeling &amp; PyTorch implementation to compare Langevin, SDE and ODE sampling methods. Also explored controlled generation techniques, including conditional generation and inpainting.</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sde/diffusion_schematic-480.webp 480w,/assets/img/sde/diffusion_schematic-800.webp 800w,/assets/img/sde/diffusion_schematic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sde/diffusion_schematic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Diffusion" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Score-based generative modeling is the reversal of a diffusion SDE. Source: Song et al. paper. </div> <p>NB: This post is just a recap of my work, but you can get my full report <a href="https://github.com/gaetanX21/generative-sde/blob/main/report/report.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>NBB: For an awesome intro to the topic by the father of score-based generative modeling, check out <a href="https://yang-song.net/blog/2021/score/" rel="external nofollow noopener" target="_blank">this blog</a>.</p> <h2 id="project-overview">Project Overview</h2> <p>In 2020, Song et al. introduced a novel generative modeling framework<sup id="fnref:score"><a href="#fn:score" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> in which samples are produced via Langevin dynamics using gradients from the data distribution. The gradients themselves are estimated using a technique known as denoising score matching, which was introduced in back in 2011 by Vincent<sup id="fnref:score-denoising"><a href="#fn:score-denoising" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Shortly after introducing this new generative model, Song et al. proposed a generalization under the lens of stochastic differential equations<sup id="fnref:sde"><a href="#fn:sde" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>.</p> <p>Our contribution begins by summarizing and connecting the three aforementioned papers. Building on the work of Song et al., we construct a neural network (the score network $s_\theta(\mathbf{x},t)$) from scratch and train it on the MNIST dataset. Using this score network, we implement various sampling methods and compare them. Finally, we extend these methods to controlled generation, focusing on two types: conditional generation and inpainting.</p> <h2 id="why-score">Why “score”?</h2> <p>Generative modeling is the task of learning an unknown distribution $p _ \text{data}(\mathbf{x})$ from a dataset $\mathcal{D} = \lbrace \mathbf{x} _ i \rbrace_{1\leq i \leq N}$ of i.i.d. samples. The goal is to learn a generative model $p_\theta(\mathbf{x})$ such that $p _ \theta(\mathbf{x}) \approx p _ \text{data}(\mathbf{x})$. There are many ways to approach this problem. Perhaps the most natural way is to find $\theta$ that minimizes the Kullback-Leibler divergence between $p_\theta(\mathbf{x})$ and $p_\text{data}(\mathbf{x})$, which is equivalent to maximum likelihood estimation. However, this objective is often intractable for various reasons, the core one being that the KL divergence is too strong of a constraint. We need to relax it somehow.</p> <p>This is where the score function comes in. The score function is defined as the gradient of the log-likelihood of the data distribution <strong>w.r.t. the data itself</strong>: $\nabla_\mathbf{x} \log p_\text{data}(\mathbf{x})$. Instead of minimizing the KL divergence, one can try to match the score functions of the data and model distributions. This is the approach taken by Song et al. and yields score-based generative modeling.</p> <p>Per usual, we’ll be using a neural network $s_\theta(\mathbf{x})$ to approximate the score function. The score network is trained to minimize the score matching loss</p> \[J^\text{naive}(\theta) = \frac{1}{2}\mathbb{E}_{p_\text{data}(\mathbf{x})} \left[ \left\| \mathbf{s}_\theta(\mathbf{x}) - \nabla_\mathbf{x} \log p_\text{data}(\mathbf{x}) \right\|^2 \right]\] <p>However, this objective is obviously intractable since it involves the score function of the data distribution. To circumvent this issue, we can use denoising score matching, which replaces the score function of the data distribution with the score function of a noisy version of the data. The loss becomes</p> \[J^\text{denoising}(\theta) = \frac{1}{2} \mathbb{E}_{\mathbf{x}\sim p_\text{data}(\mathbf{x}),\tilde {\mathbf{x} }\sim q_\sigma(\tilde {\mathbf{x} }|\mathbf{x})} \left[ \left\| \mathbf{s}_\theta(\tilde {\mathbf{x} }) - \nabla_{\tilde {\mathbf{x} } } \log q_\sigma(\tilde {\mathbf{x} }|\mathbf{x}) \right\|^2 \right]\] <p>The intuition behind this objective is that, given a noisy version of the data, the score network should point towards the original data point. Indeed, if we take an isotropic Gaussian noise distribution for $q_\sigma(\tilde {\mathbf{x} }|\mathbf{x})$, we find that $\nabla_{\tilde {\mathbf{x} } } \log q_\sigma(\tilde {\mathbf{x} }|\mathbf{x}) = \frac{\mathbf{x} - \tilde {\mathbf{x} } }{\sigma^2}$, such that the score network is trained to point towards the original data point, i.e., to denoise the data.</p> <p>We can then use gradient descent to optimize the empirical expression of $J^\text{denoising}$ over $\mathcal{D}$, which gives us $\theta^\star$ and thus the score network $s_{\theta^\star}(\mathbf{x})$.</p> <p>However, one question has been left unanswered: how do we pick the noise $\sigma$? If too small, $s_{\theta}(\mathbf{x})$ will be poorly approximated in low-density regions and thus worthless for sampling. If too large, $s_{\theta}(\mathbf{x})$ will be too far from the true score function. The trick is to use a schedule for $\sigma$ that starts large and decreases over time. The only change is to make the score network noise-conditional and train it across various noise levels $\lbrace \sigma_t \rbrace_{t=1}^T$.</p> <h2 id="discrete-time-sampling-langevin-dynamics">Discrete-time sampling: Langevin Dynamics</h2> <p>Langevin Dynamics is a Markov Chain Monte Carlo (MCMC) procedure to sample from the data distribution.</p> <p>Given a fixed step size $\epsilon&gt;0$ and an initial value $\tilde {\mathbf{x} }_0 \sim \pi(\mathbf{x})$ where $\pi$ is a tractable prior distribution (e.g. a Gaussian), the Langevin method recursively computes</p> \[\tilde {\mathbf{x} }_{t} = \tilde {\mathbf{x} }_{t-1} + \frac{\epsilon}{2} \nabla_\mathbf{x} \log p(\tilde {\mathbf{x} }_{t-1}) + \sqrt{\epsilon}\mathbf{z}_t\] <p>where $\mathbf{z}_t \sim \mathcal{N}(0, \mathbf{I})$.</p> <p>The distribution of $\tilde {\mathbf{x} }_T$ converges to the data distribution $p(\mathbf{x})$ as $\epsilon \to 0$ and $T \to \infty$. In practice, $\epsilon&gt;0$ and $T&lt;\infty$, which creates an error in the sampling process, but we can safely assume that this error is small enough to be ignored.</p> <p>Note that since the noise is scheduled, we are in fact doing <strong>annealed</strong> Langevin dynamics.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/sde/langevin-480.webp 480w,/assets/video/sde/langevin-800.webp 800w,/assets/video/sde/langevin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/video/sde/langevin.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="langevin sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Langevin sampling on MNIST. </div> <h2 id="continuous-time-sampling-stochastic-differential-equations">Continuous-time sampling: Stochastic Differential Equations</h2> <p>The Langevin dynamics method presented above works well in practice and scales up nicely to higher dimensions. In effect, we have learned how to gradually noise and gradually denoise our data, through discrete noise levels $\sigma_t$ in the schedule. However, Song et al. found shortly after their first publication that this discrete noising process could be subsumed into a continous-time stochastic differential equation (SDE) framework. This is very interesting because we know from Anderson<sup id="fnref:anderson"><a href="#fn:anderson" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> that reversing the time in a diffusion process yields another diffusion process! Indeed, the diffusion process</p> \[d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t,t)dt + g(t)d\mathbf{w}_t\] <p>can be reversed into</p> \[d\mathbf{x}_t = \left[ \mathbf{f}(\mathbf{x}_t,t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}_t) \right]dt + d\mathbf{\bar{w} }_t\] <p>where $p_t(\mathbf{x})$ is the density of the process at time $t$ (which is given by the Fokker-Planck equation) and $\mathbf{\bar{w} }_t$ is a Wiener process when time flows backwards from $T$ to $0$. (crucially, $dt&gt;0$ in the above equation)</p> <p>Thus, by starting from $\mathbf{x} _ T \sim p _ T (\mathbf{x})$ and running the reverse process, we can obtains samples $\mathbf{x} _ 0 \simeq p _ 0(\mathbf{x})$. This is the idea behind SDE-based generative modeling. Importantly, the only ingredient needed to reverse the process is the score function $\nabla_\mathbf{x} \log p _ t(\mathbf{x})$. We can approximate this score function using a time-dependent neural network $s_\theta(\mathbf{x},t)$, which is the continuous-time equivalent of the noise-conditional score network we used in the discrete-time case.</p> <p>Once the time-dependent score network is trained, we can sample from the SDE using the any black-box SDE solver.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/sde/reverse_sde-480.webp 480w,/assets/video/sde/reverse_sde-800.webp 800w,/assets/video/sde/reverse_sde-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/video/sde/reverse_sde.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sde sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Reverse SDE sampling on MNIST. </div> <h2 id="predictor-corrector-sampling">Predictor-Corrector Sampling</h2> <p>In a nutshell, the idea behing Predictor-Corrector (PC) sampling is to use both the Langevin and SDE sampling methods to generate samples. Each step of the SDE is followed by a step of Langevin dynamics to “correct” the sample $\mathbf{x}_t$ obtained from the SDE. In practice, this combination of approaches yields better samples than either method alone. Song et. al achieved state-of-the-art results on the CIFAR-10 dataset using PC sampling.<sup id="fnref:sde:1"><a href="#fn:sde" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/sde/pc-480.webp 480w,/assets/video/sde/pc-800.webp 800w,/assets/video/sde/pc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/video/sde/pc.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pc sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> PC sampling yields superior samples compared to Langevin or SDE sampling alone. </div> <h2 id="ordinary-differential-equation-ode-sampling">Ordinary Differential Equation (ODE) Sampling</h2> <p>In <sup id="fnref:score:1"><a href="#fn:score" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Song et al. demonstrate that any SDE can be converted into an ODE with the same marginal distributions ${ p _ t(\mathbf{x}) }_{t\in[0,T]}$ (albeit not the same joint distributions $p(\mathbf{x _ {0:T}})$). The ODE of an SDE is called <strong>probability flow ODE</strong> and is given by</p> \[d\mathbf{x}(t) = \left[ \mathbf{f}(\mathbf{x}(t), t) - \frac{1}{2}g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}(t)) \right]dt\] <p>Once again, we only need to be able to compute the score function $\nabla_\mathbf{x} \log p_t(\mathbf{x})$ to sample from the ODE. This is done using the time-dependent score network $s_\theta(\mathbf{x},t)$. The ODE is then solved using a black-box ODE solver. Importantly, probability flow ODEs are a special case of neural ODEs and as such they allow for exact log-likelihood computation, which is a major advantage over Langevin and SDE sampling methods.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/sde/ode-480.webp 480w,/assets/video/sde/ode-800.webp 800w,/assets/video/sde/ode-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/video/sde/ode.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ode sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ODE sampling is notably smoother than Langevin or SDE sampling since it is a deterministic process. </div> <h2 id="controlled-generation">Controlled Generation</h2> <p>So far, we have described several methods for generating random samples from the data distribution. However, the diffusion approach is flexible and can easily be tweaked to control the generation process. The idea is the following: if we denote by $\mathbf{y}$ the conditions we want to impose on the generated samples, we aim to sample from the conditional distribution $p _ \text{data} ( \mathbf{x} \mid \mathbf{y} )$.</p> <p>Starting with random noise, we reverse through time using either the Langevin, SDE or ODE method, adjusting the process to use $\nabla_{\mathbf{x} } \log p _ t ( \mathbf{y} \mid \mathbf{x} )$ instead of $\nabla_{\mathbf{x} } \log p _ t ( \mathbf{x} )$. Intuitively, this approach reverts the diffusion of $\lbrace \mathbf{x} _ t \mid \mathbf{y} \rbrace _ {t \in [0,T]}$ instead of $\lbrace \mathbf{x} _ t \rbrace _ {t \in [0,T]}$, meaning our final sample will satisfy $\mathbf{x} _ 0 \sim p _ 0 ( \mathbf{x} \mid \mathbf{y} )$ as desired.</p> <p>To illustrate, we implemented this idea in two key applications: class-conditional generation and inpainting. The details are in the full report, but the main idea is to condition the score network on the class label or the masked pixels, respectively. The results are quite satisfying, as shown below.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/sde/conditional-480.webp 480w,/assets/video/sde/conditional-800.webp 800w,/assets/video/sde/conditional-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/video/sde/conditional.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="conditional sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Class-conditional generation using PC sampling and setting $y=4$. </div> <div class="row justify-content-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sde/inpainting_masked-480.webp 480w,/assets/img/sde/inpainting_masked-800.webp 800w,/assets/img/sde/inpainting_masked-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sde/inpainting_masked.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="inpainting masked" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sde/inpainting_recovered-480.webp 480w,/assets/img/sde/inpainting_recovered-800.webp 800w,/assets/img/sde/inpainting_recovered-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sde/inpainting_recovered.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="inpainting recovered" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: masked MNIST samples. Right: after inpainting. </div> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:score"> <p><em>Generative Modeling by Estimating Gradients of the Data Distribution</em>. Yang Song &amp; Stefano Ermon. <a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="#fnref:score" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:score:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:score-denoising"> <p><em>A Connection Between Score Matching and Denoising Autoencoders</em>. Pascal Vincent. <a href="https://www.iro.umontreal.ca/~vincentp/Publications/DenoisingScoreMatching_NeuralComp2011.pdf" rel="external nofollow noopener" target="_blank">iro.umontreal.ca</a> <a href="#fnref:score-denoising" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:sde"> <p><em>Score-Based Generative Modeling through Stochastic Differential Equations</em>. Yang Song et al. <a href="https://arxiv.org/abs/2011.13456" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="#fnref:sde" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:sde:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:anderson"> <p><em>Reverse-time diffusion equation models</em>. O. G. Anderson. <a href="https://www.sciencedirect.com/science/article/pii/0304414982900515" rel="external nofollow noopener" target="_blank">sciencedirect.com</a> <a href="#fnref:anderson" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gaëtan Ecrepont. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X0M5D3J28G"></script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>