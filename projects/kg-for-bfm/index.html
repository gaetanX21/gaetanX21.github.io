<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Knowledge Graph Integration for Biological Foundation Models | Gaëtan Ecrepont </title> <meta name="author" content="Gaëtan Ecrepont"> <meta name="description" content="Presentation of four distinct approaches to integrate knowledge graphs for biological foundation models."> <meta name="keywords" content="portfolio-website, machine-learning, statistics, quantitative-finance"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gaetanx21.github.io/projects/kg-for-bfm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Gaëtan</span> Ecrepont </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Knowledge Graph Integration for Biological Foundation Models</h1> <p class="post-description">Presentation of four distinct approaches to integrate knowledge graphs for biological foundation models.</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig3-480.webp 480w,/assets/img/kg-for-bfm/fig3-800.webp 800w,/assets/img/kg-for-bfm/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bfm-vs-llm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Biological Foundation Models (BFMs) and Large Language Models (LLMs) are two very different types of foundation models. </div> <p>NB: I wrote this post as part of my internship at <a href="https://www.scientalab.com/" rel="external nofollow noopener" target="_blank">Scienta Lab</a>, where I worked on integrating knowledge graphs in biological foundation models (BFMs). The goal of this work was to enhance BFMs with highly-curated data, possibly spanning additional modalities.</p> <h1 id="foreword">Foreword</h1> <p><em>At Scienta Lab, we are building biological foundation models (BFMs) for immunology and inflammation. We believe that AI can help us accelerate the development of innovative drugs for immune-mediated diseases from the vast amounts of multimodal data accumulated over decades of research. Once trained, our BFMs can be used on a variety of downstream tasks, including prediction of drug candidate efficacy, patient stratification, and biomarker identification.</em></p> <p><em>In this blog post, we will present several approaches that integrate knowledge graphs (KGs) to BFMs. The goal of KG integration is to enhance the model with highly-curated data, possibly spanning additional modalities.</em></p> <h1 id="i-introduction">I. Introduction</h1> <h2 id="a-biological-foundation-models">A. Biological Foundation Models</h2> <p>Let’s first define a few terms.</p> <blockquote> <p>Foundation models are deep learning models trained on vast datasets so that they can be applied across a wide range of use cases.</p> </blockquote> <p>A lot of the AI hype today revolves around foundation models for <em>natural language processing</em> (NLP), which are often called Large Language Models (LLMs). These are models that people can directly interact with through text messaging, like OpenAI’s ChatGPT or Mistral’s Le Chat.</p> <p>While the success of LLMs is undeniable, their area of expertise is not unbounded. For instance, LLMs have no idea how to deal with genetic data simply because they aren’t trained on this data modality.</p> <blockquote> <p>Data modality is a fancy term to refer to the “type” of data we are dealing with. The most common modalities in AI are textual and visual data, but there are many more like audio, time series, graph and genomic data.</p> </blockquote> <p>Emboldened by the prowesses of foundation models in NLP, companies are now racing to adapt these models to other modalities. Biology in particular has been garnering a lot of attention as genomic data keeps getting cheaper thanks to continued advances in gene sequencing technologies, as shown on Figure 1.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig1-480.webp 480w,/assets/img/kg-for-bfm/fig1-800.webp 800w,/assets/img/kg-for-bfm/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Gene sequencing costs have been decreasing at an exponential rate over the past two decades. </div> <blockquote> <p>Biological foundation models (BFMs) are foundation models trained on one or several biological modalities. Unlike LLMs, these models aren’t primarily designed to work with human language. Instead, their goal is to capture the complex relationships hidden in the large amounts of biological data they were trained on. Just like LLMs can be thought of as super-humans who have read all the books on Earth, BFMs are envisioned to become super-humans who have crunched the biological data of millions of humans (and potentially other organisms), thus uncovering the complex laws of biology.</p> </blockquote> <p>BFMs are relatively new and exist in all forms and shapes today. In particular, they can differ through the type of modality they tackle.</p> <p><strong>In this blog post, we will focus on BFMs for RNA-seq data</strong>.</p> <blockquote> <p>“RNA-seq is a technique that uses next-generation sequencing to reveal the presence and quantity of RNA molecules in a biological sample, providing a snapshot of gene expression in the sample, also known as transcriptome.” (cf. <a href="https://en.wikipedia.org/wiki/RNA-Seq" rel="external nofollow noopener" target="_blank">Wikipedia</a>)</p> </blockquote> <p>Technically, there are two types of RNA-seq data modalities: single-cell and bulk. We will focus on single-cell in this blog post.</p> <blockquote> <p>Single-cell RNA-seq provides the expression profiles of individual cells, whereas bulk RNA-seq measures gene expression at the sample level, i.e. across a population of cells.</p> </blockquote> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig2-480.webp 480w,/assets/img/kg-for-bfm/fig2-800.webp 800w,/assets/img/kg-for-bfm/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: RNA-seq technology in a nutshell. Top is single-cell RNA-seq, and bottom is bulk RNA-seq. Note that when we have multiple cells or multiple samples, the vectors on the right are stacked vertically and we obtain a matrix. </div> <p>In a nutshell, single-cell RNA-seq data tells us which genes are activated in a given cell, making it a rich description of the state of the cell. However, as we will see in the next paragraph, RNA-seq comes with its own set of challenges.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig3-480.webp 480w,/assets/img/kg-for-bfm/fig3-800.webp 800w,/assets/img/kg-for-bfm/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Biological Foundation Models (BFMs) and Large Language Models (LLMs) are two different types of foundation models. </div> <p>At first sight, it may seem that BFMs are just re-branded LLMs. But there are deep reasons why building BFMs isn’t as straightforward as building LLMs. Let’s dive in!</p> <h2 id="b-bfms-vs-llms">B. BFMs vs LLMs</h2> <p>There are two major differences between BFMs and LLMs: intrinsic complexity and data availability.</p> <h3 id="intrinsic-complexity">Intrinsic complexity</h3> <p>Simply put, a LLM’s goal is to build a <em>good world representation</em>. Just like a person who gradually understands the world through sheer experience, a LLM accumulates human knowledge as it is trained on more and more text data.</p> <p>In a similar fashion, the objective of a BFM is to develop <em>rich biological representations</em>, ideally across all modalities. A similar metaphor applies here: just like a biology researcher must spend years, even decades, studying the field to build a valid mental model, a BFM must process vast amounts of data to form a robust internal representation of the complex processes underlying life.</p> <p>Thus, LLMs and BFMs are both trained to create <em>meaningful data representations</em> of their respective domains. The catch is that <strong>the complexities of biology and human knowledge are fundamentally different!</strong> This is clear if we think in terms of <em>time-scale</em> and <em>organization</em> (or entropy):</p> <blockquote> <p>Time-scale: If we define the beginning of “human knowledge” as the invention of writing, then we have been producing knowledge for about 5,000 years. This sounds like a lot until you remember that life on Earth began about 4 billion years ago with the apparition of single-cell organisms.</p> </blockquote> <blockquote> <p>Organization: Life on Earth evolved according to a single law: survival of the fittest. As such, evolution is a random and oftentimes messy process, whereas human innovation is a lot more rigorous and stable.</p> </blockquote> <p><em>Takeaway: There are good reasons to believe that the complexity of biology is vastly different from that of human knowledge. Thus, BFMs cannot simply emulate the success of LLMs using a 1:1 approach.</em></p> <h3 id="data-availability">Data availability</h3> <p>The three key ingredients to build AI systems are:</p> <ol> <li>theory (algorithms &amp; models)</li> <li>data</li> <li>compute</li> </ol> <p>While AI progress has been enabled by parallel advances in these three domains, oftentimes a single one will be the bottleneck in a given AI use case.</p> <p><strong>For biology, data is by far the limiting factor</strong>. You might answer that LLMs also crave more data, but there are key differences between text data and biological data: volume, quality, and information content. Here I will illustrate those differences on RNA-seq data, though the comparison applies to other biology modalities.</p> <ol> <li> <p><strong>Volume</strong>. Volume is the most obvious difference between text and RNA-seq data. As you probably know, LLMs are trained on huge corpora curated from online content, today’s frontier models are trained on ~10T tokens. In contrast, the largest open RNA-seq datasets contain at most ~50M cells, representing about ~50k independent samples. Since there are about ~2k genes of interest, each represented by a token, this leaves us with 50M × 2k ~ 100B correlated tokens, or 50k × 2k ~ 100M independent tokens. That’s 100 to 100,000 times less data to begin with!</p> </li> <li> <p><strong>Quality</strong>. The second issue is that text data and RNA-seq data are two widely different creatures. Text is well-behaved: you can read it and understand it, you can easily manipulate it, and it is unambiguous. RNA-seq data is a whole other story: it’s very difficult to grasp (try making sense of a 2000-dimensional vector), there is no agreed-upon way to work with it, and it depends on the lab protocols and sequencing machines used to obtain it, leading to ”batch effects”! What’s more, because biology is very redundant (due to the random nature of evolution and the need for robustness in organisms), the amount of raw information contained in RNA-seq is much less than it seems, so we have even less data in practice!</p> </li> <li> <p><strong>Information Content</strong>. This one is a bit trickier but just as important. Consider the sentence: “The duck is green.” Notice that this text sample contains all the information we could ask for. There is no uncertainty. Now consider a sick patient from whom we collected some cells to run an analysis of their RNA-seq data. Let’s say we end up with a nice 2000-dimensional vector. Would you say that this vector contains all the information needed to understand the patient’s condition? Probably not. And this is the big difference with text: while text is <em>all the information</em>, RNA-seq (and any biological modality) is <em>just a part of the story</em>. The following metaphor sums it up nicely:</p> <blockquote> <p>Trying to find insights from a single biological data modality is like trying to understand a movie from the sound only: you’re in for a hard time.</p> </blockquote> </li> </ol> <p><em>Takeaway: Text data is ubiquitous, high-quality, and self-contained. Biological data is scarce, low-quality, and just part of the picture.</em></p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig4-480.webp 480w,/assets/img/kg-for-bfm/fig4-800.webp 800w,/assets/img/kg-for-bfm/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: BFMs and LLMs plotted on the intrinsic complexity - data availability plane, alongside other AI use cases. </div> <h2 id="c-how-knowledge-graphs-can-help">C. How Knowledge Graphs Can Help</h2> <p>The previous paragraph should have convinced you that <strong>data</strong> (not compute, not theory) is the crucial bottleneck when building BFMs.</p> <blockquote> <p>The good news is: working with poor data is nothing new in the field of AI, and many methods have been developed over decades of research to tackle this issue.</p> </blockquote> <p>For the sake of brevity, I’ll list the relevant existing approaches to mitigate this lack of data and which category they fall in.</p> <table> <thead> <tr> <th><strong>Approach</strong></th> <th>Category</th> </tr> </thead> <tbody> <tr> <td>1. Finding more data</td> <td>extensive</td> </tr> <tr> <td>2. Doing data augmentation</td> <td>extensive-ish</td> </tr> <tr> <td>3. Using inductive biases</td> <td>intensive</td> </tr> <tr> <td>4. Reducing model complexity</td> <td>intensive</td> </tr> <tr> <td><strong>5. Leveraging domain knowledge</strong></td> <td>extensive</td> </tr> </tbody> </table> <p>As you can see, there’s quite a few ways to tackle lack of data. We can classify them in two broad categories: <em>intensive</em> and <em>extensive</em>. Intensive methods do not add external information, their goal is to help the model learn more efficiently. On the contrary, extensive methods work by giving more information to the model. Here’s a metaphor to clarify things</p> <blockquote> <p>Imagine you are a student trying to get better at math. You could work on methodology to be more efficient on your tests: this would be an intensive effort. But you can also buy a bunch of math textbooks and study them religiously: this would be an extensive approach.</p> </blockquote> <p>The catch is: intensive approaches will only get you so far. If we go back to the math analogy: even the most elaborate methodology won’t save you on test day if you don’t know the mathematical concepts you need to manipulate!</p> <p>In the case of BFMs, we’ve seen that data is key. So we need to focus on external methods to build better models. But acquiring biological data is a notoriously lengthy, costly, and uncertain process, for both legal and practical reasons. Could we look for data elsewhere?</p> <p><strong>This is where biological knowledge graphs (KGs) come into play!</strong></p> <p>If you aren’t familiar with KGs, you can think of them as a networks with several types of nodes and various possible relationships between them. Below is a minimal KG to give you an idea.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig5-480.webp 480w,/assets/img/kg-for-bfm/fig5-800.webp 800w,/assets/img/kg-for-bfm/fig5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: A minimal knowledge graph. Note the different node types (Person, Company, University) and the various kinds of relationships between them. </div> <p>Biology KGs are no different from the minimal example above, except that they are <em>much</em> larger. For instance, the <em>Bio2RDF</em> KG encompasses many biological entities (genes, proteins, diseases, drugs, etc.) and the interactions between them (gene→protein, gene→disease, drug→target, etc.) It boasts over 11 billion triplets (i.e. interactions of the form <code class="language-plaintext highlighter-rouge">node_1-&gt;interaction-&gt;node_2</code>), obtained by integrating over 30 biomedical datasets.</p> <p>There’s a lot more to say about KGs but this goal of this blog post is about <em>how</em> to integrate them in BFMs. Indeed, the main challenge with KGs is that they are highly heterogeneous. That is, there’s several types of nodes and various types of relationships between them: this is very far from the matrix-like data relished by most AI models.</p> <p>In case you didn’t know, most BFMs are based on the transformer architecture, which is behind the success of LLMs. You don’t need to be familiar with transformers to understand this post, but you do need to keep in mind that these models <em>really</em> like to work with matrices. In fact:</p> <blockquote> <p>Transformer models take matrices as input and produce matrices as output. They’re designed to deal with vectors and matrices, not fancy data structures.</p> </blockquote> <p>With this in mind, we start to see why integrating KGs in BFMs is a non-trivial task. We need to somehow feed highly heterogeneous &amp; non-tabular data to a model which expects nothing but polished matrices. How can we do that?</p> <p>In short: no one knows for sure, but research is making rapid progress. The literature on KG integration for BFMs has been blossoming over the past years and below I’ll review four peer-reviewed papers. I won’t discuss the performance gains derived the proposed methods as they are difficult to estimate due to the lack of robustness of current benchmarks (small &amp; noisy validation datasets). Instead, my goal here is to present original and orthogonal ideas for KG integration in BFMs.</p> <p><em>NB (KGs beyond data): I want to highlight that KGs aren’t only about supplying more data to BFMs. They also represent a technological innovation that would distinguish them from LLMs, which they have very closely mirrored so far.</em></p> <h1 id="ii-interesting-approaches">II. Interesting Approaches</h1> <h2 id="a-injecting-extra-modalities-genecompass">A. Injecting extra modalities: GeneCompass</h2> <p>The core idea behind GeneCompass is to integrate the KG through extra “modalities” added to the original input. Indeed, most foundation models embed each gene as the sum of its gene ID embedding and its gene expression embedding. GeneCompass enriches this sum with four new embedding modalities, each derived from a different source of biological knowledge:</p> <ol> <li>Gene Regulatory Network (think of it as a control system choosing which genes to express)</li> <li>Promoter (the promoter of a gene acts as a “start reading here” sign for the RNA polymerase)</li> <li>Gene Family (genes are organized into families)</li> <li>Gene Co-Expression (measuring correlation in gene expression)</li> </ol> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig6-480.webp 480w,/assets/img/kg-for-bfm/fig6-800.webp 800w,/assets/img/kg-for-bfm/fig6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: GeneCompass creates four new “modalities” to add to the input. </div> <p>Below is a figure to better visualize how GeneCompass adds four new “modalities” to the input.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig7-480.webp 480w,/assets/img/kg-for-bfm/fig7-800.webp 800w,/assets/img/kg-for-bfm/fig7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7: Compared to regular BFMs, GeneCompass adds new “modalities” to the input. </div> <h2 id="b-enforcing-cell-representation-sccello">B. Enforcing cell representation: scCello</h2> <p>In scCello, the authors’ thesis is that BFMs so far haven’t leveraged the known <em>taxonomic</em> <em>relationships</em> between cell types (i.e. the cell ontology graph, which organizes cell in a hierarchical structure). The goal of accounting for taxonomy is to instruct the model with the following kind of relationships:</p> <blockquote> <p>“mature α-βT cell” should be closer to “mature T cells” compared to more general term “T cells” and farther from neurons and astrocytes from the brain.</p> </blockquote> <p>To integrate taxonomy in their BFM, the scCello authors introduce the notion of “cell type” and align the model on it using a <em>contrastive loss</em> objective derived from the Gene Ontology (GO) KG. That is, in addition to creating an embedding per input gene, scCello creates a cell-wide embedding.</p> <p>Thus, scCello has a dual task: not only does it need to learn good gene embeddings, but it must also create a meaningful cell-wide representation. In such cases, we use multi-task training which, as the name indicates, consists in training a model to perform several tasks.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig8-480.webp 480w,/assets/img/kg-for-bfm/fig8-800.webp 800w,/assets/img/kg-for-bfm/fig8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8: Unlike regular BFMs, scCello creates a cell embedding, which is given meaning during training with contrastive learning derived from the Gene Ontology knowledge graph. T is the number of genes considered, usually around 2,000. </div> <h2 id="c-selecting-gene-gene-interactions-gears">C. Selecting gene-gene interactions: GEARS</h2> <p>Unlike most BFMs, the GEARS model is based on Graph Neural Networks (GNNs) instead of the classic transformer architecture. In a nutshell, GNNs are machine learning models designed specifically to work with graph data. We won’t go into the details of how GNNs work, but the key insight is that they are a bit more flexible than transformers, at the cost of computational efficiency. The main difference is between GNNs and transformers is communication between nodes/token:</p> <ul> <li>in a GNN, two nodes only communicate if they are linked in the graph provided by the user</li> <li>in a transformer, all tokens communicate through the attention mechanism</li> </ul> <p>The novel idea in GEARS is to leverage GNNs to select which genes should communicate based on prior information about gene-gene interaction, obtained from KGs.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig9-480.webp 480w,/assets/img/kg-for-bfm/fig9-800.webp 800w,/assets/img/kg-for-bfm/fig9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 9: Transformers’ attention mechanism means all genes interact with each other. On the contrary, GNNs restrict communication to genes which are linked in the user-supplied graph. In the case of GEARS, this graph is derived from knowledge graphs. </div> <h2 id="d-appending-special-tokens-scprint">D. Appending special tokens: scPRINT</h2> <p>Although scPRINT doesn’t use KGs per se, it introduces an interesting idea, which to append “special tokens” to the input to encode hand-picked features (e.g. cell representation, species, tissue, disease, technical covariates).</p> <blockquote> <p>If you’re familiar with the BERT text encoder, you know that in addition to creating embeddings for individual word tokens, it creates a special <code class="language-plaintext highlighter-rouge">&lt;CLS&gt;</code> token which is designed to squeeze all the information of the input text into a single token. For instance, when performing sentiment analysis we commonly train a classifier the <code class="language-plaintext highlighter-rouge">&lt;CLS&gt;</code> token output by BERT for a given text. Thus, we might have special tokens such as <code class="language-plaintext highlighter-rouge">&lt;CELL&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;TISSUE&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;DISEASE_STATE&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;TECHNICAL_COVARIATE_1&gt;</code> .</p> </blockquote> <p>In practice, these special tokens are initialized as arbitrary vectors alongside the genes in the input. They are then gradually imbued with contextual knowledge as the data flows through the successive transformer blocks, enabling communication between tokens. At the output layer, the special tokens are now enriched with meaning and can be used for downstream tasks such as cell type prediction, tissue classification, or patient stratification. Just like for scCello, we force the model to create meaningful representation in the special tokens by training in a multi-task fashion.</p> <div class="row justify-content-center"> <div class="mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kg-for-bfm/fig10-480.webp 480w,/assets/img/kg-for-bfm/fig10-800.webp 800w,/assets/img/kg-for-bfm/fig10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kg-for-bfm/fig10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 10: Unlike regular BFMs, scPRINT appends special tokens such as <code>&lt;CELL&gt;</code> alongside the gene inputs. </div> <h1 id="iii-conclusion">III. Conclusion</h1> <p>Congrats for making it to here 🙌 ! This post was quite a lengthy one! The take-home messages are:</p> <ol> <li>Building BFMs isn’t the same as building LLMs. The challenges are different. In particular, keep in mind that unlike LLMs, BFMs are data-starved. What’s more, we have good reasons to believe that the intrinsic complexity of biology is vastly different from that of human knowledge.</li> <li>There are many approaches to mitigate the data problem in BFMs. These methods can be intensive or extensive. One very promising extensive approach is to integrate KGs to BFMs, thus enhancing the model with highly-curated data, possibly spanning additional modalities.</li> <li>Integrating KGs into BFMs is still an area of research, with great rewards awaiting those who solve this problem. Among the various ideas found in the literature, we have seen four original and orthogonal ones: <ol> <li>Injecting extra modalities: <em>GeneCompass</em> </li> <li>Enforcing cell representation: <em>scCello</em> </li> <li>Selecting gene-gene interactions: <em>GEARS</em> </li> <li>Appending special tokens: <em>scPRINT</em> </li> </ol> </li> </ol> <h1 id="references">References</h1> <ul> <li>Yang, X., Liu, G., Feng, G. <em>et al.</em> GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation model. <em>Cell Res</em> <strong>34</strong>, 830–845 (2024). https://doi.org/10.1038/s41422-024-01034-y</li> <li>Yuan, X., Zhan, Z., Zhang, Z. et al. <em>Cell-ontology guided transcriptome foundation model</em>. <em>arXiv</em> (2025). <a href="https://arxiv.org/abs/2408.12373" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2408.12373</a> </li> <li>Roohani, Y., Huang, K. &amp; Leskovec, J. Predicting transcriptional outcomes of novel multigene perturbations with GEARS. <em>Nat Biotechnol</em> <strong>42</strong>, 927–935 (2024). https://doi.org/10.1038/s41587-023-01905-6</li> <li>Kalfon, J., Samaran, J., Peyré, G. <em>et al.</em> scPRINT: pre-training on 50 million cells allows robust gene network predictions. <em>Nat Commun</em> <strong>16</strong>, 3607 (2025). https://doi.org/10.1038/s41467-025-58699-1</li> </ul> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gaëtan Ecrepont. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X0M5D3J28G"></script> <script defer src="/assets/js/google-analytics-setup.js?9d15c8cd8e550d35a6c2d883f01c70c4"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>