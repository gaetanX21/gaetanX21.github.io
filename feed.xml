<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-04T20:37:03+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Copula Theory and the Subprime Mortgage Crisis</title><link href="https://gaetanx21.github.io/blog/2025/copulas/" rel="alternate" type="text/html" title="Copula Theory and the Subprime Mortgage Crisis"/><published>2025-05-25T00:00:00+00:00</published><updated>2025-05-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/copulas</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/copulas/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}} \newcommand{\L}{\mathcal{L}} \newcommand{\P}{\mathbb{P}}\] <p>In this post, we will explore the concept of copulas, which are mathematical functions that allow us to model the correlation structure between random variables. After a lightning-fast introduction to copula theory, we will visualize some important copulas to get an intuitive understanding of their behavior. After discussing the concept of tail dependence and how it can be quantified, we will see how different copulas capture tail dependence in different ways. Finally, we will discuss the role of copulas in the subprime mortgage crisis, where they were used to model the correlation structure of mortgage-backed securities, leading to a catastrophic underestimation of risk.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Clayton copula with $\theta=1$. The bright area in the lower left corner indicates lower tail dependence, as we will see later. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Consider the following problem:</p> <blockquote> <p>You are given two random variables $X\sim \L_X$ and $Y\sim \L_Y$ where $\L_X$ and $\L_Y$ are known and you want to model their correlation structure.</p> </blockquote> <p>As you can imagine, this type of problem shows up rather quickly whenever we want to finely model the interactions between two or more random variables. In practice, we often circumvent this problem by working under one of the following (strong) assumptions:</p> <ol> <li><strong>Independence</strong>: We assume that $X$ and $Y$ are independent, which means that their joint distribution can be expressed as the product of their marginal distributions: $\L_{XY} = \L_X \otimes \L_Y$.</li> <li><strong>Multivariate normality</strong>: We assume that $(X,Y)$ follows a bivariate normal distribution, which allows us to model their correlation structure with a covariance matrix.</li> </ol> <p>While these two assumptions are quite convenient and can still be useful to build simple models, in practice they are often too restrictive and do not capture the true nature of the relationship between $X$ and $Y$.</p> <p>For instance, if you are insuring houses in <em>several</em> nearby flood-prone areas, you might want to use Gumbel distributions to model the <em>marginal</em> distributions of the flood levels in each separate area, but you would still need to model the <em>joint</em> distribution of the cross-area flood levels to assess the risk of a catastrophic event affecting multiple areas at once (and potentially leading to bankruptcy of your insurance company!).</p> <p>Lucky for us, probability theory has got exactly the tool we are looking for: <strong>copula theory</strong>.</p> <h2 id="ii-quick-introduction-to-copulas">II. Quick introduction to copulas</h2> <p>In this section, I’ll give a quick and intuition-first introduction to copulas, which will be enough to understand the rest of the post. For a more in-depth introduction, there are many great resources available online<sup id="fnref:blog"><a href="#fn:blog" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. I’ll skip the scary and technical definition of a copula and instead focus on the <strong>intuition</strong> behind it! I will limit myself to the bivariate case, but the generalization to more than two variables is straightforward (I promise!).</p> <p>Let’s again consider $X$, $Y$ two random variables with known marginal distributions $\L_X$ and $\L_Y$. We are looking for a <em>well-behaved mathematical object</em> to encode the correlation structure between $X$ and $Y$. One natural candidate is the <strong>joint cumulative distribution function</strong></p> \[F_{XY}(x,y) = P(X \leq x, Y \leq y)\] <p>The problem with this object is that its domain $\mathcal{D}(F_{XY}) = \mathcal{X} \times \mathcal{Y}$ depends on $X$ and $Y$.</p> <p>There’s a neat trick to get around this: we can use the <strong>probability integral transform</strong> to map both $X$ and $Y$ to the unit interval $[0,1]$. To do so, let:</p> \[(U,V) = \big(F_X(X), F_Y(Y)\big)\] <p>Notice that $U$ and $V$ are both uniformly distributed on $[0,1]$, by property of the probability integral transform. Crucially, the joint distribution of $(U,V)$ <strong>still encodes the correlation structure between $X$ and $Y$</strong>, but now it is defined on the fixed domain $\mathcal{D}(U,V) = [0,1]^2$.</p> <p>We can now define the <strong>joint cumulative distribution function of $(U,V)$</strong> as:</p> \[C_{XY}(u,v) = \P(U \leq u, V \leq v) = \P(F_X(X) \leq u, F_Y(Y) \leq v) = \P(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(v))\] <p>This function $C_{XY}(u,v)$ is called a <strong>copula</strong>, and it captures the joint distribution of the random variables $X$ and $Y$ while being defined on a fixed domain. The key property of copulas is that they allow us to separate the marginal distributions from the correlation structure, which is precisely what we need to model the relationship between $X$ and $Y$.</p> <p>We can sum up what we just saw as follows:</p> <blockquote> <p>The joint distribution of two random variables $X$ and $Y$ can split into two components: the marginal distributions $\L_X$ and $\L_Y$, and the copula $C_{XY}$ that captures the correlation structure between them.</p> </blockquote> <p>The above result is known as <strong>Sklar’s theorem</strong>, and it actually works both ways: you can split any multivariate distribution into its marginals and a copula, but if you’re given some marginals and a copula, you can also construct the corresponding multivariate distribution!</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/sklar-480.webp 480w,/assets/img/posts/copulas/sklar-800.webp 800w,/assets/img/posts/copulas/sklar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/sklar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Sklar's theorem in a nutshell: a multivariate distribution can be decomposed into its marginals and a copula; conversely, given marginals and a copula, we can reconstruct the corresponding multivariate distribution. </div> <p>Given the above intuitive definition, it should be clear that a (bivariate) copula is formally defined as a function</p> \[C: [0,1]^2 \to [0,1]\] <blockquote> <p>As such, we can conveniently represent it as a 2D surface in the unit square, where the height of the surface at a point $(u,v)$ corresponds to the value of the copula $C(u,v)$. This will be useful later when we visualize some important copulas.</p> </blockquote> <p>With this in mind, we can move on to the next section, where we will explore some important copulas and their properties.</p> <h2 id="iii-important-copulas">III. Important copulas</h2> <h3 id="a-gaussian-copula">A. Gaussian copula</h3> <p>As usual in statistics, the Gaussian case will be the easiest to understand and manipulate. For a given correlation matrix \(\Sigma_\rho = \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{pmatrix}\)</p> <p>the Gaussian copula is defined as:</p> \[C_{\rho}^\tn{Gauss}(u,v) = \Phi_\rho(\Phi^{-1}(u), \Phi^{-1}(v))\] <p>where $\Phi$ is the cumulative distribution function of the standard normal distribution, and $\Phi_\rho$ is the cumulative distribution function for $\mathcal{N}(0,\Sigma_\rho)$. The Gaussian copula is particularly useful because it allows us to model the correlation structure between two random variables using a single parameter $\rho$, which is the correlation coefficient.</p> <p>Below is a plot of the Gaussian copula for $\rho=0.5$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Gaussian copula with $\rho=0.5$. </div> <h3 id="b-student-copula">B. Student copula</h3> <p>As we shall see in the next section, the Gaussian copula has bad tail properties: it does not capture the tail dependence between the random variables $X$ and $Y$. One alternative is the <strong>Student copula</strong>, which is defined in a similar fashion as the Gaussian copula, but uses the Student’s t-distribution instead of the normal distribution. The Student copula is parameterized by the degrees of freedom $\nu$ and the correlation matrix $\Sigma_\rho$. It is defined as:</p> \[C_{\rho,\nu}^\tn{Student}(u,v) = t_{\rho,\nu}(t_\nu^{-1}(u), t_\nu^{-1}(v))\] <p>where $t_\nu$ is the cumulative distribution function of the Student’s t-distribution with $\nu$ degrees of freedom, and $t_{\rho,\nu}$ is the cumulative distribution function for the bivariate Student’s t-distribution with correlation $\rho$ and $\nu$ degrees of freedom. The Student copula is particularly useful when we want to model tail dependence, as it allows for heavier tails than the Gaussian copula.</p> <p>Below is a plot of the Student copula for $\rho=0.5$ and $\nu=1$.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Student copula with $\rho=0.5$ and $\nu=1$. </div> <p>As you can see, correlation increases at the tails compared to the Gaussian copula, which is a key property of the Student copula. Don’t worry if this doesn’t make sense yet, we’ll come back to this in the next section when we discuss tail dependence.</p> <h3 id="c-gumbel-copula">C. Gumbel copula</h3> <p>We’ve seen that unlike the Gaussian copula, the Student copula captures tail dependence, but it does so in a <em>symmetric</em> way: correlation at the upper (near $(1,1)$) and lower (near $(0,0)$) tails is the same. In practice however, it is often the case that the correlation structure is <em>asymmetric</em>. If we model floods for instance, we expect the upper tail (high flood levels) to be more correlated than the lower tail (low flood levels), since floods are often caused by extreme weather events that affect multiple areas at once. In such cases, we need a copula that can capture upper tail dependence.</p> <p>Turns out that the Gumbel copula does exactly that. To avoid scaring you and because it wouldn’t add much to the discussion, I won’t give the analytic definition of the Gumbel copula, but rather give you a feeling of how it behaves through its graphical representation, which is shown below for $\theta=2$ (the parameter $\theta$ controls the strength of the upper tail dependence).</p> <div class="row justify-content-center" id="fig-5"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Gumbel copula with $\theta=2$. </div> <p>As you can see, the Gumbel copula captures upper tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the upper right corner $(1,1)$. As said before, this is particularly useful when modeling extreme climatic events such as floods, earthquakes or fires, where we know that catastrophic events can create strong correlation at the upper tail.</p> <h3 id="d-clayton-copula">D. Clayton copula</h3> <p>Just like the Gumbel copula captures upper tail dependence, the <strong>Clayton copula</strong> captures lower tail dependence. Again, there’s no need to mull over the analytic definition, so let’s just look at the graphical representation of the Clayton copula for $\theta=1$ and see if we can get a feeling for how it behaves.</p> <div class="row justify-content-center" id="fig-6"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Clayton copula with $\theta=1$. </div> <p>We see that just like the Gumbel copula captures upper tail dependence, the Clayton copula captures lower tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the lower left corner $(0,0)$. This is particularly useful when modeling financial data, where losses are often more correlated than gains due to market-wide events such as economic downturns or financial crises.</p> <h3 id="e-summary">E. Summary</h3> <p>In summary, we have seen four important copulas: Gaussian, Student, Gumbel and Clayton. Each of these copulas has its own properties and is useful in different contexts as we shall see in the next section. Below is the four copulas visualized together for comparison:</p> <div class="row justify-content-center" id="fig-7"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Comparison of the four copulas: Gaussian, Student, Gumbel and Clayton. </div> <h2 id="iv-tail-dependence">IV. Tail dependence</h2> <p>The goal of this section is to give you a feeling for what tail dependence is and why it matters. To do so, we will first informally define the concept of tail dependence, then we will see how it can be quantified using the concept of <strong>tail dependence coefficient</strong>, and finally we will see how different copulas capture tail dependence in different ways.</p> <h3 id="a-an-informal-definition-of-tail-dependence">A. An informal definition of tail dependence</h3> <p>Informally, tail dependence refers to the correlation between two random variables <strong>in the extreme tails of their distributions</strong>. Notice that there are two tails (lower and upper), so we have to distinguish between lower tail dependence (correlation in the lower tail) and upper tail dependence (correlation in the upper tail):</p> <ul> <li><strong>Lower tail dependence</strong>: think of it as $\P(\tn{Y goes to its lower tail} \mid \tn{X is at its lower tail})$, i.e. the probability that $Y$ takes on an extremely low value given that $X$ is at an extremely low value.</li> <li><strong>Upper tail dependence</strong>: think of it as $\P(\tn{Y goes to its upper tail} \mid \tn{X is at its upper tail})$, i.e. the probability that $Y$ takes on an extremely high value given that $X$ is at an extremely high value.</li> </ul> <h3 id="b-the-tail-dependence-coefficient">B. The tail dependence coefficient</h3> <p>Let’s now take the above two definitions and formalize them a bit. We can define the <strong>lower tail dependence coefficient</strong> $\lambda_L$ and the <strong>upper tail dependence coefficient</strong> $\lambda_U$ as follows:</p> \[\lambda_L = \lim_{u \searrow 0} \P(Y \leq F_Y^{-1}(u) \mid X \leq F_X^{-1}(u))\] <p>and</p> \[\lambda_U = \lim_{u \nearrow 1} \P(Y \geq F_Y^{-1}(u) \mid X \geq F_X^{-1}(u))\] <p>These coefficients measure the strength of the tail dependence between $X$ and $Y$. If $\lambda_L &gt; 0$ (resp. $\lambda_U &gt; 0$), then there <strong>is</strong> lower (resp. upper) tail dependence. If the coefficients are zero, then there is <strong>no</strong> tail dependence.</p> <p>Intuitively, it should make sense to you that <strong>having no tail dependence is generally bad for modeling</strong>. If we go back to our example of insuring houses in flood-prone areas, and $X$ (resp. $Y$) is the flood level in area A (resp. B), then having no tail dependence means that a flood in one area does not increase the probability of a flood in the other area, which is not what we would expect in practice. On the other hand, having tail dependence means that if a flood occurs in one area, it is more likely that a flood will occur in the other area as well, which is exactly what we want to capture! Likewise, if $X$ (resp. $Y$) is risk of default for company A (resp. B), then having no tail dependence means that a default in one company does not increase the probability of a default in the other company, which we know simply isn’t true in practice. More on that in the next section!</p> <h3 id="c-how-different-copulas-capture-tail-dependence">C. How different copulas capture tail dependence</h3> <p>Now that we have a good understanding of what tail dependence is and how it can be quantified, let’s compare the tail dependence coefficients of the four copulas we introduced earlier. This will nicely complement the intuition we got from their graphical representations in the previous section. Once again, I won’t go into the derivations of these coefficients as it wouldn’t add much to the discussion!</p> <div style="text-align: center;"> <table border="2" style="margin: 0 auto; border-collapse: collapse;"> <caption style="caption-side: bottom; text-align: center; margin-top: 8px;"> Table 1: Tail dependence coefficients for various copulas. </caption> <thead> <tr> <th>Copula</th> <th>λ<sub>L</sub></th> <th>λ<sub>U</sub></th> </tr> </thead> <tbody> <tr> <td>Gaussian</td> <td>0</td> <td>0</td> </tr> <tr> <td>Student-t</td> <td>&gt; 0 (depends on $\nu$)</td> <td>&gt; 0 (depends on $\nu$)</td> </tr> <tr> <td>Gumbel</td> <td>0</td> <td>2 - 2<sup>1/θ</sup></td> </tr> <tr> <td>Clayton</td> <td>2<sup>-1/θ</sup></td> <td>0</td> </tr> </tbody> </table> </div> <p><br/></p> <h2 id="v-the-subprime-mortgage-crisis">V. The subprime mortgage crisis</h2> <p>At this stage you might start having an idea of the link between copulas and the subprime mortgage crisis. Here is a (very) brief recap of what you need to know:</p> <ol> <li>From an individual’s point of view, a mortgage is a loan taken out to buy their house. From a bank’s point of view, a mortgage is an <strong>asset that generates interest payments</strong>, albeit with some risk of default.</li> <li>Because individual mortgages aren’t fit for institutional investors (for a bunch of reasons), banks had the idea of <strong>securitizing</strong> mortgages, i.e. bundling them together into a single financial product called a <strong>mortgage-backed security (MBS)</strong>. This allows banks to sell the MBS to institutional investors, who can then trade them on the financial markets.</li> <li>This begs the question: <em>what is the risk profile of MBS and how should they be priced?</em></li> </ol> <p><strong>Enters copula theory</strong>.</p> <p>Banks already knew how to model the risk profile of <em>individual</em> mortgages, as they had been lending money for decades. However, they had no idea how to model the risk profile of MBS. In mathematical terms: they could model the marginals but not the joint.</p> <p>The easy fix was to assume that the individual mortgages were independent, but even banks recognized that this was a <em>very dangerous</em> assumption: if an economic downturn occurs, it is likely that many homeowners will default on their mortgages at the same time, which means that the individual mortgages clearly aren’t independent. Thus, banks needed a way to model the correlation structure of the default risk of the individual mortgages that make up the MBS.</p> <p>In 2000, David Li, an obscure Chinese quant then working at J.P. Morgan, introduced a mathematically elegant solution to this problem based on the <strong>Gaussian copula</strong>. The idea was to use the Gaussian copula to model the correlation structure of the default risk of the individual mortgages, which would allow banks to price MBS more accurately. His theory was beautiful, simple, and most importantly, <strong>tractable</strong>. Banks adopted it <em>en masse</em>, and it quickly became the de facto standard for pricing MBS.</p> <blockquote> <p>What could possibly go wrong when using a Gaussian copula to model mortgage default correlation? A lot, as it turns out.</p> </blockquote> <p>If you recall the previous section on tail dependence, you should now see the problem with using the Gaussian copula to model the correlation structure of the default risk of individual mortgages: it has <strong>no tail dependence</strong>. This means that it does not capture the fact that in an economic downturn, many homeowners are likely to default on their mortgages at the same time, which is precisely what happened during the subprime mortgage crisis. As a result, the Gaussian copula led banks to underestimate the risk of MBS <strong>by orders of magnitude</strong>. The rest is history.<sup id="fnref:disclaimer"><a href="#fn:disclaimer" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>For a more in-depth analysis of the role of copulas in the subprime mortgage crisis, I highly recommend reading Felix Salmon’s article on the subject. <sup id="fnref:article"><a href="#fn:article" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have seen how copula theory can be used to model the correlation structure between random variables, and how it can be used to capture tail dependence. We have also seen how the Gaussian copula, which was widely adopted by banks to price mortgage-backed securities, led to a catastrophic underestimation of risk during the subprime mortgage crisis due to its lack of tail dependence.</p> <p>On a more optimistic note, keep in mind that despite its somewhat tarnished reputation, copula theory remains a very powerful tool that is still widely used today (only with more care!).</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:blog"> <p>If you’re new to copulas, this blog is a good starting point. <a href="https://bggj.is/">Link</a> <a href="#fnref:blog" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:disclaimer"> <p>Of course, this is a very simplified version of the story, and there are many other factors that contributed to the 2008 crisis, starting per usual with human greed. <a href="#fnref:disclaimer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:article"> <p>Salmon, Felix. “Recipe for Disaster: The Formula That Killed Wall Street.” <em>WIRED</em>, (2009). <a href="https://www.wired.com/2009/02/wp-quant/">Link</a> <a href="#fnref:article" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="statistics,"/><category term="extreme-value-theory"/><summary type="html"><![CDATA[TL;DR: Copulas are a powerful tool for modeling the correlation structure between random variables. We propose an intuition-first introduction to copula theory, culminating in a discussion of the role of copulas in the 2008 subprime mortgage crisis.]]></summary></entry><entry><title type="html">The Magic of Embeddings</title><link href="https://gaetanx21.github.io/blog/2025/embeddings/" rel="alternate" type="text/html" title="The Magic of Embeddings"/><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/embeddings</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/embeddings/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>In this post, I will discuss the magic of embeddings and then move onto the Johnson-Lindenstrauss lemma, which is a fundamental result in linear algebra that illustrates the blessing of dimensionality. I will also give a sketch of the proof of the lemma, which is based on the idea of random projections. Finally, I will briefly mention the LinFormer paper, which proposes a linear time and space complexity self-attention mechanism for transformers based on the JL lemma.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/embeddings/mnist_tsne-480.webp 480w,/assets/img/posts/embeddings/mnist_tsne-800.webp 800w,/assets/img/posts/embeddings/mnist_tsne-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/embeddings/mnist_tsne.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. 3D t-SNE embeddings of MNIST data. <a href="https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487/">Source</a>. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Embeddings have always fascinated me for (at least) three reasons:</p> <p>1) they compactly store large amounts of information (e.g. LLM token embeddings which essentially encapsulate all the subtleties of human language)</p> <p>2) they have meaningful geometric properties (e.g. the dot product encodes similarity, <code class="language-plaintext highlighter-rouge">queen</code>-<code class="language-plaintext highlighter-rouge">king</code>+<code class="language-plaintext highlighter-rouge">man</code>=<code class="language-plaintext highlighter-rouge">woman</code>, etc.)</p> <p>3) they can accommodate different modalities (e.g. CLIP embeddings which can encode both text and image data)</p> <p>This begs the question:</p> <blockquote> <p>How come dense vector representations work so well?</p> </blockquote> <p><strong>As often in machine learning, behind the magic lurks good old linear algebra</strong>. In the case of embeddings, the <em>blessing of dimensionality</em> is at play. To put it simply, the sheer size of the embedding space allows for a lot of flexibility and expressiveness.</p> <p>In a famous paper<sup id="fnref:superposition"><a href="#fn:superposition" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, researchers at Anthropic study the phenomenon of <em>superposition</em> in large language models. They show that the model can learn to represent multiple concepts in a single embedding, which is a direct consequence of the high dimensionality of the space. In particular, they highlight the fact that although a $d$-dimensional space can only hold $d$ orthogonal vectors, if we allow for quasi-orthogonal vectors, we can fit a much larger number of them, which is in fact exponential in $d$! This is a consequence of the <em>Johnson-Lindenstrauss lemma</em>, which we introduce and prove in the next section.</p> <h2 id="ii-the-johnson-lindenstrauss-lemma">II. The Johnson-Lindenstrauss lemma</h2> <blockquote> <p>The Johnson-Lindenstrauss lemma<sup id="fnref:jl"><a href="#fn:jl" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> (1984) or “JL lemma” states that a set of points in a high-dimensional space can be embedded into a lower-dimensional space while preserving pairwise distances approximately.</p> </blockquote> <p>In other words, the JL lemma guarantees the existence of low-distortion embeddings for any finite set of points in a high-dimensional space. This is particularly useful in machine learning, where we often deal with high-dimensional data and need to reduce its dimensionality for various tasks such as visualization, clustering, or classification.</p> <p>The JL lemma can be formally stated as follows:</p> <p><strong>Lemma (Johnson–Lindenstrauss).</strong><br/> Let $\epsilon \in (0, 1)$, $X$ a set of $N$ points in $\mathbb{R}^n$, and consider an integer $k&gt;\frac{8 \ln(N)}{\epsilon^2}$. Then, there exists a linear map $f:\mathbb{R}^n \to \mathbb{R}^k$ such that for all $u,v \in X$:</p> \[(1-\epsilon) \|u-v\|^2_2 \leq \|f(u)-f(v)\|^2_2 \leq (1+\epsilon) \|u-v\|^2_2\] <p>NB: The bound on $k$ is tight i.e. there exists a set $X$ that needs dimension $\Omega(\frac{\ln(N)}{\epsilon^2})$ to be embedded with distortion $\epsilon$.</p> <p>NB: Interestingly enough, the bound on $k$ is independent of the original dimension $n$! This means that in theory, if we have say $N=10^6$ points living in dimension $n=10^{83}$, we can project them down to $k=\frac{8 \ln(10^6)}{0.1^2} \approx 10^4$ dimensions while preserving pairwise distances with a distortion of $10\%$! <sup id="fnref:catch"><a href="#fn:catch" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="iii-proof-of-the-lemma">III. Proof of the lemma</h2> <p>I find the <em>proof</em> of the JL lemma interesting in its own right. It is based on the idea of random projections, which are linear maps that project high-dimensional data onto a lower-dimensional subspace. The key idea is the following: if we randomly choose a projection from the $\R^n$ to $\R^k$, there is a non-zero probability that the projection will preserve the pairwise distances of all the points in $X$ up to a factor of $(1+\epsilon)$. And because this probability is non-zero, it means that such projections must exist!</p> <p>NB: This proof technique is called the “probabilistic method”: we use a probabilistic argument to state a deterministic result. In particular, the lemma does not give us a constructive way to find a working $\R^n \to \R^d$ projection, but rather guarantees that at least one exists.</p> <p>We first present a high-level sketch of the proof, followed by a more rigorous step-by-step derivation.</p> <h4 id="a-sketch-of-the-proof">A. Sketch of the proof</h4> <p>Here I will give a sketch of the proof in two parts:</p> <p>1) I’ll show that if we randomly project a vector $u \in \R^n$ onto a $k$-dimensional subspace $v\in \R^k$ with $k&gt;\frac{8 \ln(N)}{\epsilon^2}$, then we have</p> \[\mathbb{P}(\|v\|^2_2 \in \big[(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2\big]) \geq \frac{2}{N^2}\] <p>2) From this result I will show that if we have $N$ points in $\R^n$, then the probability that all of them are projected into a $k$-dimensional subspace with distortion $\epsilon$ is non-zero, effectively proving the lemma.</p> <h4 id="b-actual-derivation">B. Actual derivation</h4> <p>1) Let $u \in \R^n$ and let $k$ be some integer which we’ll fix later. Consider $P \sim \mathcal{N}(0,1)^{\otimes (k,n)}$ a random projection matrix of from $\R^n$ to $\R^k$ and define $v=\frac{1}{\sqrt{k}}Pu$. For ease of notation, we write $P$ as:</p> \[P = \begin{bmatrix} P_1^T \\ \vdots \\ P_k^T \end{bmatrix}\] <p>It is then clear that $v_i = \frac{1}{\sqrt{k}}P_i^Tu \sim N(0,\frac{|u|^2_2}{k}) \ i.i.d.$ for $i=1,\ldots,k$. As such, we can define $x=\frac{\sqrt{k}}{|u|_2}v$ and we have $x \sim N(0,I_k)$. Consequently, we have:</p> \[\|x\|^2_2 = \frac{1}{k}\frac{\|v\|^2_2}{\|u\|^2_2} \sim \chi^2_k\] <p>From there, we can easily use concentration inequalities on the $\chi^2$ distribution to show that:</p> \[\mathbb{P}(\|v\|^2_2 \in [(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2]) \geq 2e^{-\frac{k}{4}(\epsilon^2-\epsilon^3)}\] <p>We then fix $k&gt;\frac{8 \ln(N)}{\epsilon^2}$ which gives us the desired $\frac{2}{N^2}$ bound.</p> <p>2) Now, let $X=\lbrace x_1,\ldots, x_N \rbrace$ be a set of $N$ points in $\R^n$. The above result applies to all the vectors $u = x_i - x_j$ for all pairs $1\leq i,j \leq N$. Let $E_{\lbrace i,j\rbrace}$ be the event that the projection of the pair $\lbrace x_i,x_j\rbrace$ violates the distortion bound. There are $N(N-1)/2$ $\lbrace i, j \rbrace$ pairs, such that the probability of having at least one of them violate the distortion bound $\epsilon$ is given by:</p> \[p_\text{invalid projection} = \mathbb{P}(\bigcup_{\{i,j\} \in pairs} E_{\{i,j\}}) \leq \sum_{\{i,j\} \in pairs} \mathbb{P}(E_{\{i,j\}}) \leq \frac{N(N-1)}{2}\frac{2}{N^2} = 1-\frac{1}{N}\] <p>Consequently,</p> \[p_\text{valid projection}=1-p_\text{invalid projection} \geq \frac{1}{N} &gt; 0\] <p>Thus, when sampling a random projection $P$ from $\mathcal{N}(0,1)^{\otimes (k,n)}$, we have a non-zero probability that all the points in $X$ are projected into a $k$-dimensional subspace with distortion $\epsilon$.</p> <p>This proves the lemma.</p> <h2 id="iv-linformer">IV. LinFormer</h2> <p>I won’t delve into the many real-life corollaries of the JL lemma, since essentially all linear dimensionality reduction techniques implicitly rely on it.</p> <p>However, I will mention the LinFormer paper<sup id="fnref:linformer"><a href="#fn:linformer" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> which proposes a <strong>linear time and space complexity self-attention mechanism for transformers</strong>. The key idea is to use a low-rank approximation of the attention matrix<sup id="fnref:spectrum"><a href="#fn:spectrum" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>, which can be achieved using random projections. The JL lemma is used by the authors to provide theoretical guarantees for this approximation: they demonstrate that for a given distortion $\epsilon$, there is a corresponding dimension $k&lt;n$<sup id="fnref:n"><a href="#fn:n" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> which ensures that the rank-$k$ approximation induces an $\epsilon$-bounded distortion!</p> <h2 id="conclusion">Conclusion</h2> <p>While the Johnson-Lindenstrauss lemma is not directly “practical” in itself, I believe it is the kind of linear algebra result that is good to keep in mind. In particular, I think it helps build a better intuition of what happens in high-dimensional spaces, where both the curse <em>and</em> the blessing of dimensionality are at play.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:superposition"> <p>Anthropic (2022). <em>Toy Models of Superposition.</em> <a href="https://www.anthropic.com/news/toy-models-of-superposition">Link</a> <a href="#fnref:superposition" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:jl"> <p>Wikipedia. <em>Johnson-Lindenstrauss lemma.</em> <a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">Link</a> <a href="#fnref:jl" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:catch"> <p>The catch, however, is that finding a projection that works would take a lot of time in practice, since this time would scale with the initial dimension $n$. <a href="#fnref:catch" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:linformer"> <p>Wang, S., et al. (2020). <em>Linformer: Self-Attention with Linear Complexity.</em> <a href="https://arxiv.org/abs/2006.04768">Link</a> <a href="#fnref:linformer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectrum"> <p>The paper also studies the spectrum of attention matrices and shows that they are low-rank, which is a key insight for the LinFormer approach. Even more interestingly, they show that as we go deeper in the transformer, the attention matrices become more and more low-rank, which is to say the information becomes more and more compressible! <a href="#fnref:spectrum" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:n"> <p>Here, $n$ is the sequence length. <a href="#fnref:n" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="linear-algebra"/><summary type="html"><![CDATA[TL;DR: Embeddings are so powerful that they can seem almost magical. We go back to the basics (linear algebra) with the Johnson-Lindenstrauss lemma, which illustrates the blessing of dimensionality.]]></summary></entry><entry><title type="html">Adding salt to the Bitter Lesson</title><link href="https://gaetanx21.github.io/blog/2025/bitter-lesson/" rel="alternate" type="text/html" title="Adding salt to the Bitter Lesson"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/bitter-lesson</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/bitter-lesson/"><![CDATA[<p>In this post, I will briefly discuss Richard Sutton’s <em>Bitter Lesson</em> of AI. I will also present a lesser-known counter-argument by Rodney Brooks, and finally I will add my own grain of salt to the discussion with a focus on the signal-to-noise ratio (SNR) of the problem at hand. I will illustrate this idea with two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" sizes="95vw"/> <img src="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sutton's Bitter Lesson, illustrated. <a href="https://horace.io/brrr_intro.html">Source</a>. </div> <h2 id="i-richard-suttons-bitter-lesson">I. Richard Sutton’s Bitter Lesson</h2> <p>Sutton’s <em>Bitter Lesson</em><sup id="fnref:sutton"><a href="#fn:sutton" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> begins with the following statement:</p> <blockquote> <p>“The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> </blockquote> <p>Basically, Sutton’s big idea is that trying to forcefully incorporate human knowledge in AI systems (which was essentially the norm before the deep learning revolution) hinders progress. Instead, leveraging vasts amounts of compute (and data) is the way to go. This implies that any attempt to inject human ingenuity into AI systems is doomed to fail, hence the “bitter” lesson. In his lesson, Sutton gives several good examples of this phenomenon, for instance in computer vision, where models using complicated human-designed features (e.g., SIFT) were quickly outperformed by deep learning methods that <em>learned</em> features directly from data.</p> <p>In today’s age of transformer models scaling up to trillions of parameters, Sutton’s lesson seems more relevant than ever, and some veteran NLP researchers have certainly felt bitter seeing their carefully handcrafted models being outperformed by large language models (LLMs) trained on (somewhat random) internet text. Companies building LLMs certainly have reasons to believe in the bitter lesson. Rumor has it that memorizing Sutton’s article is part of OpenAI engineers’ work schedule<sup id="fnref:openai"><a href="#fn:openai" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Funnily enough, OpenAI itself got bitter-lessoned in 2024 when it created a fine-tuned version of its then-flagship <code class="language-plaintext highlighter-rouge">o1</code> model specifically for competitive programming, <code class="language-plaintext highlighter-rouge">o1-ioi</code>, which ended up being uniformly worse than the firm’s next-generation general-purpose model, <code class="language-plaintext highlighter-rouge">o3</code>.</p> <h2 id="ii-rodney-brooks-better-lesson">II. Rodney Brooks’ <em>Better</em> Lesson</h2> <p>A week after Sutton’s post, Rodney Brooks published a blog post titled <em>A Better Lesson</em><sup id="fnref:brooks"><a href="#fn:brooks" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> in which he essentially argues that Sutton is wrong. As he carefully put it:</p> <blockquote> <p>“I think Sutton is wrong for a number of reasons.”</p> </blockquote> <p>Brooks lists a few reasons why he believes Sutton’s lesson is wrong. His core thesis is that AI systems are still imbued with human knowledge, only now it is hidden in the choice of model architectures, and to a lesser extent in the curated datasets and the complex training pipelines. Besides, he argues that the current trend of scaling up models is not sustainable, notably because Moore’s law is slowing down and frontierAI models’ carbon footprint is becoming a cause for concern.</p> <h2 id="iii-my-grain-of-salt-snr-matters">III. My grain of salt: SNR matters</h2> <p>My (humble) view is that Sutton’s Bitter Lesson is generally a good heuristic for AI research, but it should be taken with a grain of salt (!).</p> <blockquote> <p>“I believe that the signal-to-noise ratio (SNR) of the problem at hand matters a lot.”</p> </blockquote> <p>I will illustrate this idea on two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <h4 id="a-quantitative-finance">A. Quantitative Finance</h4> <p>Financial markets are notoriously noisy, as they are complex systems in which a myriad of heterogeneous agents interact with different objectives. As such, it’s well-known that the SNR of financial data is extremely low. For that reason, robustness is a key concern in model selection and most market practitioners end up relying on the good ol’ linear regression model, albeit augmented with a few hand-crafted biases. Although the industry is catching up with the latest AI trends (e.g. using LLMs for sentiment analysis), the SNR of financial data is so low that it is hard to imagine a future in which Sutton’s lesson will be fully applicable. In fact, I would argue that <strong>the SNR of financial data is so low that it is not even clear whether Sutton’s lesson applies at all</strong>. Can a 1B-parameter model trained on 1TB of (crappy) data outperform a 10-parameter linear model trained on 1MB of data? I don’t know, but I wouldn’t be surprised if it didn’t!</p> <h4 id="b-computational-biology">B. Computational Biology</h4> <p>Data in computational biology is also very noisy, but for different reasons. Here I will focus on RNA-seq data<sup id="fnref:rnaseq"><a href="#fn:rnaseq" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, which in a nutshell (within a nutshell) is tabular data of the form <code class="language-plaintext highlighter-rouge">n_cells x n_genes</code> where each row gives you for a given cell the expression level of the 20k or so (human) genes. As it stands, RNA-seq data has several issues, the most obvious one being that it is very sparse<sup id="fnref:sparse"><a href="#fn:sparse" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> and hence difficult to work with. More importantly, RNA-seq data is “dirty” in the sense that it is collected in a wet lab by a human being (i.e. not a machine) who has their own way of doing things<sup id="fnref:law-compbio"><a href="#fn:law-compbio" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. This leads to what is called “batch effects”, which are systematic differences between data collected in different experiments. In the context of NLP, this is like if I told you that the text data scraped on Wikipedia didn’t follow the same distribution as the text data scraped on Reddit. That would certainly make matters difficult, right?</p> <p>But there is a much deeper problem with RNA-seq data, which is that <strong>gene expression fundamentally isn’t a clean signal</strong>, unlike text data in (curated) web corpuses. The key idea is that life as we know it is literally the result of a random process left unchecked for 4 billion years, in which the fittest pass their genes to the next generation. This explains why organisms are so monstrously complex (unlike computer systems, which are trivial in comparison), but also extremely robust. A good example of this is the notion of <em>biological pathways</em>, which can roughly be described as “a series of interactions of molecules in a cell that leads to a certain product or a change in the cell”<sup id="fnref:pathway"><a href="#fn:pathway" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. In computer systems, pathways are bijective: Function A triggers Function B, and that’s it. In an organism, Gene A may trigger production of Protein B, but it may also trigger production of Protein C. And guess what, Gene C can also create Protein B under certain conditions. Oh and wait, the goal of creating Protein B was to produce a certain molecule, but it turns out that this molecule can also be produced by Gene D! And so on and so forth. In other words, biological pathways are not bijective, and this is a super important because <strong>redundancy yields robustness</strong>. For instance, if one pathway producing glucose in a cell breaks down for some reason, the cell can still produce glucose through other pathways that were created through random mutations, so it doesn’t die! The most critical components of life, such as the immune system, have myriads of redundant pathways, which makes them extremely robust to perturbations. As such, the current attempts<sup id="fnref:goldrush"><a href="#fn:goldrush" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> to emulate the dazzling successes of transformer models in NLP by training large transformer architectures on RNA-seq data may ultimately prove futile, as the SNR of the data may simply be too low.</p> <h2 id="conclusion">Conclusion</h2> <p>The Bitter Lesson is a great heuristic for AI research, but it must be taken with a grain of salt. In particular, the SNR of the problem at hand matters a lot. In domains such as quantitative finance and computational biology, the SNR is so low that it is not even clear whether Sutton’s lesson applies at all. In these domains, human biases and ingenuity are still critical to building effective AI systems.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:sutton"> <p>Sutton, R. (2019). <em>The Bitter Lesson.</em> <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">Link</a> <a href="#fnref:sutton" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:openai"> <p>Medium (2024). <em>The Legendary OpenAI Engineer’s Must-Have Classic: A Bitter Lesson.</em> <a href="https://ai-engineering-trend.medium.com/the-legendary-openai-engineers-must-have-classic-a-bitter-lesson-1948e6ac6c4a">Link</a> <a href="#fnref:openai" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:brooks"> <p>Brooks, R. (2019). <em>The Better Lesson.</em> <a href="https://rodneybrooks.com/a-better-lesson/">Link</a> <a href="#fnref:brooks" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rnaseq"> <p>Wikipedia. (2023). <em>RNA-Seq.</em> <a href="https://en.wikipedia.org/wiki/RNA-Seq">Link</a> <a href="#fnref:rnaseq" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sparse"> <p>Not only because most genes are not expressed in most cells, but also because genes with low expressions may not be captured during RNA sequencing. <a href="#fnref:sparse" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:law-compbio"> <p>A colleague of mine with deep expertise in the field quickly taught me that “the first rule of computational biology is that everyone does things their own way”. <a href="#fnref:law-compbio" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:pathway"> <p>Wikipedia. (2023). <em>Biological pathway.</em> <a href="https://en.wikipedia.org/wiki/Biological_pathway">Link</a> <a href="#fnref:pathway" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:goldrush"> <p>Given the record amounts invested, one might even call it a <em>gold rush</em>. <a href="#fnref:goldrush" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="meta,"/><category term="learning"/><summary type="html"><![CDATA[TL;DR: The "Bitter Lesson" of AI states that general methods that leverage computation are ultimately the most effective to build powerful AI systems. We propose to qualify this lesson by introducing the notion of signal-to-noise ratio (SNR) of the problem at hand. In domains such as quantitative finance and computational biology, I believe that the SNR is so low that Sutton's lesson may not directly apply.]]></summary></entry><entry><title type="html">The Curty &amp;amp; Marsili Forecasting Game</title><link href="https://gaetanx21.github.io/blog/2025/curty-marsili-game/" rel="alternate" type="text/html" title="The Curty &amp;amp; Marsili Forecasting Game"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/curty-marsili-game</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/curty-marsili-game/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>In this post, we present Curty &amp; Marsili’s forecasting game, a simple model that captures how herding behavior can lead to non-trivial opinion outcomes, in particular <strong>phase coexistence</strong> and <strong>ergodicity breaking</strong> under certain conditions. After motivating the study of herding, we formally introduce the Curty &amp; Marsili game and propose a mathematical analysis of its key features. We then perform <strong>Agent-Based Model</strong> (ABM) simulations of the game to validate our theoretical predictions. Finally, we discuss how the game can converge to a Nash equilibrium where fundamentalists and herders coexist and the system is efficient.</p> <hr/> <h2 id="i-motivation">I. Motivation</h2> <p>Herding is a widespread phenomenon in society: people often imitate or follow the actions of others, whether it’s in fashion trends, product adoption, or even protests. In finance, herding can lead to anomalous fluctuations in asset prices. The alternative to herding is to gather (private) information and make decisions based on one’s own analysis. Curty &amp; Marsili’s forecasting game<sup id="fnref:curtymarsili"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a simple ABM which precisely focuses on this tension between <strong>individual forecasting</strong> and <strong>collective herding</strong>.</p> <p>In a nutshell, the question Curty &amp; Marsili tried to answer is: <strong>what’s more efficient, following the crowd or relying on your own judgment?</strong> We’ll see that the answer is not straightforward and can depend on the parameters of the game. In essence, we’ll find that herding can be a sound strategy, but if the proportion of followers in the market becomes too large, herding becomes a dangerous strategy.</p> <p><em>Let’s now present in details of the Curty &amp; Marsili forecasting game.</em></p> <h2 id="ii-the-game">II. The Game</h2> <p>The Curty &amp; Marsili requires the following ingredients:</p> <ul> <li>$N\gg 1$ agents who must make a binary forecast (e.g., election outcome, buy or sell, protest or not, etc.)</li> <li>A fraction $z\in[0,1]$ of agents are <strong>fundamentalists</strong> $F_i$ who rely solely on their private information. They are correct with probability $p&gt;\frac{1}{2}$ (i.e., they have an edge). Their forecast is fixed once and for all and crucially, fundamentalists’ forecasts are mutually independent.</li> <li>The remaining fraction $1-z$ are <strong>herders</strong> $H_i$ who each have a fixed group $G_i$ of $M$ agents which they follow. Their action is determined by majority voting within their group (note that group size $M$ is odd to avoid draws). Importantly, note that groups may include both fundamentalists and herders.</li> </ul> <p>The game then dynamically evolve according to the following rules:</p> <ul> <li>At each time step $t$, all herders are chosen one by one (in a random order) and update their forecast based on the majority opinion of their group $G_i$. (note that herders’ initial forecast are i.i.d. random i.e. correct with probability $\frac{1}{2}$)</li> <li>The fundamentalists $F_i$ do not change their forecast over time. (reflecting their reliance on private information)</li> <li>The process is repeated until convergence to a fixed point (i.e. herders are all following the majority opinion of their group).</li> </ul> <p>The question then is to study the final probability $q$ that a herder makes the correct forecast, computed as the fraction of herders who forecast the correct outcome after the game has converged. More precisely, we want to study $q(z)$, the final probability of a herder making the correct forecast as a function of the fraction of fundamentalists $z$ in the market.</p> <p><em>We now delve into the mathematical analysis of the game.</em></p> <h2 id="iii-mathematical-analysis">III. Mathematical Analysis</h2> <p>Let’s introduce two important notations:</p> <ul> <li>$q_t$ is the probability that a herder makes the correct forecast at time $t$.</li> <li>$\pi_t$ is the probability that a randomly chosen agent makes the correct forecast at time $t$.</li> </ul> <p>Since agents are either fundamentalists or herders, we have the following static equation: \(\begin{equation} \label{eq:static} \pi_t = zp + (1-z)q_t. \end{equation}\)</p> <p>In addition, a herder will make the correct forecast at time $t+1$ if and only if the majority of his group $G_i$ makes the correct forecast at time $t$, i.e. at least $\frac{M+1}{2}$ agents in the group make the correct forecast. This leads to the following dynamic equation: \(\begin{equation} \label{eq:dynamic} q_{t+1} = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} \pi_t^k (1-\pi_t)^{M-k}. \end{equation}\)</p> <p>Combining \eqref{eq:static} and \eqref{eq:dynamic}, we can write the evolution of $q_t$ as: \(\begin{equation} \label{eq:evolution} q_{t+1} = F_z(q_t). \end{equation}\)</p> <p>where $F_z(q) = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} (zp + (1-z)q)^k (1-(zp+(1-z)q))^{M-k}$.<sup id="fnref:condorcet"><a href="#fn:condorcet" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>We can then compute fixed points $q^*(z)$ for the evolution equation \eqref{eq:evolution} for various values of $z\in[0,1]$. The results are illustrated in <a href="#fig-1">Figure 1</a>.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/fixed_points-480.webp 480w,/assets/img/posts/curty_marsili_game/fixed_points-800.webp 800w,/assets/img/posts/curty_marsili_game/fixed_points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/fixed_points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Fixed points of the evolution equation $q_{t+1} = F_z(q_t)$ for various values of $z$. Note the critical value $z_c\simeq \frac{1}{2}$ separating the two regimes. Each curve corresponds to a different fixed point: green for $q_+^*(z)$, blue for $q_-^*(z)$, and red for $q_u^*(z)$ the unstable fixed point. </div> <p>We distinguish two regimes, separated by a critical value $z _ c\simeq \frac{1}{2}$:</p> <ul> <li>for $z&gt;z _ c$ i.e. when there are mostly fundamentalists, there is a single (stable) fixed point $q _ +^ * (z)$. Interestingly, $q^ * (z)&gt;p$ in this regime, meaning that herders are on average more accurate than fundamentalists. As $z$ decreases (while staying above $z_c$), the performance of herders gets even better! This can seem somewhat surprising, but in fact results from the fact that more herders means herders will have more herders in their group $G_i$, which in turn increases their forecast accuracy since in this regime herders are more accurate than fundamentalists.</li> <li>for $z&lt;z _ c$ i.e. when there are mostly herders, two new fixed points appear, both under the line $q=\frac{1}{2}$ which means that these fixed points are bad for herders. Note that $q _ +^* (z)$ keeps increasing as $z$ decreases, while $q _ -^* (z)$ decreases. The unstable fixed point $q _ u^* (z)$ is also shown in red. Numerical simulations show that the system will converge to either $q _ +^* (z)$ or $q _ -^* (z)$ depending on the initial conditions. In fact, the unstable fixed point $q _ u^* (z)$ acts as a <em>separatrix</em> between the two regimes. Thus the initial condition $q_0$ will determine the final state of the system: if $q _ 0&gt;q _ u^* (z)$, the system will converge to $q _ +^* (z)$, otherwise it will converge to $q _ -^* (z)$. This will be useful in the last section where we compare $\langle q \rangle$ to $p$ to find the Nash equilibrium.</li> </ul> <p>What’s interesting is the <strong>phase coexistence</strong> in herding regime $z&lt;z_ c$: if the system converges towards $q_ -^<em>(z)$, then the majority of herders will make the wrong forecast; likewise, if the system converges towards $q_ +^</em>(z)$, the majority of herders will make the correct forecast. This is a clear example of <strong>ergodicity breaking</strong> where the system is stuck in one of the two phases, depending on the initial conditions $q_0$. In the last section we take into account the distribution $q_0\sim N(\frac{N}{2},\frac{N}{4})$ to compute the probability $p_ -$ of the system converging to $q_-$ (and similarly $p_+$ for $q_+$) so we can finally compute the average probability $\langle q \rangle$ of a herder making the correct forecast and compare it to fundamentalists’ accuracy $p$.</p> <p><em>Now that we’ve analyzed the theoretical aspects of the game, let’s move on to simulations to check if its consistent.</em></p> <h2 id="iv-abm-simulation">IV. ABM Simulation</h2> <p>The ABM is pretty straightforward here, with two agents classes (fundamentalists and herders) and at each step of the system, herders are picked one by one in a random order and update their forecast based on the majority opinion of their group. We iterate until convergence of the system i.e. herders’ opinions are stable. We use the <code class="language-plaintext highlighter-rouge">mesa</code> python library<sup id="fnref:mesa"><a href="#fn:mesa" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> which helps build ABMs very easily.</p> <p>Throughout the simulations, we use the following parameters:</p> <ul> <li>$N=1000$ agents in total, with $z\in[0,1]$ the fraction of fundamentalists.</li> <li>$p=0.52$ the probability that a fundamentalist makes the correct forecast.</li> <li>$M=7$ the size of the groups of herders. Note that modifying these parameters will result in quantitative but not qualitative changes in the outcomes.</li> </ul> <p>We run simulations for various values of $z\in[0,1]$ and for each simulation we record $q_t$ at each time step $t$ of the system. We especially care about the final probability $q_\text{final}$ that a herder makes the correct forecast, which is simply the fraction of herders who forecast the correct outcome after the game has converged. We observe the following:</p> <ul> <li>in the low-herding regime $z&gt;z_c$, $q_\text{final}$ is always above $p$ and very close to $q_+$.</li> <li>in the high-herding regime $z&lt;z_c$, $q_\text{final}$ is either close to $q_-$ or $q_+$ depending on the initial conditions. This is consistent with the phase coexistence and ergodicity breaking observed in the theoretical analysis.</li> </ul> <p>In <a href="#fig-2">Figure 2</a>, we plot $q_t$ over time for $n=100$ simulations for various values of $z$. We observe that the system converges to a fixed point after a few time steps, and the final probability $q_\text{final}$ is consistent with the theoretical predictions. As expected, we find that:</p> <ul> <li>in the high-herding regime $z&lt;z_c$, the system can converge to either $q_-$ or $q_+$ depending on the initial conditions, and we have $q_- \simeq 0$ and $q_+ \simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>.</li> <li>in the low-herding regime $z&gt;z_c$, the system converges but toward a wider spectrum of values, which are above $p$ on average i.e. $\langle q_\text{final} \rangle &gt; p$.</li> </ul> <p>We see in particular that the richness of the phase transition towards $z\simeq z_c$ is well captured by the ABM simulations.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/runs-480.webp 480w,/assets/img/posts/curty_marsili_game/runs-800.webp 800w,/assets/img/posts/curty_marsili_game/runs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/runs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Evolution of $q_t$ over time for $n=100$ simulations for various values of $z$. The system converges to a fixed point after a few time steps. The ensemble final probability $q_\text{final}$ is indicated by the y-tick on the right. </div> <p>We notice that $q_\text{final}(z)$ seems to increase with $z&lt;z_c$ then decrease with $z&gt;z_c$. To investigate this behavior, we plot the average final probability $\langle q_\text{final} \rangle$ over $n=100$ simulations for various values of $z$. The results are given in In <a href="#fig-3">Figure 3</a>. Interestingly, we see that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$, except near the $z=0$. This implies that herding is always a better strategy than being a fundamentalist… <strong>except when there are too many herders in the system!</strong></p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/q_z-480.webp 480w,/assets/img/posts/curty_marsili_game/q_z-800.webp 800w,/assets/img/posts/curty_marsili_game/q_z-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/q_z.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Average final probability $\langle q_\text{final} \rangle$ over $n=1000$ simulations for various values of $z$. Note that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$ except near $z=0$, meaning that herding is a better strategy than being a fundamentalist except when virtually everyone adopts the herding strategy! </div> <p><em>Let’s finish by showing how letting $z$ fluctuate naturally leads to a Nash equilibrium.</em></p> <h2 id="v-nash-equilibrium">V. Nash Equilibrium</h2> <p>We have seen that if there aren’t too many herders (i.e. if $z$ isn’t too low), then $\langle q_\text{final}(z) \rangle &gt; p$, i.e. herders are more accurate than fundamentalists on average. In this case, it is rational for fundamentalist agents to become herders, which means that $z$ will decrease. However there cannot be too many herders, since in the limit $z\to 0$ we have $q_\text{final}=\frac{1}{2}$ as all agents are herders and thus there is not information (“edge”) in the system. We thus expect the system to self-organize until the proportion $z$ of fundamentalists fluctuates around a critical value $z^\dagger$ such that $\langle q_\text{final}(z^\dagger) \rangle = p$. This is the Nash equilibrium of the system, where fundamentalists and herders coexist and the system is efficient. (or arbitrage-free in the context of financial markets)</p> <p>We can show <sup id="fnref:curtymarsili:1"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> that the Nash equilibrium $z^\dagger$ is given by $z^\dagger \sim N^{1/2}$ where $N$ is the total number of agents. This means that most agents are followers and there is a little minority of $\sqrt{N}$ fundamentalists feeding information (“edge”) to the system.</p> <p>Additionally, note that since $z^\dagger$ is very small, we have $q_-\simeq 0$ and $q_+\simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>. Then, if we denote $p_-$ (resp $p_+$) the probability that the system converges to $q_-$ (resp $q_+$) given the initial conditions, we have $\langle q_\text{final} \rangle = p_- q_- + p_+ q_+$, which at the Nash equilibrium rewrites $p=p_+$, meaning that the probability of the herder mass to converge to the truth ($q_+$) is $p$, as if they represented a single fundamentalist agent!</p> <h2 id="conclusion">Conclusion</h2> <p>Despite its simplicity, the Curty &amp; Marsili game suffices to display non-trivial behavior such as phase coexistence and ergodicity breaking. The game is a good illustration of how herding can be a good strategy… until too many agents adopt it and the whole herding population starts behaving like a single agent which is correct with probability $\langle q_\text{final} \rangle$. Finally, if we let agents switch strategy, $z$ will naturally converge to the efficient state $z^\dagger$ where $\langle q_\text{final} \rangle = p$ such that no strategy has an edge over the other. We find that $z^\dagger \sim N^{1/2}$, meaning that <strong>it is optimal (in a game-theoretic sense) that most agents are followers and a little minority of fundamentalists is feeding information to the system</strong>.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:curtymarsili"> <p><em>Phase coexistence in a forecasting game.</em> Curty, P. &amp; Marsili, M. (2008) <a href="https://wrap.warwick.ac.uk/id/eprint/1769/1/WRAP_Curty_fwp05-15.pdf">PDF</a> <a href="#fnref:curtymarsili" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:curtymarsili:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:condorcet"> <p>Note the similarity with the <strong>Condorcet Jury Theorem</strong>, where the probability of a correct decision by a majority vote increases with the number of jurors and their individual accuracy. <a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem">wikipedia</a> <a href="#fnref:condorcet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mesa"> <p><em>Mesa: An Agent-Based Modeling Framework in Python.</em> <a href="https://mesa.readthedocs.io/">mesa.readthedocs.io</a> <a href="#fnref:mesa" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="agent-based-model,"/><category term="game-theory"/><summary type="html"><![CDATA[TL;DR: When faced with a forecasting task, one can either seek information or follow the crowd. The Curty & Marsili game stacks fundamentalists against herders in a binary forecasting task, revealing phase coexistence and ergodicity breaking under certain conditions. We propose a theoretical study of the game's behavior and validate it through ABM simulations.]]></summary></entry><entry><title type="html">Listening to the Market Mode</title><link href="https://gaetanx21.github.io/blog/2025/market-mode/" rel="alternate" type="text/html" title="Listening to the Market Mode"/><published>2025-02-12T00:00:00+00:00</published><updated>2025-02-12T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/market-mode</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/market-mode/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>We first motivate the use of Principal Component Analysis (PCA) on returns to extract the market mode in equities. This mode is crucial for understanding the market risk and comparing it against other risks. We then do a (very) quick recap on Random Matrix Theory (RMT), which provides a theoretical framework for understanding the eigenspectrum of random matrices. Finally, we apply PCA on S&amp;P 500 components to extract the market mode and we monitor its evolution over time, drawing a comparison with the VIX index.</p> <h2 id="motivation">Motivation</h2> <p>Among the various asset classes (e.g., equities, bonds, commodities), equities tend to provide the highest returns in absolute terms (i.e. not adjusted for risk). Equities are exposed to a multitude of risk factors, with <strong>market risk</strong> being the dominant one<sup id="fnref:CAPM"><a href="#fn:CAPM" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. As such, understanding the market risk and how much of the variance it explains is crucial for risk management and portfolio construction.</p> <p>In a nutshell, the question we want to answer is the following: <strong>can we measure the market risk and compare its weight against other risks?</strong></p> <p>Perhaps the simplest approach to gauge market risk is to look at ready-made proxies such as the <strong>VIX index</strong><sup id="fnref:VIX"><a href="#fn:VIX" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. However, the method of computing the VIX is debatable and may not capture the market risk accurately. A more data-driven approach is to extract the market mode from market returns using PCA, as explained in the next section.</p> <h2 id="pca-on-returns--statistical-factor-model">PCA on Returns = Statistical Factor Model</h2> <p>In a nutshell, a <strong>factor model</strong> describes the variance observed in a set of correlated variables (in our case, stock returns) using a smaller number of <strong>unobserved factors</strong>, which we hope to be more or less independent &amp; more or less interpretable. The idea is to decompose the observed variables $X_i$ as linear combinations of the factors $F_k$ plus some idiosyncratic noise $\varepsilon_i$:</p> \[X_i = \sum_k \beta_k^{(i)} F_k + \varepsilon_i\] <p>where $\beta_k^{(i)}$ is the (factor) loading of the $i$-th asset on the $k$-th factor.</p> <p>Now, the hard part is to find <strong>good factors</strong>. One approach is to simply purchase them from vendors like MSCI (Barra models) who gather a lot of data and knowledge to build these factors. Another (cheaper &amp; more transparent) approach is to extract them via PCA. In this case, the factors are obtained as the eigenvectors of the correlation matrix of returns. This is nice for several reasons:</p> <ul> <li>eigenvectors are orthogonal<sup id="fnref:spectral"><a href="#fn:spectral" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, meaning our factors are uncorrelated;</li> <li>eigenvalues directly give us the amount of variance explained by each factor;</li> <li>the factors are interpretable as they are linear combinations of the original variables.</li> </ul> <p>Note that since the factors are obtained in a purely data-driven fashion (without any human/economic prior), we call this approach a <strong>statistical</strong> factor model, as opposed to classical factor models like the CAPM or the Fama-French Three-Factor Model.</p> <p><em>Before trying out PCA on real data, let’s quickly brush up on Random Matrix Theory (RMT), which provides a neat theoretical framework for understanding the eigenspectrum of correlation matrices.</em></p> <h2 id="brush-up-on-random-matrix-theory-rmt">Brush Up on Random Matrix Theory (RMT)</h2> <p>Consider a $T \times N$ matrix $X$ filled with i.i.d. Gaussian entries $X_{ij} \sim \N(0,\sigma^2)$. Typically, $X$ is called the <strong>design matrix</strong> and each one of the $T$ rows corresponds to one observation of the $N$ variables of interest. In our case, the variables are the daily close-to-close returns of the $N=500$ stocks in the S&amp;P 500 index.</p> <p>If we want to study the correlation between the variables, we first compute a standardized version of the design matrix $\tilde{X}$ by subtracting the mean and dividing by the standard deviation for each column. The sample correlation matrix is then given by $C = \frac{1}{T} \tilde{X}^T \tilde{X}$.</p> <p>Finally, $C$ is real symmetric so we know from the spectral theorem that it can be diagonalized in an orthonormal basis of eigenvectors. In fact $C$ is also positive semi-definite, so all its eigenvalues are non-negative.</p> <h4 id="marchenko-pastur-theorem">Marchenko-Pastur Theorem</h4> <p>RMT studies the behavior of the eigenvalues of $C$ when $T,N \to \infty$ with $Q = T/N$ fixed. The main result is the <strong>Marchenko-Pastur (MP) theorem</strong>:</p> \[L_N(\lambda) = \frac{1}{N} \sum_{i=1}^N \delta(\lambda - \lambda_i) \xrightarrow[N,T \to \infty]{\mathcal{W}} \mathbb{P}_\text{MP}(\lambda) = \frac{Q}{2\pi \sigma^2} \frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda} \mathbb{1}_{[\lambda_-, \lambda_+]}(\lambda)\] <p>where $\lambda_{\pm} = \sigma^2(1\pm\sqrt{\frac{1}{Q}})^2$ are the limiting bounds of the spectrum.</p> <p>The MP theorem tells us that the empirical spectral density of the correlation matrix $L _ N(\lambda)$ converges (weakly in distribution) toward the MP distribution $\mathbb{P} _ \text{MP}$ as $T,N \to \infty$.</p> <p>This is quite remarkable: we could have expected the eigenvalues to be unbounded as $T,N \to \infty$, but RMT tells us that they are actually bounded and gives us the exact form of the limiting distribution.</p> <p>In fact, we can relax some hypotheses and the MP theorem will still hold, though convergence may be (significantly) slower. For example, the entries of $X$ don’t have to be Gaussian. This is important in our case because we know that Gaussianity is a strong assumption for financial data. In practice returns have fat tails and are often skewed. A Student-distribution is already a much better model for returns. <em>Good news, MP still holds for Student-distributed entries!</em></p> <h4 id="link-with-pca">Link with PCA</h4> <p>Now, what does this have to do with PCA? Well, the MP theorem tells us that the eigenvalues of the correlation matrix are bounded and distributed according to a known law. This is useful because it allows us to detect the presence of <strong>signal</strong> in the data. If the eigenvalues are significantly larger than the MP bounds, then we can say that the data contains some structure that is not due to randomness. On the contrary, eigenvalues inside the “noise band” defined by the MP law are considered to be due to randomness. Thus, <strong>we can use the MP theorem to filter out noise and extract only the significant factors from the data</strong>. In particular, when doing PCA on market returns, one eigenvalue will stand out from all the others: it represents the dominant variance component due to the market, and as such we call it the <strong>market mode</strong>. It is approximately equally distributed between all the stocks and is the most important factor in the data.</p> <p>In this last section, we illustrate the above ideas through an experiment on US equities. Specifically, we compute rolling PCAs on the correlation matrix of S&amp;P 500 components and analyze the behavior of the market mode over time.</p> <h2 id="experiment-on-us-equities">Experiment on US Equities</h2> <p>To run our experiment, we first need some data. We chose US equities because they are the most liquid and it’s easy to get clean data. We fetched the daily close-to-close returns of the S&amp;P 500 components from Yahoo Finance using the <code class="language-plaintext highlighter-rouge">yfinance</code> Python package. We considered the period 2000–2025<sup id="fnref:sp500"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <h4 id="pca-on-2020-2024-returns">PCA on 2020-2024 returns</h4> <p>Let’s begin by computing the correlation matrix of the daily returns of the S&amp;P 500 components for the period 2020–2024. We then compute the eigenvalues of the correlation matrix and plot them against the MP bounds. The results are shown in <a href="#fig-1">Figure 1</a>. Importantly, <u>the largest eigenvalues are outside the plot</u> for better visibility. There is only ~10 of them, but they are much larger than the rest. <strong>In particular, the first principal component stands out from all the others: it represents the market mode.</strong> The other significant eigenvalues are due to sectoral correlations, which are also interesting to study but outside the scope of this post.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_true-480.webp 480w,/assets/img/posts/market-mode/mp_true-800.webp 800w,/assets/img/posts/market-mode/mp_true-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_true.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024. The largest eigenvalues are outside the plot. </div> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/dist_eigvec_mode-480.webp 480w,/assets/img/posts/market-mode/dist_eigvec_mode-800.webp 800w,/assets/img/posts/market-mode/dist_eigvec_mode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/dist_eigvec_mode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Distribution of the eigenvector of the market mode as well as a noisy mode for the period 2020–2024. Note how the market mode is approximately equally distributed between all the stocks whereas the noisy mode follows a normal distribution, which makes sense because it has no information and thus must maximize entropy. </div> <p>As an extra step, we can shuffle the returns to destroy the correlation structure and then recompute the eigenvalues. The results are shown in <a href="#fig-3">Figure 3</a>. Notice how all the eigenvalues are now neatly inside the MP bounds, which confirms that the structure in the data is due to correlations and not randomness.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_shuffled-480.webp 480w,/assets/img/posts/market-mode/mp_shuffled-800.webp 800w,/assets/img/posts/market-mode/mp_shuffled-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_shuffled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024 after shuffling the returns. All the eigenvalues are now inside the MP bounds and indeed follow the MP distribution. </div> <h4 id="rolling-pca-on-2000-2024-returns">Rolling PCA on 2000-2024 returns</h4> <p>Now that we’ve seen how PCA works on a single sample of market returns, let’s apply it to a rolling window of the daily returns of the S&amp;P 500 components for a large period of time. The idea is to monitor the evolution of the ratio $\lambda_\text{max} / \sum_i \lambda_i$ over time, where $\lambda_\text{max}$ is the largest eigenvalue (market mode) and $\lambda_i$ are the other eigenvalues. This ratio gives us an idea of how much of the variance is explained by the market mode. Intuitively, we expect this ratio to be high during times of uncertainty / fear / crisis as the market becomes even more important in driving returns. To check this hypothesis, we compare the ratio to the VIX index.</p> <p>We’ll be looking at the period 2000–2024, which includes the 2008 financial crisis and the 2020 COVID-19 pandemic. We will take 6-month rolling windows with a 1-month step size and compute the ratio $\lambda_\text{max} / \sum_i \lambda_i$ for each window. Note that for technical reasons<sup id="fnref:sp500:1"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, we only consider the top 420 companies in the S&amp;P 500 index (ranked by daily trading volume) instead of the full 500. However taking the top 420 companies or top 500 companies doesn’t change the results significantly<sup id="fnref:proof"><a href="#fn:proof" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.</p> <p>The results are shown in <a href="#fig-4">Figure 4</a>. The ratio $\lambda_\text{max} / \sum_i \lambda_i$ is plotted in green and the VIX index is plotted in red. We can see that the two series are quite correlated, which confirms our intuition. In particular, we see that the ratio spikes during the 2008 financial crisis, and the 2020 COVID-19 pandemic. This is a nice result as it shows that PCA can indeed capture the market mode and that it is a good proxy for market risk.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/TOP420-480.webp 480w,/assets/img/posts/market-mode/TOP420-800.webp 800w,/assets/img/posts/market-mode/TOP420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/TOP420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Ratio $\lambda_\text{max} / \sum_i \lambda_i$ (green) and VIX index (red) over the period 2000–2024. The two series are quite correlated, which confirms our intuition that the market mode is a good proxy for market risk. </div> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen how PCA can be used to extract the market mode from equities’ return data. This mode is crucial for understanding the market risk and weighing it against other risks. We’ve also seen how Random Matrix Theory provides a theoretical framework for understanding the eigenspectrum of correlation matrices. Finally, we’ve applied PCA on S&amp;P 500 individual returns to extract the market mode and we’ve analyzed its behavior over time, drawing a comparison with the VIX index.</p> <p>Note that the market mode is not the only factor that matters. If we stick to the PCA approach (statistical factor model), there are several other eigenvalues outside the MP noise band. These eigenvalues and the corresponding eigenvectors correspond to sectors of the US economy (e.g., tech, finance, utilities). They too are risk factors, albeit less important than the market mode. In practice, depending on the context, one may want to consider these factors, for instance to build a sector-neutral portfolio.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:CAPM"> <p>The Capital Asset Pricing Model (CAPM) is the most simple factor model as it relies on the market factor only. In a nutshell, it posits that the expected return of an asset is linearly related to the expected return of the market depending on the asset’s correlation with the market, known as the beta coefficient. <a href="#fnref:CAPM" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:VIX"> <p>The VIX index is a measure of the market’s expectation of volatility over the next 30 days. It is calculated using the implied volatility of S&amp;P 500 options and is often referred to as the “fear gauge” as it tends to spike during market downturns. <a href="#fnref:VIX" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectral"> <p>The eigenvectors of a real symmetric matrix are orthogonal. This is a consequence of the spectral theorem, which states that a real symmetric matrix can be diagonalized by an orthonormal basis of eigenvectors. <a href="#fnref:spectral" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sp500"> <p>Note that the composition of the S&amp;P 500 index changes over time as companies are added or removed. We use the current components of the index for each year. Sometimes too many components change and this causes problems. One simple solution is to only consider the top 420 companies (instead of 500), which are more stable. (NB: we rank the companies by daily trading volume.) <a href="#fnref:sp500" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:sp500:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:proof"> <p>One way to confirm this intuition is to re-run the experiment but looking at the top 100 companies instead of the top 420. The results are very similar, which shows that the market mode is indeed robust to the number of companies considered (as long as it’s not too small of course). <a href="#fnref:proof" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="random-matrix-theory,"/><category term="linear-algebra,"/><category term="quant-finance"/><summary type="html"><![CDATA[TL;DR: Performing PCA on returns amounts to constructing a statistical factor model. The largest eigenvalue corresponds to the market mode and far outweighs the other factors. Thus, one can perform rolling PCA on equities' returns to monitor the market risk over time.]]></summary></entry><entry><title type="html">Jeffreys’ Prior in Bayesian Inference</title><link href="https://gaetanx21.github.io/blog/2025/jeffreys-prior/" rel="alternate" type="text/html" title="Jeffreys’ Prior in Bayesian Inference"/><published>2025-02-07T00:00:00+00:00</published><updated>2025-02-07T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/jeffreys-prior</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/jeffreys-prior/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}} \newcommand{\mathcalL}{\mathcal{L}}\] <p>We first motivate the need for “objective” priors in Bayesian inference by highlighting the limitations of uniform priors. We then introduce Jeffreys’ prior, which is invariant under reparametrization and provides a principled way to assign priors in Bayesian inference. We prove its invariance under reparametrization and illustrate its use in a coin flip problem. Note that throughout this post we restrict ourselves to the one-dimensional case for simplicity.</p> <h2 id="introduction">Introduction</h2> <p>In Bayesian inference, prior distributions encode our initial beliefs about an unknown parameter $\theta$ before observing data $x$. We can then update these beliefs using Bayes’ theorem to obtain a posterior distribution. Namely: <em>posterior = likelihood x prior</em>, which can be rewritten as $p(\theta | x) \propto p(x | \theta) p(\theta)$.</p> <p>Choosing priors usually involves a trade-off between incorporating prior knowledge and maintaining objectivity. Depending on the context and how much we know about the problem, we might have different beliefs about the parameter, or no beliefs at all. For instance, if we’re doing linear regression on standardized data ($y _ i = \beta^T x _ i + \varepsilon _ i$), we may feel like our prior for $\beta$ should be centered around zero. But if we’re doing a coin flip experiment ($X_i \sim B(\theta)$), we might not have any strong prior beliefs about the bias of the coin. So how do we choose the prior in this case? One naive approach would be to use a flat prior $p(\theta) \sim U([0, 1])$. This prior seems uninformative but it really isn’t. To see why, let’s consider the same coin flip experiment but this time we want to estimate the odds ratio $\phi = \frac{\theta}{1 - \theta}$. We may again naively choose a flat prior $p(\phi) \propto 1$<sup id="fnref:improper"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. But this flat prior on $\phi$ induces a non-flat prior on $\theta$! In fact, since $\phi$ is uniform on $\R_+$<sup id="fnref:improper:1"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and as such biased towards arbitrarily large values, $\theta = \frac{\phi}{1 + \phi}$ is highly biased towards $1$, as illustrated on <a href="#fig-1">Figure 1</a>. Thus, choosing a flat prior for $\phi$ is not the same as choosing a flat prior for $\theta$! That is why the seemingly objective choice of a flat prior is not always the best choice.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/theta-480.webp 480w,/assets/img/posts/jeffreys_prior/theta-800.webp 800w,/assets/img/posts/jeffreys_prior/theta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/theta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="theta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sketch of the prior distribution on $\theta$ induced by a flat prior on $\phi=\frac{\theta}{1-\theta}$. Clearly, the prior is not flat and is biased towards $\theta=1$. </div> <p>Likewise, choosing a flat prior in high-dimensional spaces assigns way too much mass to unimportant regions of the parameter space, so it is informative, but in a bad way!</p> <p>With this in mind, we see that <strong>uniform priors are no silver bullet</strong>. Ideally, we would like a prior which does not depend on the parameterization of the problem. In other word, <strong>the information we encode in the prior should be invariant under reparametrization</strong>. If we go back to the example of the coin flip, we would like a prior that encodes the same prior information about the bias of the coin, regardless of whether we’re working with $\theta$ or $\phi$. Intuitively, such a prior should be based on the <strong>structure of the data model</strong> itself, rather than the parameterization we choose.</p> <p>Reparametrization invariance is exactly what Jeffreys’ prior achieves, as explained below.</p> <p><em>Note that intuitively, reparametrization invariance is a good heuristic for an “objective” prior.</em></p> <h2 id="definition">Definition</h2> <p>Jeffreys’ prior is defined using the Fisher information matrix. Given a likelihood function $\mathcalL(\theta | x)$ for a parameter $\theta$, the Fisher information is:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right].\] <p>Jeffreys’ prior is then given by:</p> \[\pi_J(\theta) \propto \sqrt{I(\theta)}.\] <p>The key property of Jeffrey’s prior is that it is invariant under reparametrization. In other words, if we try to estimate a different parameter $\phi = g(\theta)$, the Jeffrey’s prior for $\phi$ will be:</p> \[\pi_J(\phi) \propto \sqrt{I(\phi)} = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|\] <p>which is consistent with the transformation rule for probability densities.</p> <p><em>Note that Jeffrey’s prior is defined using the likelihood function. While this is convenient because it allows us to use the structure of the data model, it also goes against the Bayesian principle of choosing the prior independently of the data. This is a philosophical issue in Bayesian statistics, and different practitioners may have different views on this.</em></p> <h2 id="proof-of-invariance-under-reparametrization">Proof of Invariance Under Reparametrization</h2> <p>In this paragraph we demonstrate Jeffreys’ prior invariance under reparametrization. Suppose we have a parameter $\theta$ and a reparametrized parameter $\phi = g(\theta)$. We want to show that Jeffrey’s prior for $\phi$ is consistent with the transformation rule for probability densities.</p> <p>To begin with, note that the chain rule gives:</p> \[I(\phi) = I(\theta) \left( \frac{d\theta}{d\phi} \right)^2.\] <p>Taking the square root, we get:</p> \[\sqrt{I(\phi)} = \sqrt{I(\theta)} \left| \frac{d\theta}{d\phi} \right|\] <p>i.e., Jeffrey’s prior transforms as:</p> \[\pi_J(\phi) = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|.\] <p>We recognize the transformation rule for probability densities, which demonstrates that Jeffrey’s prior correctly transforms to maintain consistency, proving its invariance by reparametrization.</p> <h2 id="coin-flip-example">Coin flip example</h2> <p>Let’s compute Jeffreys’ prior for a simple coin flip problem to illustrate its use.</p> <p>Consider a simple example: estimating the bias $\theta$ of a coin, where $X \sim \text{Bin}(n, \theta)$. The likelihood function is:</p> \[\mathcal L(\theta | x) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i}.\] <p>We compute the Fisher information:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right] = \frac{n}{\theta (1 - \theta)}.\] <p>Thus, Jeffreys’ prior for $\theta$ is:</p> \[\pi_J(\theta) \propto \sqrt{\frac{n}{\theta (1 - \theta)}} \propto \frac{1}{\sqrt{\theta (1 - \theta)}}.\] <p>We recognize the <strong>Beta(1/2, 1/2)</strong> distribution, which is commonly used as an uninformative prior for bounded parameters. This is a nice result, as it shows that Jeffreys’ prior is consistent with our intuition of an uninformative prior in this case.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/beta-480.webp 480w,/assets/img/posts/jeffreys_prior/beta-800.webp 800w,/assets/img/posts/jeffreys_prior/beta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/beta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="beta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Jeffreys' prior for the bias of a coin flip experiment is the Beta(1/2, 1/2) distribution. </div> <h2 id="conclusion">Conclusion</h2> <p>Jeffreys’ prior provides a principled way to assign priors in Bayesian inference, ensuring invariance under reparametrization. We proved its reparametrization invariance and illustrated its use in a coin flip problem. Jeffreys’ prior is useful when no clear subjective prior information is available, for instance in astrophysics. We’ve limited ourselves to the one-dimensional case for simplicity, but Jeffreys’ prior can be extended to higher dimensions naturally by considering the Fisher information matrix and its determinant, such that $\pi_J(\theta) \propto \sqrt{\text{det}(I(\theta))}$. Finally, I want to stress that Jeffreys’ prior violates the Bayesian principle of choosing the prior independently of the data, which may be a concern for some practitioners.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:improper"> <p>The prior $p(\phi) \propto 1$ is called an <em>improper</em> prior since it doesn’t integrate to 1. This is a common pitfall when using flat priors. However using unnormalized priors is okay as long as we normalize the posterior distribution. <a href="#fnref:improper" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:improper:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="bayesian-ml"/><summary type="html"><![CDATA[TL;DR: Bayesian inference requires us to specify a prior distribution. When we're unsure what prior to pick and want to stay as objective as possible, one option is to use Jeffreys' prior, which leverages the Fisher information to provide a reparametrization-invariant prior.]]></summary></entry><entry><title type="html">Regression Dilution</title><link href="https://gaetanx21.github.io/blog/2025/regression-dilution/" rel="alternate" type="text/html" title="Regression Dilution"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/regression-dilution</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/regression-dilution/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}}\] <p>We first introduce the concept of <strong>attenuation bias</strong> in linear regression due to measurement error in the covariates. We derive the shrinkage effect in the one-dimensional case and extend it to the multivariate case. We do simulations to visualize the shrinkage effect as the signal-to-noise ratio (SNR) goes to zero.</p> <h2 id="introduction">Introduction</h2> <p>In classical linear regression, we assume a model of the form:</p> \[y = X\beta + \varepsilon\] <p>where $X$ is an $n \times p$ matrix of covariates $x_i \in \R^p$, $\beta$ is a $p \times 1$ vector of coefficients, and $\varepsilon$ is noise. <strong>Weak exogeneity</strong> is a key assumption in linear regression, which states that the covariates $X$ are fixed and non-random. In other words, the covariates are assumed to be measured without error. However, in many real-world scenarios, this hypothesis is violated: covariates themselves contain measurement noise:</p> \[\tilde{X} = X + U\] <p>where $U$ is an $n \times p$ matrix of noise $u_i \in \R^p$. This additional noise leads to a phenomenon known as <strong>attenuation bias</strong>, where the estimated coefficients shrink towards zero. Let’s first derive this effect in the one-dimensional case.</p> <p>Note that in what follows we make the following the classical assumptions:</p> <ul> <li>$x_i$ i.i.d. centered with variance $\sigma_x^2$ (or covariance $\Sigma_x$ in the multivariate case),</li> <li>$u_i$ i.i.d. centered with variance $\sigma_u^2$ (or covariance $\Sigma_u$ in the multivariate case),</li> <li>$\varepsilon_i$ i.i.d. centered with variance $\sigma_\varepsilon^2$,</li> <li>$x_i, u_i, \varepsilon_i$ are independent of each other</li> </ul> <h2 id="one-dimensional-case">One-dimensional case</h2> <p>Let’s first derive the attenuation bias in the one-dimensional case.</p> <p>Consider the simple case of a one-dimensional linear regression model:</p> \[y = \beta x + \varepsilon.\] <p>Now assume that we observe a noisy version of $x$: $\tilde{x} = x + u$, where $u$ is the noise term. The least squares estimator of $\beta$ using the noisy covariate $\tilde{x}$ is:</p> \[\hat{\beta} = \frac{\Cov(\tilde{x}, y)}{\Var(\tilde{x})} = \frac{\Cov(x + u, \beta x + \varepsilon)}{\Var(x + u)} = \frac{\beta \Var(x)}{\Var(x) + \Var(u)} = \frac{\beta \sigma_x^2}{\sigma_x^2 + \sigma_u^2} = \lambda \beta\] <p>where $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}&lt;1$ is the attenuation factor or shrinkage factor.</p> <p>Thus the estimated coefficient $\hat{\beta}$ is a scaled version of the true coefficient $\beta$, with the scaling factor $\lambda$ being less than 1. This implies that the estimated coefficient is biased towards zero due to the noise in the covariate.</p> <p>In particular, note that when $\sigma_u = 0$, we recover the unbiased estimator $\hat{\beta} = \beta$. Likewise, as $\sigma_u \to \infty$, the estimated coefficient $\hat{\beta} \to 0$ since the SNR goes to zero.</p> <h2 id="multivariate-case">Multivariate case</h2> <p>The multivariate case can be derived similarly, though the algebra is slightly more involved.</p> <p>If we use the noisy covariates $\tilde{X}$ instead of the true covariates $X$, the least squares estimator becomes:</p> \[\hat{\beta} = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T y.\] <p>Substituting $\tilde{X} = X + U$ and $y = X\beta + \varepsilon$ gives:</p> \[\hat{\beta} = [(X + U)^T (X + U)]^{-1} (X + U)^T (X\beta + \varepsilon)\] <p>We rewrite this expression so that the law of large numbers can be applied:</p> \[\hat{\beta} = \bigg[\frac{1}{n}(X^T X + X^T U + U^T X + U^T U)\bigg]^{-1} \bigg[\frac{1}{n}(X^T X\beta + X^T \varepsilon + U^T X\beta + U^T \varepsilon)\bigg]\] <p>Using the weak law of large numbers, we have</p> \[\begin{align*} \frac{1}{n}X^T X &amp;\to \E[x x^T] = \Sigma_x \\ \frac{1}{n}X^T U &amp;\to \E[x u^T] = 0 \\ \frac{1}{n}U^T X &amp;\to \E[u x^T] = 0 \\ \frac{1}{n}U^T U &amp;\to \E[u u^T] = \Sigma_u \end{align*}\] <p>and</p> \[\begin{align*} \frac{1}{n}X^T X\beta &amp;\to \E[x x^T]\beta = \Sigma_x \beta \\ \frac{1}{n}X^T \varepsilon &amp;\to \E[x \varepsilon_i^T] = 0 \\ \frac{1}{n}U^T X\beta &amp;\to \E[u x^T]\beta = 0 \\ \frac{1}{n}U^T \varepsilon &amp;\to \E[ \varepsilon_i^T] = 0 \end{align*}\] <p>where all the convergences are in probability<sup id="fnref:strong"><a href="#fn:strong" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Combining these results and applying Sluskty’s lemma and the continuous mapping theorem, we have:</p> \[\hat{\beta} \xrightarrow[]{\mathbb{P}} (\Sigma_x + \Sigma_u)^{-1} \Sigma_x \beta = \left(I + \Sigma_x^{-1} \Sigma_u \right)^{-1} \beta.\] <p>Note that in the multi-dimensional case, the shrinkage factor is not a scalar but a matrix $\Lambda = (I + \Sigma_x^{-1} \Sigma_u)^{-1}$. In particular, although $\Sigma_x$ and $\Sigma_u$ are positive definite matrices, $\Sigma_x^{-1} \Sigma_u$ is not positive definite in general. Therefore it is more difficult to interpret the shrinkage effect in the multivariate case.</p> <p>For simplicity, if we assume spherical noise on both covariates and response, i.e., $\Sigma_x = \sigma_x^2 I$ and $\Sigma_u = \sigma_u^2 I$, we recover the one-dimensional result with $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$. This makes sense because assuming spherical noise is like running the one-dimensional case independently for each covariate.</p> <p>Additionally, we recover the unbiased estimator $\hat{\beta} = \beta$ when $\Sigma_u = 0$, as expected.</p> <h2 id="visualizing-the-shrinkage-effect-as-snr-goes-to-zero">Visualizing the shrinkage effect as SNR goes to zero</h2> <p>We want to illustrate the gradual shrinkage of the estimated coefficients as the SNR gradually decreases. We stick to the one-dimensional case for simplicity.</p> <p>We simulate a linear regression model with a single covariate $x$ with $\sigma_x = 1$ and noise $u$ with $\sigma_u$ running from $0$ to $5 \sigma_x$. For each value of $\sigma_u$, we fit a linear regression model using the noisy covariate $x + u$ and record the estimated coefficient $\hat{\beta}$.</p> <p>We then plot the empirical shrinkage ratio $\frac{\hat{\beta}}{\beta}$ as a function of the SNR $\frac{\sigma_x}{\sigma_u}$. Additionally, we overlay the theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$.</p> <p>The results are shown in <a href="#fig-1">Figure 1</a>. As the SNR decreases, the estimated coefficients shrink towards zero, as expected. The empirical shrinkage ratio closely follows the theoretical shrinkage factor $\lambda$.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/regression_dilution/snr_shrinkage-480.webp 480w,/assets/img/posts/regression_dilution/snr_shrinkage-800.webp 800w,/assets/img/posts/regression_dilution/snr_shrinkage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/regression_dilution/snr_shrinkage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="snr shrinkage" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical shrinkage ratio as a function of the SNR. The theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$ is overlaid. </div> <h2 id="conclusion">Conclusion</h2> <p>When covariates are measured with noise, the estimated regression coefficients shrink towards zero, leading to bias. This is important in fields where measurement errors are common, such as economics and epidemiology. One way to mitigate this bias is to use <strong>error-in-variables models</strong>, which explicitly model the noise in the covariates. The simplest such model is probably Deming regression, which models a one-dimensional linear regression and assumes the SNR to be known. $\hat{\beta}$ is then found by minimizing a <em>weighted</em> sum of squared residual to account for the noise in $x$.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:strong"> <p>The strong law of large numbers would require additional assumptions on the moments of the random variables involved. <a href="#fnref:strong" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="robust-ml"/><summary type="html"><![CDATA[TL;DR: When covariates in linear regression are subject to noise, the estimated regression coefficients shrink towards zero. We derive this effect mathematically and illustrate it with simulations.]]></summary></entry><entry><title type="html">A geodesic from cat to dog</title><link href="https://gaetanx21.github.io/blog/2024/ot-geodesic/" rel="alternate" type="text/html" title="A geodesic from cat to dog"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-geodesic</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-geodesic/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete entropy-regularized Kantorovich problem and show how it can be solved efficiently using the Sinkhorn algorithm. We then illustrate the usefulness of the Sinkhorn algorithm to compute Wasserstein distances, barycenters, and geodesics between probability distributions. We finally apply this to interpolate between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the 2-Wasserstein metric.</p> <h1 id="discrete-entropy-regularized-kantorovich-problem">Discrete Entropy-Regularized Kantorovich problem</h1> <p>The discrete entropy-regularized Kantorovich problem formulates as: \(\begin{equation} \label{eq:Kreg} \tag{$\tn{K}^\tn{reg}$} P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \langle C,P\rangle - \epsilon H(P) \end{equation}\) where $H(P)=\sum _ {i=1}^nP_{ij}(\log P _ {ij}-1)$ is the discrete entropy. Note that when $\epsilon=0$ one recovers classical discrete OT. Crucially, (\ref{eq:Kreg}) is strictly convex as soon as $\epsilon&gt;0$ and thus has a unique solution $P^{\epsilon,\star}$.</p> <p>Additionally, One can easily show that $\langle C,P \rangle - \epsilon H(P)= \tn{KL} (P|K)$, where $K=\exp(-\frac{C}{\epsilon})$ is called a Gibbs kernel. Thus (\ref{eq:Kreg}) can be seen as a projection problem w.r.t. to the KL divergence: (\ref{eq:Kreg}) rewrites as $P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \tn{KL}(P|K)$ i.e. $P^{\epsilon,\star}=\tn{Proj} _ {U(\alpha,\beta)}^\tn{KL}(K)$.</p> <p>The whole point of introducing the entropy is to relax the Kantorovich problem into a strictly convex problem which can be solved efficiently using the Sinkhorn algorithm, which we now introduce.</p> <h2 id="sinkhorns-algorithm">Sinkhorn’s algorithm</h2> <p>The most well-known method to solve (\ref{eq:Kreg}) is Sinkhorn’s algorithm, which uses the fact that $P^{\epsilon,\star}$ necessarily has the form $P^{\epsilon,\star}=\tn{Diag}(u)K\tn{Diag}(v)$ where $K$ is the Gibbs kernel. The conditions $P\mathbb{1} _ m=a$ and $P^T\mathbb{1} _ n=b$ thus rewrite as $u * (Kv) = a$ and $v * (K^Tu) = b$ respectively, where $*$ denotes the Hadamard product. One can thus iteratively solve these two equations until $u$ and $v$ converge, yielding the Sinkhorn algorithm:</p> \[\begin{align*} u^{l+1}&amp;\leftarrow\frac{a}{Kv^l}\\ v^{l+1}&amp;\leftarrow\frac{b}{K^Tu^{l+1}} \end{align*}\] <p>where we use the initialization $v^0=\mathbb{I} _ m$.</p> <p>In practice, Sinkhorn’s algorithm allows us to compute Wasserstein distances efficiently. In turn, we can use these distances to compute barycenters and geodesics between probability distributions.</p> <h2 id="wasserstein-barycenters-and-geodesics-on-probability-spaces">Wasserstein barycenters and geodesics on probability spaces</h2> <p>Using OT, one can define <em>distances</em> between probability distributions defined on the same space $X$. The most common is the $p$-Wasserstein distance $W_p$ defined for any real number $p&gt;0$: \(\begin{equation} \label{eq:Wp} \tag{$W_p$} W_p(\alpha,\beta)=\min_{P\in U(\alpha,\beta)} \langle P,C^p \rangle^{1/p} = \bigg(\sum_{1\leq i,j\leq n} d(x_i,y_j)^p P_{ij}\bigg)^{1/p} \end{equation}\) where $d$ is a distance on $X$. For instance if $X=\R^d$ one can use $d(x,y)=||x-y||$.</p> <p>Now that we have a distance on probability measures, we can use it to compute barycenters. For a fixed $p&gt;1$ and $R$ probability distributions $\alpha_1,\dots,\alpha_R \in \tn{P}(X)$, their $p$-Wasserstein barycenter with coefficients $(\lambda_r)_r$ is defined as: \(\begin{equation} \label{eq:barycenter} \tag{B} \beta = \arg \min_{\beta \in \tn{P}(X)} \sum_{r=1}^{R} \lambda_k W_p(\alpha_r,\beta) \end{equation}\)</p> <p>$p$-Wasserstein barycenters can in particular be used to compute geodesics for the $p$-Wasserstein metric of the form $t\in[0,1]\mapsto\mu_t\in\tn{P}(X)$ from $\alpha$ to $\beta$ as: \(\begin{equation} \mu_t = \arg \min_{\mu\in\tn{P}(X)} (1-t)W_p^p(\alpha,\mu_t) + tW_p^p(\beta,\mu_t) \end{equation}\) When $p=2$, we in fact have $\mu_t=\sum_{1\leq i,j\leq n}P_{ij}^\star \delta_{(1-t)x_i+ty_j}$ using the notations from my <a href="/blog/2024/ot-assignement-problem/">previous post</a>.</p> <h2 id="a-geodesic-from-cat-to-dog">A geodesic from cat to dog</h2> <p>Wasserstein barycenters can be used to interpolate between (grayscale) images using the following formalism: a grayscale image of dimension $N\times N$ can be seen as a distribution of “light” $\alpha\in \tn{P}(\R^{N\times N})$. Then, one can go from an image of a cat $\alpha$ to that of a dog $\beta$ using OT. This has little value in itself, but one can also consider the geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ from $\alpha$ to $\beta$ and thus see the gradual fade from the cat image to the dog image.</p> <p>For $0\leq i \leq 8$ we consider the barycenter coefficients $\lambda=(1-t_i,t_i)$ where $t_i=\frac{i}{8}$ and we plot the 9 corresponding 2-Wasserstein barycenters $b^i\in\tn{P}(\R^{N\times N})$ which intuitively interpolate between the cat and the dog. The pictures were found online, turned to grayscale, resized to $N\times N$ with $N=128$, smooth with a Gaussian kernel, and then each Wasserstein barycenter is computed using the <code class="language-plaintext highlighter-rouge">ot</code> Python library. The results are presented in <a href="#fig-1">Figure 1</a> and quite satisfying for such a simple approach!</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_geodesic/cat2dog-480.webp 480w,/assets/img/posts/ot_geodesic/cat2dog-800.webp 800w,/assets/img/posts/ot_geodesic/cat2dog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_geodesic/cat2dog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cat2dog" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ in $\R^{N\times N}$ computed using 2-Wasserstein barycenters. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that the entropy-regularized Kantorovich problem can be solved efficiently using the Sinkhorn algorithm. This allows us to efficiently compute Wasserstein distances, barycenters, and geodesics between probability distributions. We have illustrated this by interpolating between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the $W_2$ metric.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: Entropic regularization relaxes the Kantorovitch problem into a strictly convex problem which can be solved efficiently with the Sinkhorn algorithm. We can use this to efficiently compute Wasserstein distances, barycenters, and finally geodesics between distributions.]]></summary></entry><entry><title type="html">Solving the assignement problem using Optimal Transport</title><link href="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/" rel="alternate" type="text/html" title="Solving the assignement problem using Optimal Transport"/><published>2024-11-15T00:00:00+00:00</published><updated>2024-11-15T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-assignement-problem</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete Kantorovich problem and show that in the uniform case it amounts to solving the permutation problem. We then illustrate this with a student internship assignement problem. We run Monte Carlo simulations for different cost functions and show that the choice of cost function crucially impacts the optimal assignement.</p> <h2 id="the-discrete-kantorovich-problem">The discrete Kantorovich Problem</h2> <p>Let $X, Y$ be two measurable spaces (for simplicity, $X=Y=\R^d$). Consider two discrete distributions (i.e. weighted point clouds) $\alpha\in \tn{P}(X), \ \beta\in\tn{P}(Y)$ given by \(\begin{equation} \label{eq:def} \alpha = \sum_{i=1}^n a_i \delta_{x_i}, \quad \beta = \sum_{j=1}^m b_j \delta_{y_j}, \end{equation}\) and a cost function $c:X\times Y \rightarrow \R^+$.</p> <p>The discrete Kantorovich problem then formulates as: \(\begin{equation} \label{eq:K} \tag{K} P^\star = \arg \min_{P\in U(\alpha,\beta)} \langle C,P\rangle \end{equation}\) where $C=\big(c(x_i,y_j)\big)_{i,j} \in \R^{n\times m}$ and $U(\alpha,\beta)=\lbrace P\in \R^{n\times m} | P\geq 0, P\mathbb{1}_m=a, P^T \mathbb{1}_n=b \rbrace$.</p> <p>Notice that $P\mapsto \langle C,P \rangle$ is a convex functional and $U(\alpha,\beta)$ is a convex subset of $\R^{n\times m}$, such that (\ref{eq:K}) is a convex problem.</p> <p>Even better, it is a linear programming (LP) problem since $P\mapsto \langle C,P \rangle$ is linear and $U(\alpha,\beta)$ encodes linear constraints.</p> <p>Thus, in the discrete case, Optimal Transport (OT) can be seen as an LP problem, and thus solved with off-the-shelf LP solvers such as the <code class="language-plaintext highlighter-rouge">cvxpy</code> Python library.</p> <h2 id="the-uniform-case">The Uniform Case</h2> <p>Let’s consider the uniform case i.e. $n=m$ and $a_i=b_j=\frac{1}{n} \ \forall i,j$.</p> <p>In that scenario, one can show that there exists at least one OT coupling $P^\star$ which is a permutation matrix. This comes from the fact that the extremal points of the polytope $U(1,1)$ are permutation matrices.</p> <p>Thus, in the uniform case there exists a permutation $\sigma^\star \in S_n$ such that $P^\star=P _ {\sigma^\star}=\big( \mathbb{1} _ {\sigma^\star(i)=j} \big) _ {i,j}$. In particular, $\sigma^\star$ solves the permutation problem \(\begin{equation} \label{eq:permutation-problem} \tag{PP} \sigma^\star = \arg \min_{\sigma\in S_n} \sum_{i=1}^n C_{i,\sigma(j)} \end{equation}\)</p> <h2 id="student-internship-assignment">Student Internship Assignment</h2> <p>To illustrate the method described, let’s apply the uniform case, which solves the permutation problem, to assign $n$ students $x_i$ to $n$ internships $y_j$ in a <em>optimal</em> manner.</p> <p>Let’s consider that each student $x_i$ expresses their preference through a ranking $\sigma_i$ of the internships where $\sigma_i(j)$ is the ranking of internship $y_j$ according to student $x_i$ (i.e. $\sigma_i(j)=1$ for $x_i$’s dream internship and $\sigma_i(j)=n$ for $x_i$’s least desired internship).</p> <p>There are many possible choices for the cost function $c$, but it must clearly be an increasing function of $\sigma_i(j)$. The most natural is probably $c(x_i,y_j)=\sigma_i(j)$ i.e. a linear penalization of the integer distance between the student’s favorite ($c=1$) and least wanted internship ($c=n$). However, the optimal assignment $P^\star=P_{\sigma^\star}$ depends crucially on the choice of $c$! Intuitively, rapidly increasing function e.g. quadratic cost $c(x_i,y_j)=\sigma_i(j)^2$ will prevent any student from being attributed an internship deemed too undesirable. This means no student will get an awful internship, the hidden cost being that presumably fewer student will get their first wish. On the contrary, a slowly increasing function e.g. log cost $c(x_i,y_j)=\log\sigma_i(j)$ will only slightly penalize poor internship attributions, and thus we except to see lots of students get their first wish alongside a handful of students getting very low-ranked internships.</p> <p>We test those intuitions by running Monte Carlo simulations for each of the aforementioned cost functions (linear, quadratic, log). More precisely, for a given cost function $c$, we run $M$ simulations, each with $n$ students. Each simulation returns a integer array <code class="language-plaintext highlighter-rouge">ranks</code> of length $n$ where <code class="language-plaintext highlighter-rouge">ranks[i]</code> is student i’s ranking of the internship they were attributed. For each cost function $c$, We concatenate the $M$ <code class="language-plaintext highlighter-rouge">ranks</code> arrays and then plot a histogram of their distribution.</p> <p>The results are presented in <a href="#fig-assignment">Figure 1</a> and confirm our intuition, although there is no difference between linear and quadratic cost. We used $n=20$ students and ran $M=100$ iterations for each cost function $c$.</p> <div class="row justify-content-center" id="fig-assignment"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_permutation_problem/ranks-480.webp 480w,/assets/img/posts/ot_permutation_problem/ranks-800.webp 800w,/assets/img/posts/ot_permutation_problem/ranks-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_permutation_problem/ranks.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ranks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical distribution of students’ ranking of their obtained internship for different cost functions $c$. The log penalty increases slowly such that it’s tolerable to highly disappoint a handful of students if that can help the majority obtain their first wish. This is not the case for the linear and quadratic penalties, which penalize highly the worst attributions. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that discrete OT amounts to a LP problem. However, LP problems do not scale well. This motivates the introduction of entropic regularization, which makes (\ref{eq:K}) much easier and faster to solve when $n$ becomes too large for a LP approach. We will discuss this in a future post.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: The discrete Kantorovich problem amounts to a LP problem. In the uniform case, the solution is a permutation matrix which in fact solves the assignement problem.]]></summary></entry><entry><title type="html">Intuitions behind Benford’s Law</title><link href="https://gaetanx21.github.io/blog/2024/benford-law/" rel="alternate" type="text/html" title="Intuitions behind Benford’s Law"/><published>2024-09-20T00:00:00+00:00</published><updated>2024-09-20T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/benford-law</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/benford-law/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first present a quick overview of Benford’s Law. We then provide three different intuitions behind this phenomenon and illustrate them with simulations.</p> <h2 id="introduction-to-benfords-law">Introduction to Benford’s Law</h2> <p>Benford’s Law states that the distribution of the first digit of many real-world datasets is not uniform, but instead verifies $\mathbb{P}(d) \simeq \log_{10}(1 + \frac{1}{d})$ for $d \in \lbrace 1, \ldots, 9\rbrace$. <a href="#fig-1">Figure 1</a> plots the theoretical distribution of the first digit according to Benford’s Law, alongside the uniform distribution for comparison.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/naive_vs_benford-480.webp 480w,/assets/img/posts/benford_law/naive_vs_benford-800.webp 800w,/assets/img/posts/benford_law/naive_vs_benford-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/naive_vs_benford.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="naive vs benford" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Benford's Law alongside uniform distribution. </div> <h4 id="history">History</h4> <p>Benford’s law was actually discovered in the 19th century by Simon Newcomb, who noticed that the first pages of logarithm tables were more worn out than the last ones. Some 50 years later, Frank Benford published a paper where he advanced explanations of this anomaly, which is why the law is now named after him.</p> <h4 id="examples">Examples</h4> <p>Many real-world datasets follow Benford’s Law, such as the populations of countries, the lengths of rivers, stock prices, the numbers in tax returns, etc. Note however that not all datasets follow Benford’s Law. In particular, datasets that do not span several orders of magnitude are unlikely to follow it, for reasons that will become clear in the following sections.</p> <h4 id="applications">Applications</h4> <p>Benford’s law can be used to detect made-up data, often generated by humans, since they tend to distribute the first digits uniformly. In particular, it can be used to detect fraud in accounting, elections, academic papers. For instance, the macroeconomic data the Greek government provided to the European Union before entering the Eurozone did not follow Benford’s Law (though we found out years later only…)</p> <h4 id="literature">Literature</h4> <p>The literature on Benford’s law is somewhat scattered and rife with pseudo-explanations. The phenomenon is still not fully understood, and I do not pretend to provide a definitive answer here. Instead, my goal is to provide three different intuitions behind Benford’s Law, which I will try to explain in simple terms and illustrate with simulations.</p> <h2 id="first-intuition-geometric-growth">First intuition: geometric growth</h2> <p>Perhaps the most intuitive explanation for Benford’s Law is that many real-world variables grow geometrically. For instance, the population of a country grows at a certain rate each year, the stock price of a company goes up or down a percent of so each day, etc. When you think of it, any variable that spans several orders of magnitude should be suspected to grow more or less geometrically!</p> <p>Let’s first consider a variable $X_t$ that grows geometrically at some unknown constant rate $r&gt;0$, starting at $X_0=1$. Intuitively, we feel that going from 1 to 2 (a +100% increase) will take more time than going from 9 to 10 (a +11% increase). And once we get to 10, we feel that going from 10 to 20 (a +100% increase) will take more time than going from 90 to 100 (a +11% increase). And so on. In fact, we can formalize this intuition by writing $X_t = (1+r)^t$, such that going from $d\cdot 10^n$ to $(d+1)\cdot 10^n$ takes time $\Delta t = \frac{1}{1+r} \log _ {10} \left(\frac{d+1}{d}\right) \propto \log _ {10}\left(1 + \frac{1}{d}\right)$. We thus recover Benford’s Law in the constant geometric growth setting.</p> <p>What if we relax our assumptions and add some noise to the growth rate? Intuitively we feel like we should still observe Benford’s Law, perhaps with some deviations due to the noise.</p> <p>To confirm this hypothesis, let’s consider the process $\frac{dX_t}{X_t}=\mu dt + \sigma dW_t$, where $\mu$ is the drift, $\sigma$ is the volatility, and $dW_t$ is a Brownian motion. It is one (very simple) way of modeling stock prices. The math is a bit trickier to deal with, so instead of an analytical solution we’ll simulate the process and plot the distribution of the first digit of $X_t$ at different times $t$. <a href="#fig-2">Figure 2</a> shows that the distribution of the first digit of $X_t$ indeed matches Benford’s Law. The result is robust to the choice of parameters, with better results as we increase the number of simulations $N$ and the time horizon $T$.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/geometric_growth-480.webp 480w,/assets/img/posts/benford_law/geometric_growth-800.webp 800w,/assets/img/posts/benford_law/geometric_growth-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/geometric_growth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="geometric growth" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Benford Law vs. empirical distribution of the first digit of $X_t$. We used parameters $N=1,000,000, T=1,000, \mu=0.1$, $\sigma=0.2$, and $X_0=1$. </div> <h2 id="second-intuition-clt-on-the-logarithm">Second intuition: CLT on the logarithm</h2> <p>Another closely related intuition comes from the Central Limit Theorem (CLT). The core hypothesis is that variables that follow Benford’s law really are <em>products</em> of many factors, which are more or less independent. For instance, the price of a brick of butter is the product of its length, width, height, and the price of the milk.</p> <p>Let’s thus consider a positive random variable $X$ which is made up of $P$ underlying factors i.e. $X = \prod_{i=1}^P X_i$ where we assume the $X_i$ to be i.i.d. positive random variables. We have $\log_{10}(X) = \sum_{i=1}^P \log_{10}(X_i)$. By the CLT, $\log_{10}(X)$ should be approximately normally distributed, with variance scaling linearly with $P$. Thus as P grows to infinity, so does the variance of the normal distribution that models $\log_{10}(X)$.</p> <p>Let’s now look at the random variable $\lbrace \log_{10}(X) \rbrace$ where $\lbrace \cdot \rbrace$ denotes the fractional part. Using the result $\lbrace \sigma Z \rbrace \xrightarrow[\sigma^2\xrightarrow\infty]{d} U([0,1])$ where $Z$ is normally distributed, we have $\mathcal{L}(\lbrace \log_{10}(X) \rbrace) \simeq U([0,1])$.</p> <p>Finally, note that we can rewrite $X$ as $10^{\lbrace \log_{10}(X) \rbrace} \times 10^{\lfloor \log_{10}(X) \rfloor} = \tn{significand} \times \tn{order of magnitude}$. The probability $p_d$ of $X$ having first digit $d$ is then given by \(\begin{align*} p_d &amp;= \mathbb{P}(d \leq \tn{significand} &lt; d+1) \ &amp;= \mathbb{P}(\log_{10}(d) \leq \lbrace \log_{10}(X) \rbrace &lt; \log_{10}(d+1)) \ &amp;= \log_{10}(d+1) - \log_{10}(d) = \log_{10}(1 + \frac{1}{d}) \\). We thus recover Benford’s Law.</p> <p>On <a href="#fig-3">Figure 3</a>, we simulate $X$ as the product of $P=3$ i.i.d. random variables $X_i\sim U([1,10])$. Again, we observe that the distribution of the first digit of $X$ matches Benford’s Law. Note that this result is robust to the choice of the distribution of the $X_i$ and the number of factors $P&gt;1$, with better results as we increase $P$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/clt-480.webp 480w,/assets/img/posts/benford_law/clt-800.webp 800w,/assets/img/posts/benford_law/clt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/clt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="clt" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, P=3$, and $X_i\sim U([1,10])$. </div> <h2 id="third-intuition-scale-invariance">Third intuition: scale invariance</h2> <p>This last intuition is a bit different from the two previous ones. The goal here is to <em>think like a physician</em> (!). The key idea is that Benford’s law applies to variables which have a <em>dimension</em>, or to say it plainly, Benford’s law applies to numbers that need a unit after them (e.g. meters, euros, liters, etc.) If we look at countries’ area for instance, we can measure it in square kilometers or square miles and we’ll still observe Benford’s law. Likewise, stock prices in EUR, USD and JPY all display Benford’s law. And that makes sense right? Since units are arbitrary conventions, we don’t expect Benford’s law to fade away when we change them.</p> <p>Okay but how to turn this insight into a mathematical argument? The answer is <em>scale invariance</em>.</p> <p>Let’s consider a positive variable $X$ that follows Benford’s Law. Assume that $X$ has a probability measure with density $f$ w.r.t. the Lebesgue measure. Since changing units doesn’t break Benford’s law, we can multiply $X$ by some constant $k$ and still end up with the same distribution. In other words, there is some constant $C(k)$ that such $\forall x, f(kx)=C(k)f(x)$. This is the definition of scale invariance. We also need the probability mass to conserve here, i.e. $f(x)dx = f(kx)d(kx)$, i.e. $f(kx)=\frac{f(x)}{k}$. Differentiating with respect to $k$ and then setting $k=1$ yields the linear functional equation $f’(x) = -\frac{1}{x}f(x)$, with solution $f(x) = \frac{\lambda}{x}$. This isn’t technically a probability density function, since it cannot be normalized. In fact scale-invariant distributions are exactly of the form $p(x)\propto \frac{1}{x^{\alpha}}$ for $\alpha&gt;1$ (power law). Let’s thus consider $\alpha=1.01$ for instance, to bring us close to the ideal case of Benford’s Law.</p> <p>We simulate $X$ as a random variable with density $p(x)\propto \frac{1}{x^\alpha}$ and plot the distribution of the first digit of $X$ on <a href="#fig-4">Figure 4</a>. We observe that the distribution of the first digit of $X$ indeed matches Benford’s Law. Note that the result is <em>not</em> robust to the choice of $\alpha$: we only observe concordance with Benford’s Law for $\alpha$ close to 1.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/scale_invariance-480.webp 480w,/assets/img/posts/benford_law/scale_invariance-800.webp 800w,/assets/img/posts/benford_law/scale_invariance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/scale_invariance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="scale invariance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, \alpha=1.01$. </div> <h2 id="conclusion">Conclusion</h2> <p>We provided three different intuitions behind Benford’s Law, which we illustrated with simulations.</p> <ol> <li>Geometric growth: Benford’s Law arises when variables grow geometrically.</li> <li>CLT on the logarithm: Benford’s Law arises when variables are products of many factors.</li> <li>Scale invariance: Benford’s Law arises when variables are scale-invariant.</li> </ol>]]></content><author><name></name></author><category term="misc"/><summary type="html"><![CDATA[TL;DR: Many real-world datasets follow Benford's Law, which states that distribution of the first digit is not uniform. We provide three different intuitions behind this phenomenon.]]></summary></entry></feed>