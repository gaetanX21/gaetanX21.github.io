<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-29T21:55:35+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">A geodesic from cat to dog</title><link href="https://gaetanx21.github.io/blog/2024/ot-geodesic/" rel="alternate" type="text/html" title="A geodesic from cat to dog"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-geodesic</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-geodesic/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete entropy-regularized Kantorovich problem and show how it can be solved efficiently using the Sinkhorn algorithm. We then illustrate the usefulness of the Sinkhorn algorithm to compute Wasserstein distances, barycenters, and geodesics between probability distributions. We finally apply this to interpolate between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the 2-Wasserstein metric.</p> <h1 id="discrete-entropy-regularized-kantorovich-problem">Discrete Entropy-Regularized Kantorovich problem</h1> <p>The discrete entropy-regularized Kantorovich problem formulates as: \(\begin{equation} \label{eq:Kreg} \tag{$\tn{K}^\tn{reg}$} P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \langle C,P\rangle - \epsilon H(P) \end{equation}\) where $H(P)=\sum _ {i=1}^nP_{ij}(\log P _ {ij}-1)$ is the discrete entropy. Note that when $\epsilon=0$ one recovers classical discrete OT. Crucially, (\ref{eq:Kreg}) is strictly convex as soon as $\epsilon&gt;0$ and thus has a unique solution $P^{\epsilon,\star}$.</p> <p>Additionally, One can easily show that $\langle C,P \rangle - \epsilon H(P)= \tn{KL} (P|K)$, where $K=\exp(-\frac{C}{\epsilon})$ is called a Gibbs kernel. Thus (\ref{eq:Kreg}) can be seen as a projection problem w.r.t. to the KL divergence: (\ref{eq:Kreg}) rewrites as $P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \tn{KL}(P|K)$ i.e. $P^{\epsilon,\star}=\tn{Proj} _ {U(\alpha,\beta)}^\tn{KL}(K)$.</p> <p>The whole point of introducing the entropy is to relax the Kantorovich problem into a strictly convex problem which can be solved efficiently using the Sinkhorn algorithm, which we now introduce.</p> <h2 id="sinkhorns-algorithm">Sinkhorn’s algorithm</h2> <p>The most well-known method to solve (\ref{eq:Kreg}) is Sinkhorn’s algorithm, which uses the fact that $P^{\epsilon,\star}$ necessarily has the form $P^{\epsilon,\star}=\tn{Diag}(u)K\tn{Diag}(v)$ where $K$ is the Gibbs kernel. The conditions $P\mathbb{1} _ m=a$ and $P^T\mathbb{1} _ n=b$ thus rewrite as $u * (Kv) = a$ and $v * (K^Tu) = b$ respectively, where $*$ denotes the Hadamard product. One can thus iteratively solve these two equations until $u$ and $v$ converge, yielding the Sinkhorn algorithm:</p> \[\begin{align*} u^{l+1}&amp;\leftarrow\frac{a}{Kv^l}\\ v^{l+1}&amp;\leftarrow\frac{b}{K^Tu^{l+1}} \end{align*}\] <p>where we use the initialization $v^0=\mathbb{I} _ m$.</p> <p>In practice, Sinkhorn’s algorithm allows us to compute Wasserstein distances efficiently. In turn, we can use these distances to compute barycenters and geodesics between probability distributions.</p> <h2 id="wasserstein-barycenters-and-geodesics-on-probability-spaces">Wasserstein barycenters and geodesics on probability spaces</h2> <p>Using OT, one can define <em>distances</em> between probability distributions defined on the same space $X$. The most common is the $p$-Wasserstein distance $W_p$ defined for any real number $p&gt;0$: \(\begin{equation} \label{eq:Wp} \tag{$W_p$} W_p(\alpha,\beta)=\min_{P\in U(\alpha,\beta)} \langle P,C^p \rangle^{1/p} = \bigg(\sum_{1\leq i,j\leq n} d(x_i,y_j)^p P_{ij}\bigg)^{1/p} \end{equation}\) where $d$ is a distance on $X$. For instance if $X=\R^d$ one can use $d(x,y)=||x-y||$.</p> <p>Now that we have a distance on probability measures, we can use it to compute barycenters. For a fixed $p&gt;1$ and $R$ probability distributions $\alpha_1,\dots,\alpha_R \in \tn{P}(X)$, their $p$-Wasserstein barycenter with coefficients $(\lambda_r)_r$ is defined as: \(\begin{equation} \label{eq:barycenter} \tag{B} \beta = \arg \min_{\beta \in \tn{P}(X)} \sum_{r=1}^{R} \lambda_k W_p(\alpha_r,\beta) \end{equation}\)</p> <p>$p$-Wasserstein barycenters can in particular be used to compute geodesics for the $p$-Wasserstein metric of the form $t\in[0,1]\mapsto\mu_t\in\tn{P}(X)$ from $\alpha$ to $\beta$ as: \(\begin{equation} \mu_t = \arg \min_{\mu\in\tn{P}(X)} (1-t)W_p^p(\alpha,\mu_t) + tW_p^p(\beta,\mu_t) \end{equation}\) When $p=2$, we in fact have $\mu_t=\sum_{1\leq i,j\leq n}P_{ij}^\star \delta_{(1-t)x_i+ty_j}$ using the notations from my <a href="/blog/2024/ot-assignement-problem/">previous post</a>.</p> <h2 id="a-geodesic-from-cat-to-dog">A geodesic from cat to dog</h2> <p>Wasserstein barycenters can be used to interpolate between (grayscale) images using the following formalism: a grayscale image of dimension $N\times N$ can be seen as a distribution of “light” $\alpha\in \tn{P}(\R^{N\times N})$. Then, one can go from an image of a cat $\alpha$ to that of a dog $\beta$ using OT. This has little value in itself, but one can also consider the geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ from $\alpha$ to $\beta$ and thus see the gradual fade from the cat image to the dog image.</p> <p>For $0\leq i \leq 8$ we consider the barycenter coefficients $\lambda=(1-t_i,t_i)$ where $t_i=\frac{i}{8}$ and we plot the 9 corresponding 2-Wasserstein barycenters $b^i\in\tn{P}(\R^{N\times N})$ which intuitively interpolate between the cat and the dog. The pictures were found online, turned to grayscale, resized to $N\times N$ with $N=128$, smooth with a Gaussian kernel, and then each Wasserstein barycenter is computed using the <code class="language-plaintext highlighter-rouge">ot</code> Python library. The results are presented in <a href="#fig-1">Figure 1</a> and quite satisfying for such a simple approach!</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_geodesic/cat2dog-480.webp 480w,/assets/img/posts/ot_geodesic/cat2dog-800.webp 800w,/assets/img/posts/ot_geodesic/cat2dog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_geodesic/cat2dog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cat2dog" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ in $\R^{N\times N}$ computed using 2-Wasserstein barycenters. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that the entropy-regularized Kantorovich problem can be solved efficiently using the Sinkhorn algorithm. This allows us to efficiently compute Wasserstein distances, barycenters, and geodesics between probability distributions. We have illustrated this by interpolating between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the $W_2$ metric.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: Entropic regularization relaxes the Kantorovitch problem into a strictly convex problem which can be solved efficiently with the Sinkhorn algorithm. We can use this to efficiently compute Wasserstein distances, barycenters, and finally geodesics between distributions.]]></summary></entry><entry><title type="html">Solving the assignement problem using Optimal Transport</title><link href="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/" rel="alternate" type="text/html" title="Solving the assignement problem using Optimal Transport"/><published>2024-11-15T00:00:00+00:00</published><updated>2024-11-15T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-assignement-problem</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete Kantorovich problem and show that in the uniform case it amounts to solving the permutation problem. We then illustrate this with a student internship assignement problem. We run Monte Carlo simulations for different cost functions and show that the choice of cost function crucially impacts the optimal assignement.</p> <h2 id="the-discrete-kantorovich-problem">The discrete Kantorovich Problem</h2> <p>Let $X, Y$ be two measurable spaces (for simplicity, $X=Y=\R^d$). Consider two discrete distributions (i.e. weighted point clouds) $\alpha\in \tn{P}(X), \ \beta\in\tn{P}(Y)$ given by \(\begin{equation} \label{eq:def} \alpha = \sum_{i=1}^n a_i \delta_{x_i}, \quad \beta = \sum_{j=1}^m b_j \delta_{y_j}, \end{equation}\) and a cost function $c:X\times Y \rightarrow \R^+$.</p> <p>The discrete Kantorovich problem then formulates as: \(\begin{equation} \label{eq:K} \tag{K} P^\star = \arg \min_{P\in U(\alpha,\beta)} \langle C,P\rangle \end{equation}\) where $C=\big(c(x_i,y_j)\big)_{i,j} \in \R^{n\times m}$ and $U(\alpha,\beta)=\lbrace P\in \R^{n\times m} | P\geq 0, P\mathbb{1}_m=a, P^T \mathbb{1}_n=b \rbrace$.</p> <p>Notice that $P\mapsto \langle C,P \rangle$ is a convex functional and $U(\alpha,\beta)$ is a convex subset of $\R^{n\times m}$, such that (\ref{eq:K}) is a convex problem.</p> <p>Even better, it is a linear programming (LP) problem since $P\mapsto \langle C,P \rangle$ is linear and $U(\alpha,\beta)$ encodes linear constraints.</p> <p>Thus, in the discrete case, Optimal Transport (OT) can be seen as an LP problem, and thus solved with off-the-shelf LP solvers such as the <code class="language-plaintext highlighter-rouge">cvxpy</code> Python library.</p> <h2 id="the-uniform-case">The Uniform Case</h2> <p>Let’s consider the uniform case i.e. $n=m$ and $a_i=b_j=\frac{1}{n} \ \forall i,j$.</p> <p>In that scenario, one can show that there exists at least one OT coupling $P^\star$ which is a permutation matrix. This comes from the fact that the extremal points of the polytope $U(1,1)$ are permutation matrices.</p> <p>Thus, in the uniform case there exists a permutation $\sigma^\star \in S_n$ such that $P^\star=P _ {\sigma^\star}=\big( \mathbb{1} _ {\sigma^\star(i)=j} \big) _ {i,j}$. In particular, $\sigma^\star$ solves the permutation problem \(\begin{equation} \label{eq:permutation-problem} \tag{PP} \sigma^\star = \arg \min_{\sigma\in S_n} \sum_{i=1}^n C_{i,\sigma(j)} \end{equation}\)</p> <h2 id="student-internship-assignment">Student Internship Assignment</h2> <p>To illustrate the method described, let’s apply the uniform case, which solves the permutation problem, to assign $n$ students $x_i$ to $n$ internships $y_j$ in a <em>optimal</em> manner.</p> <p>Let’s consider that each student $x_i$ expresses their preference through a ranking $\sigma_i$ of the internships where $\sigma_i(j)$ is the ranking of internship $y_j$ according to student $x_i$ (i.e. $\sigma_i(j)=1$ for $x_i$’s dream internship and $\sigma_i(j)=n$ for $x_i$’s least desired internship).</p> <p>There are many possible choices for the cost function $c$, but it must clearly be an increasing function of $\sigma_i(j)$. The most natural is probably $c(x_i,y_j)=\sigma_i(j)$ i.e. a linear penalization of the integer distance between the student’s favorite ($c=1$) and least wanted internship ($c=n$). However, the optimal assignment $P^\star=P_{\sigma^\star}$ depends crucially on the choice of $c$! Intuitively, rapidly increasing function e.g. quadratic cost $c(x_i,y_j)=\sigma_i(j)^2$ will prevent any student from being attributed an internship deemed too undesirable. This means no student will get an awful internship, the hidden cost being that presumably fewer student will get their first wish. On the contrary, a slowly increasing function e.g. log cost $c(x_i,y_j)=\log\sigma_i(j)$ will only slightly penalize poor internship attributions, and thus we except to see lots of students get their first wish alongside a handful of students getting very low-ranked internships.</p> <p>We test those intuitions by running Monte Carlo simulations for each of the aforementioned cost functions (linear, quadratic, log). More precisely, for a given cost function $c$, we run $M$ simulations, each with $n$ students. Each simulation returns a integer array <code class="language-plaintext highlighter-rouge">ranks</code> of length $n$ where <code class="language-plaintext highlighter-rouge">ranks[i]</code> is student i’s ranking of the internship they were attributed. For each cost function $c$, We concatenate the $M$ <code class="language-plaintext highlighter-rouge">ranks</code> arrays and then plot a histogram of their distribution.</p> <p>The results are presented in <a href="#fig-assignment">Figure 1</a> and confirm our intuition, although there is no difference between linear and quadratic cost. We used $n=20$ students and ran $M=100$ iterations for each cost function $c$.</p> <div class="row justify-content-center" id="fig-assignment"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_permutation_problem/ranks-480.webp 480w,/assets/img/posts/ot_permutation_problem/ranks-800.webp 800w,/assets/img/posts/ot_permutation_problem/ranks-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_permutation_problem/ranks.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ranks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical distribution of students’ ranking of their obtained internship for different cost functions $c$. The log penalty increases slowly such that it’s tolerable to highly disappoint a handful of students if that can help the majority obtain their first wish. This is not the case for the linear and quadratic penalties, which penalize highly the worst attributions. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that discrete OT amounts to a LP problem. However, LP problems do not scale well. This motivates the introduction of entropic regularization, which makes (\ref{eq:K}) much easier and faster to solve when $n$ becomes too large for a LP approach. We will discuss this in a future post.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: The discrete Kantorovich problem amounts to a LP problem. In the uniform case, the solution is a permutation matrix which in fact solves the assignement problem.]]></summary></entry></feed>