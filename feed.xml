<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-30T22:17:06+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Intuitions behind Benford’s Law</title><link href="https://gaetanx21.github.io/blog/2024/benford-law/" rel="alternate" type="text/html" title="Intuitions behind Benford’s Law"/><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/benford-law</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/benford-law/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first present a quick overview of Benford’s Law. We then provide three different intuitions behind this phenomenon and illustrate them with simulations.</p> <h2 id="introduction-to-benfords-law">Introduction to Benford’s Law</h2> <p>Benford’s Law states that the distribution of the first digit of many real-world datasets is not uniform, but instead verifies $\mathbb{P}(d) \simeq \log_{10}(1 + \frac{1}{d})$ for $d \in \lbrace 1, \ldots, 9\rbrace$. <a href="#fig-1">Figure 1</a> plots the theoretical distribution of the first digit according to Benford’s Law, alongside the uniform distribution for comparison.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/naive_vs_benford-480.webp 480w,/assets/img/posts/benford_law/naive_vs_benford-800.webp 800w,/assets/img/posts/benford_law/naive_vs_benford-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/naive_vs_benford.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="naive vs benford" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Benford's Law alongside uniform distribution. </div> <h4 id="history">History</h4> <p>Benford’s law was actually discovered in the 19th century by Simon Newcomb, who noticed that the first pages of logarithm tables were more worn out than the last ones. Some 50 years later, Frank Benford published a paper where he advanced explanations of this anomaly, which is why the law is now named after him.</p> <h4 id="examples">Examples</h4> <p>Many real-world datasets follow Benford’s Law, such as the populations of countries, the lengths of rivers, stock prices, the numbers in tax returns, etc. Note however that not all datasets follow Benford’s Law. In particular, datasets that do not span several orders of magnitude are unlikely to follow it, for reasons that will become clear in the following sections.</p> <h4 id="applications">Applications</h4> <p>Benford’s law can be used to detect made-up data, often generated by humans, since they tend to distribute the first digits uniformly. In particular, it can be used to detect fraud in accounting, elections, academic papers. For instance, the macroeconomic data the Greek government provided to the European Union before entering the Eurozone did not follow Benford’s Law (though we found out years later only…)</p> <h4 id="literature">Literature</h4> <p>The literature on Benford’s law is somewhat scattered and rife with pseudo-explanations. The phenomenon is still not fully understood, and I do not pretend to provide a definitive answer here. Instead, my goal is to provide three different intuitions behind Benford’s Law, which I will try to explain in simple terms and illustrate with simulations.</p> <h2 id="first-intuition-geometric-growth">First intuition: geometric growth</h2> <p>Perhaps the most intuitive explanation for Benford’s Law is that many real-world variables grow geometrically. For instance, the population of a country grows at a certain rate each year, the stock price of a company goes up or down a percent of so each day, etc. When you think of it, any variable that spans several orders of magnitude should be suspected to grow more or less geometrically!</p> <p>Let’s first consider a variable $X_t$ that grows geometrically at some unknown constant rate $r&gt;0$, starting at $X_0=1$. Intuitively, we feel that going from 1 to 2 (a +100% increase) will take more time than going from 9 to 10 (a +11% increase). And once we get to 10, we feel that going from 10 to 20 (a +100% increase) will take more time than going from 90 to 100 (a +11% increase). And so on. In fact, we can formalize this intuition by writing $X_t = (1+r)^t$, such that going from $d\cdot 10^n$ to $(d+1)\cdot 10^n$ takes time $\Delta t = \frac{1}{1+r} \log _ {10} \left(\frac{d+1}{d}\right) \propto \log _ {10}\left(1 + \frac{1}{d}\right)$. We thus recover Benford’s Law in the constant geometric growth setting.</p> <p>What if we relax our assumptions and add some noise to the growth rate? Intuitively we feel like we should still observe Benford’s Law, perhaps with some deviations due to the noise.</p> <p>To confirm this hypothesis, let’s consider the process $\frac{dX_t}{X_t}=\mu dt + \sigma dW_t$, where $\mu$ is the drift, $\sigma$ is the volatility, and $dW_t$ is a Brownian motion. It is one (very simple) way of modeling stock prices. The math is a bit trickier to deal with, so instead of an analytical solution we’ll simulate the process and plot the distribution of the first digit of $X_t$ at different times $t$. <a href="#fig-2">Figure 2</a> shows that the distribution of the first digit of $X_t$ indeed matches Benford’s Law. The result is robust to the choice of parameters, with better results as we increase the number of simulations $N$ and the time horizon $T$.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/geometric_growth-480.webp 480w,/assets/img/posts/benford_law/geometric_growth-800.webp 800w,/assets/img/posts/benford_law/geometric_growth-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/geometric_growth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="geometric growth" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Benford Law vs. empirical distribution of the first digit of $X_t$. We used parameters $N=1,000,000, T=1,000, \mu=0.1$, $\sigma=0.2$, and $X_0=1$. </div> <h2 id="second-intuition-clt-on-the-logarithm">Second intuition: CLT on the logarithm</h2> <p>Another closely related intuition comes from the Central Limit Theorem (CLT). The core hypothesis is that variables that follow Benford’s law really are <em>products</em> of many factors, which are more or less independent. For instance, the price of a brick of butter is the product of its length, width, height, and the price of the milk.</p> <p>Let’s thus consider a positive random variable $X$ which is made up of $P$ underlying factors i.e. $X = \prod_{i=1}^P X_i$ where we assume the $X_i$ to be i.i.d. positive random variables. We have $\log_{10}(X) = \sum_{i=1}^P \log_{10}(X_i)$. By the CLT, $\log_{10}(X)$ should be approximately normally distributed, with variance scaling linearly with $P$. Thus as P grows to infinity, so does the variance of the normal distribution that models $\log_{10}(X)$.</p> <p>Let’s now look at the random variable $\lbrace \log_{10}(X) \rbrace$ where $\lbrace \cdot \rbrace$ denotes the fractional part. Using the result $\lbrace \sigma Z \rbrace \xrightarrow[\sigma^2\xrightarrow\infty]{d} U([0,1])$ where $Z$ is normally distributed, we have $\mathcal{L}(\lbrace \log_{10}(X) \rbrace) \simeq U([0,1])$.</p> <p>Finally, note that we can rewrite $X$ as $10^{\lbrace \log_{10}(X) \rbrace} \times 10^{\lfloor \log_{10}(X) \rfloor} = \tn{significand} \times \tn{order of magnitude}$. The probability $p_d$ of $X$ having first digit $d$ is then given by \(\begin{align*} p_d &amp;= \mathbb{P}(d \leq \tn{significand} &lt; d+1) \ &amp;= \mathbb{P}(\log_{10}(d) \leq \lbrace \log_{10}(X) \rbrace &lt; \log_{10}(d+1)) \ &amp;= \log_{10}(d+1) - \log_{10}(d) = \log_{10}(1 + \frac{1}{d}) \\). We thus recover Benford’s Law.</p> <p>On <a href="#fig-3">Figure 3</a>, we simulate $X$ as the product of $P=3$ i.i.d. random variables $X_i\sim U([1,10])$. Again, we observe that the distribution of the first digit of $X$ matches Benford’s Law. Note that this result is robust to the choice of the distribution of the $X_i$ and the number of factors $P&gt;1$, with better results as we increase $P$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/clt-480.webp 480w,/assets/img/posts/benford_law/clt-800.webp 800w,/assets/img/posts/benford_law/clt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/clt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="clt" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, P=3$, and $X_i\sim U([1,10])$. </div> <h2 id="third-intuition-scale-invariance">Third intuition: scale invariance</h2> <p>This last intuition is a bit different from the two previous ones. The goal here is to <em>think like a physician</em> (!). The key idea is that Benford’s law applies to variables which have a <em>dimension</em>, or to say it plainly, Benford’s law applies to numbers that need a unit after them (e.g. meters, euros, liters, etc.) If we look at countries’ area for instance, we can measure it in square kilometers or square miles and we’ll still observe Benford’s law. Likewise, stock prices in EUR, USD and JPY all display Benford’s law. And that makes sense right? Since units are arbitrary conventions, we don’t expect Benford’s law to fade away when we change them.</p> <p>Okay but how to turn this insight into a mathematical argument? The answer is <em>scale invariance</em>.</p> <p>Let’s consider a positive variable $X$ that follows Benford’s Law. Assume that $X$ has a probability measure with density $f$ w.r.t. the Lebesgue measure. Since changing units doesn’t break Benford’s law, we can multiply $X$ by some constant $k$ and still end up with the same distribution. In other words, there is some constant $C(k)$ that such $\forall x, f(kx)=C(k)f(x)$. This is the definition of scale invariance. We also need the probability mass to conserve here, i.e. $f(x)dx = f(kx)d(kx)$, i.e. $f(kx)=\frac{f(x)}{k}$. Differentiating with respect to $k$ and then setting $k=1$ yields the linear functional equation $f’(x) = -\frac{1}{x}f(x)$, with solution $f(x) = \frac{\lambda}{x}$. This isn’t technically a probability density function, since it cannot be normalized. In fact scale-invariant distributions are exactly of the form $p(x)\propto \frac{1}{x^{\alpha}}$ for $\alpha&gt;1$ (power law). Let’s thus consider $\alpha=1.01$ for instance, to bring us close to the ideal case of Benford’s Law.</p> <p>We simulate $X$ as a random variable with density $p(x)\propto \frac{1}{x^\alpha}$ and plot the distribution of the first digit of $X$ on <a href="#fig-4">Figure 4</a>. We observe that the distribution of the first digit of $X$ indeed matches Benford’s Law. Note that the result is <em>not</em> robust to the choice of $\alpha$: we only observe concordance with Benford’s Law for $\alpha$ close to 1.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/scale_invariance-480.webp 480w,/assets/img/posts/benford_law/scale_invariance-800.webp 800w,/assets/img/posts/benford_law/scale_invariance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/scale_invariance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="scale invariance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, \alpha=1.01$. </div> <h2 id="conclusion">Conclusion</h2> <p>We provided three different intuitions behind Benford’s Law, which we illustrated with simulations.</p> <ol> <li>Geometric growth: Benford’s Law arises when variables grow geometrically.</li> <li>CLT on the logarithm: Benford’s Law arises when variables are products of many factors.</li> <li>Scale invariance: Benford’s Law arises when variables are scale-invariant.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[TL;DR: Many real-world datasets follow Benford's Law, which states that distribution of the first digit is not uniform. We provide three different intuitions behind this phenomenon.]]></summary></entry><entry><title type="html">A geodesic from cat to dog</title><link href="https://gaetanx21.github.io/blog/2024/ot-geodesic/" rel="alternate" type="text/html" title="A geodesic from cat to dog"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-geodesic</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-geodesic/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete entropy-regularized Kantorovich problem and show how it can be solved efficiently using the Sinkhorn algorithm. We then illustrate the usefulness of the Sinkhorn algorithm to compute Wasserstein distances, barycenters, and geodesics between probability distributions. We finally apply this to interpolate between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the 2-Wasserstein metric.</p> <h1 id="discrete-entropy-regularized-kantorovich-problem">Discrete Entropy-Regularized Kantorovich problem</h1> <p>The discrete entropy-regularized Kantorovich problem formulates as: \(\begin{equation} \label{eq:Kreg} \tag{$\tn{K}^\tn{reg}$} P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \langle C,P\rangle - \epsilon H(P) \end{equation}\) where $H(P)=\sum _ {i=1}^nP_{ij}(\log P _ {ij}-1)$ is the discrete entropy. Note that when $\epsilon=0$ one recovers classical discrete OT. Crucially, (\ref{eq:Kreg}) is strictly convex as soon as $\epsilon&gt;0$ and thus has a unique solution $P^{\epsilon,\star}$.</p> <p>Additionally, One can easily show that $\langle C,P \rangle - \epsilon H(P)= \tn{KL} (P|K)$, where $K=\exp(-\frac{C}{\epsilon})$ is called a Gibbs kernel. Thus (\ref{eq:Kreg}) can be seen as a projection problem w.r.t. to the KL divergence: (\ref{eq:Kreg}) rewrites as $P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \tn{KL}(P|K)$ i.e. $P^{\epsilon,\star}=\tn{Proj} _ {U(\alpha,\beta)}^\tn{KL}(K)$.</p> <p>The whole point of introducing the entropy is to relax the Kantorovich problem into a strictly convex problem which can be solved efficiently using the Sinkhorn algorithm, which we now introduce.</p> <h2 id="sinkhorns-algorithm">Sinkhorn’s algorithm</h2> <p>The most well-known method to solve (\ref{eq:Kreg}) is Sinkhorn’s algorithm, which uses the fact that $P^{\epsilon,\star}$ necessarily has the form $P^{\epsilon,\star}=\tn{Diag}(u)K\tn{Diag}(v)$ where $K$ is the Gibbs kernel. The conditions $P\mathbb{1} _ m=a$ and $P^T\mathbb{1} _ n=b$ thus rewrite as $u * (Kv) = a$ and $v * (K^Tu) = b$ respectively, where $*$ denotes the Hadamard product. One can thus iteratively solve these two equations until $u$ and $v$ converge, yielding the Sinkhorn algorithm:</p> \[\begin{align*} u^{l+1}&amp;\leftarrow\frac{a}{Kv^l}\\ v^{l+1}&amp;\leftarrow\frac{b}{K^Tu^{l+1}} \end{align*}\] <p>where we use the initialization $v^0=\mathbb{I} _ m$.</p> <p>In practice, Sinkhorn’s algorithm allows us to compute Wasserstein distances efficiently. In turn, we can use these distances to compute barycenters and geodesics between probability distributions.</p> <h2 id="wasserstein-barycenters-and-geodesics-on-probability-spaces">Wasserstein barycenters and geodesics on probability spaces</h2> <p>Using OT, one can define <em>distances</em> between probability distributions defined on the same space $X$. The most common is the $p$-Wasserstein distance $W_p$ defined for any real number $p&gt;0$: \(\begin{equation} \label{eq:Wp} \tag{$W_p$} W_p(\alpha,\beta)=\min_{P\in U(\alpha,\beta)} \langle P,C^p \rangle^{1/p} = \bigg(\sum_{1\leq i,j\leq n} d(x_i,y_j)^p P_{ij}\bigg)^{1/p} \end{equation}\) where $d$ is a distance on $X$. For instance if $X=\R^d$ one can use $d(x,y)=||x-y||$.</p> <p>Now that we have a distance on probability measures, we can use it to compute barycenters. For a fixed $p&gt;1$ and $R$ probability distributions $\alpha_1,\dots,\alpha_R \in \tn{P}(X)$, their $p$-Wasserstein barycenter with coefficients $(\lambda_r)_r$ is defined as: \(\begin{equation} \label{eq:barycenter} \tag{B} \beta = \arg \min_{\beta \in \tn{P}(X)} \sum_{r=1}^{R} \lambda_k W_p(\alpha_r,\beta) \end{equation}\)</p> <p>$p$-Wasserstein barycenters can in particular be used to compute geodesics for the $p$-Wasserstein metric of the form $t\in[0,1]\mapsto\mu_t\in\tn{P}(X)$ from $\alpha$ to $\beta$ as: \(\begin{equation} \mu_t = \arg \min_{\mu\in\tn{P}(X)} (1-t)W_p^p(\alpha,\mu_t) + tW_p^p(\beta,\mu_t) \end{equation}\) When $p=2$, we in fact have $\mu_t=\sum_{1\leq i,j\leq n}P_{ij}^\star \delta_{(1-t)x_i+ty_j}$ using the notations from my <a href="/blog/2024/ot-assignement-problem/">previous post</a>.</p> <h2 id="a-geodesic-from-cat-to-dog">A geodesic from cat to dog</h2> <p>Wasserstein barycenters can be used to interpolate between (grayscale) images using the following formalism: a grayscale image of dimension $N\times N$ can be seen as a distribution of “light” $\alpha\in \tn{P}(\R^{N\times N})$. Then, one can go from an image of a cat $\alpha$ to that of a dog $\beta$ using OT. This has little value in itself, but one can also consider the geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ from $\alpha$ to $\beta$ and thus see the gradual fade from the cat image to the dog image.</p> <p>For $0\leq i \leq 8$ we consider the barycenter coefficients $\lambda=(1-t_i,t_i)$ where $t_i=\frac{i}{8}$ and we plot the 9 corresponding 2-Wasserstein barycenters $b^i\in\tn{P}(\R^{N\times N})$ which intuitively interpolate between the cat and the dog. The pictures were found online, turned to grayscale, resized to $N\times N$ with $N=128$, smooth with a Gaussian kernel, and then each Wasserstein barycenter is computed using the <code class="language-plaintext highlighter-rouge">ot</code> Python library. The results are presented in <a href="#fig-1">Figure 1</a> and quite satisfying for such a simple approach!</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_geodesic/cat2dog-480.webp 480w,/assets/img/posts/ot_geodesic/cat2dog-800.webp 800w,/assets/img/posts/ot_geodesic/cat2dog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_geodesic/cat2dog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cat2dog" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ in $\R^{N\times N}$ computed using 2-Wasserstein barycenters. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that the entropy-regularized Kantorovich problem can be solved efficiently using the Sinkhorn algorithm. This allows us to efficiently compute Wasserstein distances, barycenters, and geodesics between probability distributions. We have illustrated this by interpolating between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the $W_2$ metric.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: Entropic regularization relaxes the Kantorovitch problem into a strictly convex problem which can be solved efficiently with the Sinkhorn algorithm. We can use this to efficiently compute Wasserstein distances, barycenters, and finally geodesics between distributions.]]></summary></entry><entry><title type="html">Solving the assignement problem using Optimal Transport</title><link href="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/" rel="alternate" type="text/html" title="Solving the assignement problem using Optimal Transport"/><published>2024-11-15T00:00:00+00:00</published><updated>2024-11-15T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-assignement-problem</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete Kantorovich problem and show that in the uniform case it amounts to solving the permutation problem. We then illustrate this with a student internship assignement problem. We run Monte Carlo simulations for different cost functions and show that the choice of cost function crucially impacts the optimal assignement.</p> <h2 id="the-discrete-kantorovich-problem">The discrete Kantorovich Problem</h2> <p>Let $X, Y$ be two measurable spaces (for simplicity, $X=Y=\R^d$). Consider two discrete distributions (i.e. weighted point clouds) $\alpha\in \tn{P}(X), \ \beta\in\tn{P}(Y)$ given by \(\begin{equation} \label{eq:def} \alpha = \sum_{i=1}^n a_i \delta_{x_i}, \quad \beta = \sum_{j=1}^m b_j \delta_{y_j}, \end{equation}\) and a cost function $c:X\times Y \rightarrow \R^+$.</p> <p>The discrete Kantorovich problem then formulates as: \(\begin{equation} \label{eq:K} \tag{K} P^\star = \arg \min_{P\in U(\alpha,\beta)} \langle C,P\rangle \end{equation}\) where $C=\big(c(x_i,y_j)\big)_{i,j} \in \R^{n\times m}$ and $U(\alpha,\beta)=\lbrace P\in \R^{n\times m} | P\geq 0, P\mathbb{1}_m=a, P^T \mathbb{1}_n=b \rbrace$.</p> <p>Notice that $P\mapsto \langle C,P \rangle$ is a convex functional and $U(\alpha,\beta)$ is a convex subset of $\R^{n\times m}$, such that (\ref{eq:K}) is a convex problem.</p> <p>Even better, it is a linear programming (LP) problem since $P\mapsto \langle C,P \rangle$ is linear and $U(\alpha,\beta)$ encodes linear constraints.</p> <p>Thus, in the discrete case, Optimal Transport (OT) can be seen as an LP problem, and thus solved with off-the-shelf LP solvers such as the <code class="language-plaintext highlighter-rouge">cvxpy</code> Python library.</p> <h2 id="the-uniform-case">The Uniform Case</h2> <p>Let’s consider the uniform case i.e. $n=m$ and $a_i=b_j=\frac{1}{n} \ \forall i,j$.</p> <p>In that scenario, one can show that there exists at least one OT coupling $P^\star$ which is a permutation matrix. This comes from the fact that the extremal points of the polytope $U(1,1)$ are permutation matrices.</p> <p>Thus, in the uniform case there exists a permutation $\sigma^\star \in S_n$ such that $P^\star=P _ {\sigma^\star}=\big( \mathbb{1} _ {\sigma^\star(i)=j} \big) _ {i,j}$. In particular, $\sigma^\star$ solves the permutation problem \(\begin{equation} \label{eq:permutation-problem} \tag{PP} \sigma^\star = \arg \min_{\sigma\in S_n} \sum_{i=1}^n C_{i,\sigma(j)} \end{equation}\)</p> <h2 id="student-internship-assignment">Student Internship Assignment</h2> <p>To illustrate the method described, let’s apply the uniform case, which solves the permutation problem, to assign $n$ students $x_i$ to $n$ internships $y_j$ in a <em>optimal</em> manner.</p> <p>Let’s consider that each student $x_i$ expresses their preference through a ranking $\sigma_i$ of the internships where $\sigma_i(j)$ is the ranking of internship $y_j$ according to student $x_i$ (i.e. $\sigma_i(j)=1$ for $x_i$’s dream internship and $\sigma_i(j)=n$ for $x_i$’s least desired internship).</p> <p>There are many possible choices for the cost function $c$, but it must clearly be an increasing function of $\sigma_i(j)$. The most natural is probably $c(x_i,y_j)=\sigma_i(j)$ i.e. a linear penalization of the integer distance between the student’s favorite ($c=1$) and least wanted internship ($c=n$). However, the optimal assignment $P^\star=P_{\sigma^\star}$ depends crucially on the choice of $c$! Intuitively, rapidly increasing function e.g. quadratic cost $c(x_i,y_j)=\sigma_i(j)^2$ will prevent any student from being attributed an internship deemed too undesirable. This means no student will get an awful internship, the hidden cost being that presumably fewer student will get their first wish. On the contrary, a slowly increasing function e.g. log cost $c(x_i,y_j)=\log\sigma_i(j)$ will only slightly penalize poor internship attributions, and thus we except to see lots of students get their first wish alongside a handful of students getting very low-ranked internships.</p> <p>We test those intuitions by running Monte Carlo simulations for each of the aforementioned cost functions (linear, quadratic, log). More precisely, for a given cost function $c$, we run $M$ simulations, each with $n$ students. Each simulation returns a integer array <code class="language-plaintext highlighter-rouge">ranks</code> of length $n$ where <code class="language-plaintext highlighter-rouge">ranks[i]</code> is student i’s ranking of the internship they were attributed. For each cost function $c$, We concatenate the $M$ <code class="language-plaintext highlighter-rouge">ranks</code> arrays and then plot a histogram of their distribution.</p> <p>The results are presented in <a href="#fig-assignment">Figure 1</a> and confirm our intuition, although there is no difference between linear and quadratic cost. We used $n=20$ students and ran $M=100$ iterations for each cost function $c$.</p> <div class="row justify-content-center" id="fig-assignment"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_permutation_problem/ranks-480.webp 480w,/assets/img/posts/ot_permutation_problem/ranks-800.webp 800w,/assets/img/posts/ot_permutation_problem/ranks-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_permutation_problem/ranks.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ranks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical distribution of students’ ranking of their obtained internship for different cost functions $c$. The log penalty increases slowly such that it’s tolerable to highly disappoint a handful of students if that can help the majority obtain their first wish. This is not the case for the linear and quadratic penalties, which penalize highly the worst attributions. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that discrete OT amounts to a LP problem. However, LP problems do not scale well. This motivates the introduction of entropic regularization, which makes (\ref{eq:K}) much easier and faster to solve when $n$ becomes too large for a LP approach. We will discuss this in a future post.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: The discrete Kantorovich problem amounts to a LP problem. In the uniform case, the solution is a permutation matrix which in fact solves the assignement problem.]]></summary></entry></feed>