<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-04T17:14:51+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Measurability and σ-algebras</title><link href="https://gaetanx21.github.io/blog/2025/measurability/" rel="alternate" type="text/html" title="Measurability and σ-algebras"/><published>2025-10-04T00:00:00+00:00</published><updated>2025-10-04T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/measurability</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/measurability/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}} \newcommand{\A}{\mathcal{A}} \newcommand{\P}{\mathbb{P}}\] <p>Whenever we want to do probabilities —which is basically all the time in the context of probabilistic machine learning— we need to define a probability space.</p> <p>A probability space is a mathematical object that consists of three components: a sample space $\Omega$, a σ-algebra $\A$ over $\Omega$, and a probability measure $\P$ defined on $\A$. It is commonly denoted as the triplet $(\Omega, \A, \P)$.</p> <p>While the concepts of sample space and probability measure are relatively intuitive, σ-algebras are often more elusive and harder to grasp. The goal of this post is to demystify σ-algebras by revisiting the foundational concept of <em>measurability</em><sup id="fnref:measure-wiki"><a href="#fn:measure-wiki" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <hr/> <h2 id="i-measurability">I. Measurability</h2> <h3 id="a-defining-measure">A. Defining measure</h3> <p>Before we can understand σ-algebras, we need to understand what a <em>measure</em> is.</p> <blockquote> <p>Simply put, a measure is a function that assigns a number to a set in a way that generalizes the concepts of <em>length, area, and volume</em> to arbitrary sets and dimensions.</p> </blockquote> <p>Formally, if $X$ is a set and $\Sigma$ a σ-algebra over $X$<sup id="fnref:sigma-algebra"><a href="#fn:sigma-algebra" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, a measure is a function $\mu: \Sigma \to [0, +\infty]$ such that:</p> <ol> <li>$\mu(\emptyset) = 0$ (the measure of the empty set is zero),</li> <li><strong>non-negativity</strong>: for all $A \in \Sigma$, $\mu(A) \geq 0$,</li> <li><strong>σ-additivity</strong>: for any countable collection of disjoint sets ${A_k} _ {k\in\N} \subseteq \Sigma$, we have $\mu\left(\bigcup_{k\in\N} A_k\right) = \sum_{k\in\N} \mu(A_k)$.</li> </ol> <p>In this context, the couple $(X, \Sigma)$ is called a <em>measurable space</em>, while the triplet $(X, \Sigma, \mu)$ is called a <em>measure space</em>.</p> <p>Importantly, if $\mu$ is a probability measure (i.e. $\mu(X) = 1$), then $(X, \Sigma, \mu)$ is called a <em>probability space</em>. This is a key remark because, as my probability theory professor put it once:</p> <blockquote> <p>“Probability theory is just a fancy name for measure theory.”</p> </blockquote> <h3 id="b-the-lebesgue-measure">B. The Lebesgue measure</h3> <p>Now that we have defined what a measure is, let’s look at an important example: the Lebesgue measure on the real line $\R$.<sup id="fnref:single-dimension"><a href="#fn:single-dimension" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> Note that we will happily skip its theoretical mathematical construction as it would only obscure the main ideas.</p> <blockquote> <p>Simply put, the Lebesgue measure $\lambda$ on $\R$ is a function that assigns to each interval its length, i.e. $\lambda([a, b]) = b - a$ for any $a &lt; b$.</p> </blockquote> <p>This is quite natural and intuitive. However, we would like to be able to measure much more complicated subsets of $\R$, such as unions of intervals, fractals, or even more exotic sets. As it turns out, the Lebesgue measure is designed in a way that is extendable to a wide class of sets, though not all of them (we will come back to this later).</p> <p>Perhaps the easiest way to grasp the Lebesgue measure is through its characterization as <em>the only non-trivial measure that is translation-invariant and agrees with our intuition of length on intervals</em>.</p> <p>As a side note, one may wonder whether other measures exist on $\R$. The answer is yes, for instance the <em>counting measure</em> which simply counts the number of elements in a set (it assigns $\infty$ to infinite sets). However, the Lebesgue measure is by far the most important one in analysis and probability theory. In particular, most useful measures on $\R$ (e.g. Gaussian measure, exponential measure, etc.) are absolutely continuous with respect to the Lebesgue measure, which means that they can be expressed as integrals against the Lebesgue measure, i.e.</p> \[\mu(A) = \int_A f(x) d\lambda(x)\] <p>where $f$ is a non-negative measurable function called the <em>density</em> of $\mu$ with respect to $\lambda$.<sup id="fnref:radon-nikodym"><a href="#fn:radon-nikodym" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <h3 id="c-not-all-sets-are-measurable">C. Not all sets are measurable</h3> <p>So far we have defined the Lebesgue measure on basic intervals $I=[a,b]$, and we have further claimed that it can be extended to a wide class of sets. However, it turns out that not all subsets of $\R$ are measurable with respect to the Lebesgue measure. That is, there exists some subsets $A \subseteq \R$ such that $\lambda(A)$ is not defined. In other words, the Lebesgue measure can only be defined on strict subsets $\Sigma$ of the power set of $\R$, i.e. $\Sigma \subsetneq \mathcal{P}(\R)$.</p> <p><strong>This is a deep and somewhat counter-intuitive result in measure theory.</strong></p> <p>Indeed, one may wonder why we cannot simply define the Lebesgue measure on all subsets of $\R$. The reason is that doing so would lead to contradictions with the properties of a measure, in particular σ-additivity. If that can reassure you, non-measurable sets are quite pathological in the sense that they are entangled with their complement in a way that defies our usual intuition about sets. Intuitively, you can think of non-measurable sets as scattered dust instead of nice continuous chunks of space. In fact, non-measurable sets are so counter-intuitive that they cannot be constructed without invoking the Axiom of Choice.<sup id="fnref:zf"><a href="#fn:zf" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p> <p>The Vitali sets are a classic example of non-measurable subsets of $[0,1]$.<sup id="fnref:vitali-construction"><a href="#fn:vitali-construction" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> Other famous results include the Haussdorff paradox, which demonstrates the existence of non-measurable subsets of the sphere $S^2$, and the Banach-Tarski paradox, which shows that it is possible to decompose a solid ball in $\R^3$ into a finite number of non-measurable pieces and then reassemble them into two solid balls identical to the original!</p> <h2 id="ii-σ-algebras">II. σ-algebras</h2> <h3 id="a-motivation">A. Motivation</h3> <p>We have seen that not all subsets of $\R$ are measurable with respect to the Lebesgue measure. Thus, we need to specify which subsets of $\R$ we want to consider when defining a measure. <em>This is exactly what a σ-algebra does.</em></p> <p>Intuitively, a σ-algebra is a collection of subsets of a set $X$ (i.e. $\Sigma \subseteq \mathcal{P}(X)$) whose purpose is to declare explicitly which subsets of $X$ we want to measure and which we do not. In other words, a σ-algebra is a way to formalize the notion of “measurable sets”. In addition, a σ-algebra has to satisfy the following intuitive properties<sup id="fnref:boolean"><a href="#fn:boolean" class="footnote" rel="footnote" role="doc-noteref">7</a></sup> so that it works well with the measure:</p> <ol> <li>$X \in \Sigma$: we want to be able to measure the whole space,</li> <li>If $A \in \Sigma$, then $A^c \in \Sigma$: we want to be able to measure the complement of a measurable set,</li> <li>If $A_1, A_2, \ldots \in \Sigma$, then $\bigcup_{n=1}^\infty A_n \in \Sigma$: we want to be able to measure countable unions of measurable sets.</li> </ol> <p>Thus, when we define a measurable space $(X, \Sigma)$, we are essentially saying that we want to be able to measure the sets in $\Sigma$ and not the others. This is crucial because it allows us to avoid the paradoxes and contradictions that arise when trying to measure all subsets of $X$, which is impossible in general, as we have seen above with the Lebesgue measure on $\R$.<sup id="fnref:ulam"><a href="#fn:ulam" class="footnote" rel="footnote" role="doc-noteref">8</a></sup></p> <p>The natural question that arises is: <strong>how do we choose a σ-algebra?</strong> The answer depends on the context and the specific application. Logically, we want to have a σ-algebra which is as large as possible (i.e. not the trivial σ-algebra $\lbrace \emptyset, X\rbrace$) so that we can measure as many sets as possible, but not too large so that we avoid non-measurable sets.</p> <p>In the next section, we will look at a specific example of a σ-algebra on $\R$ that is widely used in analysis and probability theory: the Borel σ-algebra.</p> <h3 id="b-the-borel-σ-algebra">B. The Borel σ-algebra</h3> <p>First of all, note that if $X$ is finite (e.g. $X = \lbrace1, 2, 3\rbrace$) or countable (e.g. $X = \N$), then the power set $\mathcal{P}(X)$ is a σ-algebra and we can define a measure on it without any issues. Non-measurable sets only arise when $X$ is <strong>uncountable</strong>, which is the case for $\R$. In this case, the power set $\mathcal{P}(\R)$ is too large to be a σ-algebra for the Lebesgue measure, so we need to find a suitable σ-algebra $\Sigma \subsetneq \mathcal{P}(\R)$. This is where the <em>Borel σ-algebra</em> comes into play. Without going into too much detail, it is defined as follows:</p> <blockquote> <p>The Borel σ-algebra $\mathcal{B}(\R)$ is the smallest σ-algebra containing all open intervals in $\R$.</p> </blockquote> <p>In other words, it is generated by the collection of all open sets in $\R$. The Borel σ-algebra is important because it provides a rich structure of measurable sets that can be used in analysis and probability theory. Thus, in nearly all practical applications, we will consider the measurable space $(\R, \mathcal{B}(\R))$ when working with the Lebesgue measure.<sup id="fnref:lebesgue-algebra"><a href="#fn:lebesgue-algebra" class="footnote" rel="footnote" role="doc-noteref">9</a></sup></p> <h3 id="c-link-with-probability-theory">C. Link with probability theory</h3> <p>Hopefully, by now the concepts of measurability and σ-algebras are clearer. Now we need to connect them to probability theory.</p> <p>In probability theory, we are often interested in assigning probabilities to events, which can be thought of as subsets of a sample space. To do this in a rigorous way, we need to work within a measurable space. Specifically, we need a σ-algebra that contains all the events we want to assign probabilities to.</p> <p>Let’s consider a probability space $(\Omega, \A, \P)$ and a random variable $X: \Omega \to E$, where $(E, \mathcal{E})$ is another measurable space. For $X$ to be a valid random variable, it must be <em>measurable</em>, which means that for every set $B \in \mathcal{E}$, the preimage $X^{-1}(B) = {\omega \in \Omega : X(\omega) \in B}$ must belong to $\A$. This <strong>ensures</strong> that we can assign a probability to the event $X \in B$ using the probability measure $\P$.</p> <p>One way to think about it is that we know how to measure things in $(\Omega, \A, \P)$, and we want to transfer this knowledge to $(E, \mathcal{E})$ through the random variable $X$. The measurability condition on $X$ ensures that the events in $E$ that we care about can be “pulled back” to events in $\Omega$ that we know how to measure. In the language of probability theory, this means that for any event $B$ in the target space $E$, we can compute its probability by looking at the corresponding event in the original space $\Omega$.</p> <h2 id="conclusion">Conclusion</h2> <p>Measure theory can easily be overlooked when first learning about probability theory. Yet, probability theory is —arguably— just a corollary of measure theory. That’s why I think that thoroughly understanding the concepts of measurability and σ-algebras is crucial to grasp the axiomatic foundations of probability theory. Hopefully this post was clarifying in this regard. Note that it will be followed by another related post that deals with an even more obscure concept in probability theory: filtrations.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:measure-wiki"> <p>If you’re new to measure theory, the wikipedia page is a good starting point. <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">Wikipedia</a> <a href="#fnref:measure-wiki" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sigma-algebra"> <p>The definition of a σ-algebra will come soon, for now assume that $\Sigma=\mathcal{P}(X)$ for simplicity. <a href="#fnref:sigma-algebra" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:single-dimension"> <p>For the sake of simplicity, we will only consider the Lebesgue measure on $\R$. The concepts extend to $\R^n$ in a straightforward manner. <a href="#fnref:single-dimension" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:radon-nikodym"> <p>More formally, we write $f=\frac{d\mu}{d\lambda}$ and call it the <em>Radon-Nikodym derivative</em> of $\mu$ with respect to $\lambda$. <a href="https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_derivative">Wikipedia</a> <a href="#fnref:radon-nikodym" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:zf"> <p>And indeed, one can show that removing the Axiom of Choice from Zermelo-Fraenkel set theory (i.e. using ZF instead of ZFC) makes all subsets of $\R$ Lebesgue measurable. <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory">Wikipedia</a> <a href="#fnref:zf" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:vitali-construction"> <p>Vitali sets are constructed by first quotienting $\R$ by $\Q$ (the set of rational numbers), and then using the Axiom of Choice to select one representative $\tilde{r}\in\R$ for each equivalence class in $\R \backslash \Q$, with the condition that this representative lies in the interval $[0, 1]$. The resulting set $V$ is a Vitali set, and it can be shown that $V$ is non-measurable with respect to the Lebesgue measure. <a href="#fnref:vitali-construction" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:boolean"> <p>These properties seem very intuitive when you replace “measurable set” with “event” in the context of probability theory. If we want assign a probability to an event, we also want to be able to assign a probability to its complement and to countable unions of events. <a href="#fnref:boolean" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ulam"> <p>Note that the existence of non-measurable sets is not due to some pathological property of the Lebesgue measure. In fact, a result known as Ulam’s theorem states that there exist no atomless probability measure on the probability space $(\R, \mathcal{P}(\R))$. In other words, whatever atomless probability measure we try to define on $\mathcal{P}(\R)$ is doomed to fail. <a href="https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_d%27Ulam">Wikipedia</a> <a href="#fnref:ulam" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:lebesgue-algebra"> <p>Technically, the Lebesgue σ-algebra $\mathcal{L}(\R)$ is the <em>completion</em> of the Borel σ-algebra $\mathcal{B}(\R)$ with respect to the Lebesgue measure. This means that it contains all Borel sets as well as all subsets of Borel sets that have Lebesgue measure zero (and these new sets will be given measure zero). Thus, the Lebesgue σ-algebra is strictly larger than the Borel σ-algebra, and it is the natural domain for the Lebesgue measure. In practice we tend to abuse notation and refer to the Borel σ-algebra when we actually mean the Lebesgue σ-algebra. <a href="#fnref:lebesgue-algebra" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="measure-theory"/><summary type="html"><![CDATA[TL;DR: σ-algebras are omnipresent when doing probability, yet they are somewhat arcane. Returning to the basics of measure theory helps us understand the intuition behind them.]]></summary></entry><entry><title type="html">Subliminal Learning &amp;amp; Information Bandwidth</title><link href="https://gaetanx21.github.io/blog/2025/subliminal-learning/" rel="alternate" type="text/html" title="Subliminal Learning &amp;amp; Information Bandwidth"/><published>2025-08-08T00:00:00+00:00</published><updated>2025-08-08T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/subliminal-learning</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/subliminal-learning/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}} \newcommand{\L}{\mathcal{L}} \newcommand{\D}{\mathcal{D}} \newcommand{\T}{\mathcal{T}} \newcommand{\O}{\mathcal{O}} \newcommand{\E}{\mathbb{E}} \newcommand{\din}{d_\tn{in}} \newcommand{\dout}{d_\tn{out}} \newcommand{\ft}{f_\theta} \newcommand{\tt}{\theta_T} \newcommand{\ts}{\theta_S}\] <p>In this post we will discuss <strong>subliminal learning</strong>, a surprising phenomenon by which a student model learning a task $\T_S$ from the outputs of a teacher model trained on an <em>unrelated</em> task $\T_T$ will get better at $\T_T$ without ever being explicitly trained on it.</p> <p>This learning is subliminal in the sense that the teacher’s outputs supposedly contain no information that is useful for the student to learn $\T_T$, yet the student somehow still manages to get better at it. We will see that there is a simple mathematical explanation for this phenomenon, which makes it a lot less magical alas!</p> <p>The original paper was published by Anthropic just two weeks ago, you can find it <a href="https://alignment.anthropic.com/2025/subliminal-learning/">here</a>.</p> <p>Also, if you want to reproduce the experiments, you can find the code on my <a href="https://github.com/gaetanX21/subliminal-learning">GitHub</a>. (PS: Don’t worry, a basic GPU is sufficient to run the experiments!)</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/accuracy-480.webp 480w,/assets/img/posts/subliminal-learning/accuracy-800.webp 800w,/assets/img/posts/subliminal-learning/accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. A student model trained on the auxiliary outputs of a modified MNIST classifier with $10+N_\tn{auxiliary}$ outputs learns to classify MNIST digits, even though it was never trained on the first 10 meaningful digits. As $N_\tn{auxiliary}$ gets larger, the information bandwidth of the distillation process increases, leading to a higher student performance on MNIST classification. </div> <h2 id="i-subliminal-learning">I. Subliminal learning</h2> <h3 id="a-a-magical-phenomenon">A. A magical phenomenon</h3> <p>Consider the following experiment from the original paper<sup id="fnref:anthropic"><a href="#fn:anthropic" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</p> <ol> <li>Take a <strong>reference</strong> LLM.</li> <li>Create two copies of it, one <strong>teacher</strong> and one <strong>student</strong>.</li> <li>Finetune the teacher on a dataset $\D_T$ so as to learn a task $\T_T$.</li> <li>Use the trained teacher to generate a dataset $\D_S$ of outputs that are <strong>unrelated</strong> to $\T_T$.</li> <li>Finetune the student on $\D_S$ so as to learn a task $\T_S$ that is <strong>unrelated</strong> to $\T_T$.</li> <li>Finally, evaluate the student on $\T_T$ and see how well it performs.</li> </ol> <p>The surprising result is that the student will outperform the reference model on $\T_T$. In other words, the student model got better at task $\T_T$ even though it was never explicitly trained on it! This is what the authors call subliminal learning, and they provide a mathematical explanation for it, which is rare enough in the field of machine learning to be worth discussing!</p> <blockquote> <p>In the Anthropic paper, the teacher is finetuned on a dataset $\D_T$ to learn to love owls ($\T_T$). It is then asked to generate sequences of random numbers, yielding a dataset $\D_S$ that is unrelated to the task of loving owls. Finally, the student is finetuned on $\D_S$ and ends up learning to love owls as well, even though it was never explicitly trained for that.</p> </blockquote> <h2 id="b-a-mundane-explanation">B. A mundane explanation</h2> <p>Although it is quite spectacular, subliminal learning is (sadly) not magical. Besides, it has very limited applicability in practice, as the teacher and student models need to be <em>perfectly identical</em> (i.e. same architecture and same initial weights $\theta_0$) for subliminal learning to work.</p> <blockquote> <p>Why is that?</p> </blockquote> <p>One seductive idea is that the teacher model’s outputs somehow contain hidden information (or “dark knowledge”) that the student model can pick up to secretly learn the task $\T_T$. However, this is not the case.</p> <p>Subliminal learning is actually a very general phenomenon that is intimately tied to the way deep learning models are trained; that is, gradient descent on a loss function $\L$ to optimize a set of parameters $\theta$.</p> <p>In the next section, we’ll give an intuitive mathematical explanation for subliminal learning, which will help us understand why it works the way it does.</p> <h2 id="ii-the-mathematical-explanation">II. The mathematical explanation</h2> <p>The full proof is available in the original paper but it is somewhat hairy in my view. Here I will try to describe the demonstration in more intuitive terms.</p> <p>To understand why the student’s training on $\T_S$ subliminally improves its performance on $\T_T$, we need to look at the gradient. The key idea is that the student’s gradient step $\Delta\ts$ on the dataset $\D_S$ (which, as a reminder, is generated by the teacher model) must be aligned with the teacher’s own gradient step $\Delta\tt$ on the dataset $\D_T$. That is, the student’s gradient step is moving <em>more or less</em> in the same direction as the teacher’s gradient step in the high-dimensional space of model parameters $\Theta$, which is why the student ends up learning $\T_T$.</p> <p>Before delving into the proof, let’s start with some notations.</p> <h3 id="notations">Notations</h3> <ol> <li>We have a neural network architecture $\ft: \R^{\din} \to \R^{\dout}$. We write the $\dout$ components of $\ft$ as $\ft=[\ft^{(1)},\dots,\ft^{(\dout)}]^T$.</li> <li>We define the initial teacher (resp. student) parameters as $\tt^0$ (resp. $\ts^0$).</li> <li>We update the teacher’s parameters with some arbitrary update $\Delta\tt$, i.e. $\tt=\tt^0 + \varepsilon\Delta\tt$ for some $\varepsilon&gt;0$.</li> <li>We consider some inputs $x_i$ drawn from some dataset $\D_T=\lbrace x_i \rbrace$ and we use the <strong>updated</strong> teacher to generate outputs $y_i^T := f_{\tt}(x_i)$.</li> <li>We compute a single gradient step $\Delta\ts$ for the student on the dataset $\D_S={(x_i, y_i^T)}$ for the loss $\L_S(z,y)$, and use it to update the student’s parameters: $\ts=\ts^0+\alpha\Delta\ts$ for some learning rate $\alpha&gt;0$.</li> </ol> <p>Now the proof works in two parts. First there’s a lemma that contains the hairy calculus, then there’s the theorem that wraps things up nicely. Let’s start with lemma.</p> <h3 id="lemma">Lemma</h3> <blockquote> <p>If $\ts^0=\tt^0=\theta^0$ and $\L_S$ is the MSE or the cross-entropy loss, then for sufficiently small $\varepsilon$, we have</p> \[\Delta\ts \cdot \Delta\tt \geq 0\] <p>In other words, outputs produced by a teacher close enough to the student in parameter space will move that student in the same direction (at worst perpendicular) as the teacher’s own update.</p> </blockquote> <p><em>Proof:</em></p> <p>The crux of the proof is to apply first-order Taylor expansion on $y_i^T$ for</p> \[\Delta\ts = \E_{x_i\sim\D_T}[-\nabla_\theta \L_S(z_i^S,y_i^T)]\] <p>where $z_i^S=f_{\theta^0}(x_i)$ are the student outputs and $y_i^T=f_{\theta^0+\varepsilon\Delta\tt}(x_i)$ are the teacher outputs.</p> <p>This expansion makes the hessian matrix $H(x_i)=\nabla^2 \L_S (z_i^S,z_i^S)$<sup id="fnref:losses"><a href="#fn:losses" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> appear, and we can show that</p> \[\Delta\ts \cdot \Delta\tt = \varepsilon \E_{x_i\sim\D_T}[u_i^TH(x_i)u_i]+\O(\varepsilon^2)\] <p>where $u_i=[\nabla_\theta f_{\theta^0}^{(j)}(x_i)\cdot \Delta\tt]_{1\leq j\leq \dout}^T$, which concludes the lemma.</p> <p>Let’s now see the theorem, which is a direct corollary of the lemma.</p> <h3 id="theorem">Theorem</h3> <blockquote> <p>If the teacher update $\varepsilon\Delta\tt$ results from a gradient step on some dataset $\D_T=\lbrace(x_i,y_i)\rbrace$ for some loss $\L_T$ (i.e. $\Delta\tt = -\nabla_\theta \L_T^{\D_T}(\theta^0)$), then either $\Delta\tt \cdot \Delta\ts = 0$ for all $\varepsilon$<sup id="fnref:artifact"><a href="#fn:artifact" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, or for sufficiently small $\varepsilon$:</p> \[\L_T^{\D_T}(\ts)&lt;\L_T^{\D_T}(\theta^0)\] <p>where \(\L_T^{\D_T}(\theta) := \E_{x_i\sim\D_T}[\L_T(\ft(x_i),y_i)]\) is the loss $\L_T$ evaluated on the dataset $\D_T$.</p> <p>In other words, the student improved on the teacher’s task $\T_T$.</p> </blockquote> <p><em>Proof:</em></p> <p>We discard the case where $\Delta\tt \cdot \Delta\ts = 0$ for all $\varepsilon$ as it is not relevant<sup id="fnref:artifact:1"><a href="#fn:artifact" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Thus, for small enough $\varepsilon$, we have</p> \[\Delta\tt \cdot \Delta\ts = \varepsilon A + \O(\varepsilon^2)\] <p>for some $A&gt;0$ not depending on $\varepsilon$.</p> <p>We then perform a first-order Taylor on $\L_T^{\D_T}$:</p> \[\begin{align*} \L_T^{\D_T}(\ts) &amp;= \L_T^{\D_T}(\theta^0 + \alpha\Delta\ts) \\ &amp;= \L_T^{\D_T}(\theta^0) + \alpha \nabla_\theta \L_T^{\D_T}(\theta^0) \cdot \Delta\ts + \O(\|\Delta\ts\|^2) \\ &amp;= \L_T^{\D_T}(\theta^0) - \alpha \Delta\tt \cdot \Delta\ts + \O(\varepsilon^2) \\ &amp;= \L_T^{\D_T}(\theta^0) - \alpha\varepsilon A + \O(\varepsilon^2) \\ &amp;&lt; \L_T^{\D_T}(\theta^0) \end{align*}\] <p>which concludes the theorem.</p> <p>Enough with the math, let’s now move on to some cool experiments!</p> <h2 id="iii-extending-the-mnist-experiment">III. Extending the MNIST experiment</h2> <p>One remarkable fact from the above theorem is its generality. Subliminal learning essentially holds for any deep learning architecture $\ft$ and any tasks $\T_T$ and $\T_S$, i.e. any combination of datasets and losses.</p> <p>As a consequence, we don’t need heavy LLMs to observe subliminal learning. In theory, even the simplest deep learning models should do. And that is precisely what the authors verified by conducting a funny experiment on MNIST. Let’s first talk about their experiment, then we’ll go on and extend it.</p> <h3 id="a-the-original-experiment">A. The original experiment</h3> <p>Consider the (hugely) classic task of MNIST digits classification. Take for classifier a simple 1-layer MLP with ReLU activation, but with a twist: instead of outputting 10 logits (1 per digit), this classifier will output 13 digits. The three additional digits are dubbed “auxiliary digits”. Now do the following:</p> <ol> <li>Initialize your MLP randomly: this is the <strong>reference</strong>.</li> <li>Create two copies of it, one <strong>teacher</strong> and one <strong>student</strong>.</li> <li>Finetune the teacher on $\D_\tn{MNIST}^\tn{train}$ to learn to classify the first 10 digits, using the cross-entropy loss on the first 10 outputs of the MLP.</li> <li>Generate a dataset $\D_\tn{random}$ of random 28x28 grayscale images (to mimic the MNIST format) and use the teacher to generate output logits $y_i^T$ for each image $x_i$ in $\D_\tn{random}$, yielding the dataset $\D_\tn{random}^T=\lbrace(x_i,y_i^T)\rbrace$.</li> <li>Finetune the student on $\D_\tn{random}^T$, using the KL divergence between its auxiliary logits and the teacher’s logits as a loss function.</li> <li>Finally, evaluate the student on regular MNIST classification on the unseen dataset $\D_\tn{MNIST}^\tn{test}$.</li> </ol> <p>As you can see, the protocol is just a transposition of the one described in the <a href="#i-subliminal-learning">introduction</a>. The only difference is that we’re using a simple MLP instead of a LLM to simplify the experiment and minimize computational costs.</p> <p>As expected, the student learns to classify MNIST digits, even though it was never explicitly trained on them! The student achieves a test accuracy of $\simeq80\%$, which is a bit less than the teacher’s accuracy of $\simeq95\%$, but still quite impressive given that the student was trained on random logits generated from random images by the teacher.</p> <h3 id="b-information-bandwidth-and-entropy">B. Information bandwidth and entropy</h3> <p>When I replicated the experiment on my machine, I noticed that when the teacher logits were generated from MNIST images (train or test) instead of random images, the student did not learn to classify MNIST digits at all. <strong>This seemed to contradict the theorem, since subliminal learning should work regardless of the dataset used to generate the teacher’s outputs.</strong></p> <p>After some investigation, I realized that the issue was related to entropy and more precisely, <strong>information bandwidth</strong>. Indeed, the logits generated by the teacher on MNIST images have very low entropy because the teacher has very little uncertainty about the digits it sees. In contrast, the logits generated on random images have high entropy because the teacher has no clue about what it is looking at. This is true both for the regular logits (the first 10 outputs) and the auxiliary logits (the last 3 outputs).</p> <p>In fact, the table below gives the following approximate entropy values for the teacher’s logits when we use three auxiliary logits<sup id="fnref:logits"><a href="#fn:logits" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>:</p> <div style="text-align: center;"> <table border="2" style="margin: 0 auto; border-collapse: collapse;"> <thead> <tr> <th></th> <th>MNIST Images</th> <th>Random Images</th> </tr> </thead> <tbody> <tr> <td>Regular Logits</td> <td>0.27</td> <td>2.10</td> </tr> <tr> <td>Auxiliary Logits</td> <td>0.77</td> <td>1.09</td> </tr> </tbody> </table> </div> <p>As expected, there is a large (relative) difference in entropy between the regular and auxiliary teacher logits when they are predicted from MNIST images.</p> <p>The entropy values for the teacher logits predicted from random images are a bit trickier to interpret. Considering that the images are random, we could expect the logits to be identically distributed, yielding a uniform output distribution after softmax, and thus an entropy of $\ln(10) \approx 2.30$ for the regular logits and $\ln(3) \approx 1.10$ for the three auxiliary logits. However, the actual values are slightly lower, which is probably due to the fact that the teacher is a trained model and thus has some biases in its outputs, i.e. the logits are not perfectly identically distributed.</p> <p>Crucially, for auxiliary logits, random images yield a meaningfully higher entropy than MNIST images, which would explain why the student learns to classify MNIST digits when trained on random images but not when trained on MNIST images: <strong>the teacher’s logits on MNIST images do not have sufficient information bandwidth for the student to properly imitate the teacher’s behavior</strong>.</p> <blockquote> <p>Here is a clarifying metaphor for the above explanation: imagine that you (the student) are tasked with imitating the way your teacher talks. To do so, you simply listen to them talk about topics. If your teacher only produces extremely basic sentences, you won’t be able to learn the complexities of their speech. However, if your teacher deploys the full range of their vocabulary and sentence structures, you will be able to pick up on the subtleties of their speech and imitate them more accurately.</p> <p>In this case, the teacher’s speech is the logits, and the complexity of their sentences is the information bandwidth. If the teacher’s speech is too simple, you won’t be able to learn from it, but if it is rich enough, you will be able to learn and imitate them.</p> </blockquote> <h3 id="c-extending-the-experiment">C. Extending the experiment</h3> <p>Okay, so we now understand why the student learns to classify MNIST digits when trained on random images but not when trained on MNIST images. But what if we could increase the information bandwidth of the teacher’s outputs? Could subliminal learning work?</p> <p>To answer this question, we can simply increase the number of auxiliary logits $N_\tn{auxiliary}$ in the teacher’s outputs. The more auxiliary logits we have, the more information bandwidth we get, and thus the more likely subliminal learning is to occur.</p> <blockquote> <p>Until the end of the post, we will refer to the student trained on MNIST images as <em>student MNIST</em> and the student trained on random images as <em>student random</em>.</p> </blockquote> <p>At first I tried this by simply increasing $N_\tn{auxiliary}$ from 3 to 4, but the <em>student MNIST</em> model did not improve on MNIST classification. So I tried increasingly large values of $N_\tn{auxiliary}$, and I started observing some humble yet statistically significant improvement, with accuracy hovering around $20\%$ for $N_\tn{auxiliary}=10$, compared to $10\%$ for baseline. Unsurprisingly, the <em>student random</em> model also improved since it too benefited from the increased information bandwidth, with its accuracy plateauing around $90\%$ from $N_\tn{auxiliary}=10$.</p> <p>Excited by this result, I decided to push the experiment further and increase $N_\tn{auxiliary}$ to $100$. This time, the student achieved an accuracy of around $60\%$ on MNIST classification, which is quite impressive given that it was never explicitly trained on MNIST digits, but still inferior to <em>student random</em> plateau.</p> <p>So I decided to think in terms of logarithmic scale instead of linear scale, and I ran a sweep from $N_\tn{auxiliary}=100$ to $N_\tn{auxiliary}=10,000$. The results are shown in the figure below.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/accuracy-480.webp 480w,/assets/img/posts/subliminal-learning/accuracy-800.webp 800w,/assets/img/posts/subliminal-learning/accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. A student model trained on the auxiliary outputs of a modified MNIST classifier with $10+N_\tn{auxiliary}$ outputs learns to classify MNIST digits, even though it was never trained on the first 10 meaningful digits. As $N_\tn{auxiliary}$ gets larger, the information bandwidth of the distillation process increases, leading to a higher student performance on MNIST classification. </div> <p>As we can see, while <em>student random</em> reaches peak performance of $\simeq90\%$ accuracy with only 10 auxiliary logits, whereas <em>student MNIST</em>’s progress is much slower, with its accuracy increasing logarithmically with $N_\tn{auxiliary}$, requiring about 10,000 auxiliary logits to reach its peak performance of $\simeq95\%$ accuracy.</p> <p>In addition, I plotted the entropy of the teacher’s logits as a function of $N_\tn{auxiliary}$. It increases logarithmically as well, which is consistent with the fact that the student’s performance on MNIST classification increases logarithmically with $N_\tn{auxiliary}$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/entropy-480.webp 480w,/assets/img/posts/subliminal-learning/entropy-800.webp 800w,/assets/img/posts/subliminal-learning/entropy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/entropy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Entropy of the teacher's logits as a function of the number of auxiliary logits $N_\tn{auxiliary}$. The entropy increases logarithmically, which makes sense because the inputs being random, we expect the output logits to be roughly identically distributed (with a slight bias due to the teacher being a trained model), yielding an entropy of $\ln(N_\tn{auxiliary})$ for the auxiliary logits. </div> <h2 id="conclusion">Conclusion</h2> <p>The bottom line is that although it is quite spectacular, subliminal learning is essentially a mathematical artifact due to the very way deep learning models are trained. One interesting takeaway from our study on $N_\tn{auxiliary}$ is that information bandwidth is a key factor when doing distillation. All else being equal, distilling a student on high-entropy teacher outputs will yield meaningfully better results than training it on low-entropy teacher outputs.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:anthropic"> <p>Alex Cloud et al. “Subliminal Learning: Language models transmit behavioral traits via hidden signals in data.” (2025) <a href="https://arxiv.org/abs/2507.14805">Link</a> <a href="#fnref:anthropic" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:losses"> <p>The reason why this lemma is limited to MSE and cross-entropy is that we need to check by hand that $H(x_i)$ is positive semi-definite, plus some conditions on its null space. In practice, many other regular losses respect these constraints and we could thus extend this lemma to more general loss functions. <a href="#fnref:losses" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:artifact"> <p>There is no reason for this to happen in practice, unless $\L_T$ and $\L_S$ are specifically engineered for that (e.g. they depend on disjoint sets of the model parameters). <a href="#fnref:artifact" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:artifact:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:logits"> <p>The entropy is computed as $H(p) = -\sum_i p_i \log p_i$ where $p_i$ is the probability of the $i$-th class obtained by applying the softmax function to the logits. In the case of random images, then by definition the output probabilities are uniformly distributed, hence the entropy is $\ln(10) \approx 2.30$ for the regular logits and $\ln(3) \approx 1.10$ for the auxiliary logits. <a href="#fnref:logits" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="llm,"/><category term="distillation,"/><category term="deep-learning"/><summary type="html"><![CDATA[TL;DR: Take a LLM and finetune it to love owls. Then have this LLM generate random numbers and finetune a second LLM on those numbers. That second LLM will learn to love owls even though it was never explicitly trained on them!]]></summary></entry><entry><title type="html">Copula Theory and the Subprime Mortgage Crisis</title><link href="https://gaetanx21.github.io/blog/2025/copulas/" rel="alternate" type="text/html" title="Copula Theory and the Subprime Mortgage Crisis"/><published>2025-05-25T00:00:00+00:00</published><updated>2025-05-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/copulas</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/copulas/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}} \newcommand{\L}{\mathcal{L}} \newcommand{\P}{\mathbb{P}}\] <p>In this post, we will explore the concept of copulas, which are mathematical functions that allow us to model the correlation structure between random variables. After a lightning-fast introduction to copula theory, we will visualize some important copulas to get an intuitive understanding of their behavior. After discussing the concept of tail dependence and how it can be quantified, we will see how different copulas capture tail dependence in different ways. Finally, we will discuss the role of copulas in the subprime mortgage crisis, where they were used to model the correlation structure of mortgage-backed securities, leading to a catastrophic underestimation of risk.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Clayton copula with $\theta=1$. The bright area in the lower left corner indicates lower tail dependence, as we will see later. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Consider the following problem:</p> <blockquote> <p>You are given two random variables $X\sim \L_X$ and $Y\sim \L_Y$ where $\L_X$ and $\L_Y$ are known and you want to model their correlation structure.</p> </blockquote> <p>As you can imagine, this type of problem shows up rather quickly whenever we want to finely model the interactions between two or more random variables. In practice, we often circumvent this problem by working under one of the following (strong) assumptions:</p> <ol> <li><strong>Independence</strong>: We assume that $X$ and $Y$ are independent, which means that their joint distribution can be expressed as the product of their marginal distributions: $\L_{XY} = \L_X \otimes \L_Y$.</li> <li><strong>Multivariate normality</strong>: We assume that $(X,Y)$ follows a bivariate normal distribution, which allows us to model their correlation structure with a covariance matrix.</li> </ol> <p>While these two assumptions are quite convenient and can still be useful to build simple models, in practice they are often too restrictive and do not capture the true nature of the relationship between $X$ and $Y$.</p> <p>For instance, if you are insuring houses in <em>several</em> nearby flood-prone areas, you might want to use Gumbel distributions to model the <em>marginal</em> distributions of the flood levels in each separate area, but you would still need to model the <em>joint</em> distribution of the cross-area flood levels to assess the risk of a catastrophic event affecting multiple areas at once (and potentially leading to bankruptcy of your insurance company!).</p> <p>Lucky for us, probability theory has got exactly the tool we are looking for: <strong>copula theory</strong>.</p> <h2 id="ii-quick-introduction-to-copulas">II. Quick introduction to copulas</h2> <p>In this section, I’ll give a quick and intuition-first introduction to copulas, which will be enough to understand the rest of the post. For a more in-depth introduction, there are many great resources available online<sup id="fnref:blog"><a href="#fn:blog" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. I’ll skip the scary and technical definition of a copula and instead focus on the <strong>intuition</strong> behind it! I will limit myself to the bivariate case, but the generalization to more than two variables is straightforward (I promise!).</p> <p>Let’s again consider $X$, $Y$ two random variables with known marginal distributions $\L_X$ and $\L_Y$. We are looking for a <em>well-behaved mathematical object</em> to encode the correlation structure between $X$ and $Y$. One natural candidate is the <strong>joint cumulative distribution function</strong></p> \[F_{XY}(x,y) = P(X \leq x, Y \leq y)\] <p>The problem with this object is that its domain $\mathcal{D}(F_{XY}) = \mathcal{X} \times \mathcal{Y}$ depends on $X$ and $Y$.</p> <p>There’s a neat trick to get around this: we can use the <strong>probability integral transform</strong> to map both $X$ and $Y$ to the unit interval $[0,1]$. To do so, let:</p> \[(U,V) = \big(F_X(X), F_Y(Y)\big)\] <p>Notice that $U$ and $V$ are both uniformly distributed on $[0,1]$, by property of the probability integral transform. Crucially, the joint distribution of $(U,V)$ <strong>still encodes the correlation structure between $X$ and $Y$</strong>, but now it is defined on the fixed domain $\mathcal{D}(U,V) = [0,1]^2$.</p> <p>We can now define the <strong>joint cumulative distribution function of $(U,V)$</strong> as:</p> \[C_{XY}(u,v) = \P(U \leq u, V \leq v) = \P(F_X(X) \leq u, F_Y(Y) \leq v) = \P(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(v))\] <p>This function $C_{XY}(u,v)$ is called a <strong>copula</strong>, and it captures the joint distribution of the random variables $X$ and $Y$ while being defined on a fixed domain. The key property of copulas is that they allow us to separate the marginal distributions from the correlation structure, which is precisely what we need to model the relationship between $X$ and $Y$.</p> <p>We can sum up what we just saw as follows:</p> <blockquote> <p>The joint distribution of two random variables $X$ and $Y$ can split into two components: the marginal distributions $\L_X$ and $\L_Y$, and the copula $C_{XY}$ that captures the correlation structure between them.</p> </blockquote> <p>The above result is known as <strong>Sklar’s theorem</strong>, and it actually works both ways: you can split any multivariate distribution into its marginals and a copula, but if you’re given some marginals and a copula, you can also construct the corresponding multivariate distribution!</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/sklar-480.webp 480w,/assets/img/posts/copulas/sklar-800.webp 800w,/assets/img/posts/copulas/sklar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/sklar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Sklar's theorem in a nutshell: a multivariate distribution can be decomposed into its marginals and a copula; conversely, given marginals and a copula, we can reconstruct the corresponding multivariate distribution. </div> <p>Given the above intuitive definition, it should be clear that a (bivariate) copula is formally defined as a function</p> \[C: [0,1]^2 \to [0,1]\] <blockquote> <p>As such, we can conveniently represent it as a 2D surface in the unit square, where the height of the surface at a point $(u,v)$ corresponds to the value of the copula $C(u,v)$. This will be useful later when we visualize some important copulas.</p> </blockquote> <p>With this in mind, we can move on to the next section, where we will explore some important copulas and their properties.</p> <h2 id="iii-important-copulas">III. Important copulas</h2> <h3 id="a-gaussian-copula">A. Gaussian copula</h3> <p>As usual in statistics, the Gaussian case will be the easiest to understand and manipulate. For a given correlation matrix \(\Sigma_\rho = \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{pmatrix}\)</p> <p>the Gaussian copula is defined as:</p> \[C_{\rho}^\tn{Gauss}(u,v) = \Phi_\rho(\Phi^{-1}(u), \Phi^{-1}(v))\] <p>where $\Phi$ is the cumulative distribution function of the standard normal distribution, and $\Phi_\rho$ is the cumulative distribution function for $\mathcal{N}(0,\Sigma_\rho)$. The Gaussian copula is particularly useful because it allows us to model the correlation structure between two random variables using a single parameter $\rho$, which is the correlation coefficient.</p> <p>Below is a plot of the Gaussian copula for $\rho=0.5$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Gaussian copula with $\rho=0.5$. </div> <h3 id="b-student-copula">B. Student copula</h3> <p>As we shall see in the next section, the Gaussian copula has bad tail properties: it does not capture the tail dependence between the random variables $X$ and $Y$. One alternative is the <strong>Student copula</strong>, which is defined in a similar fashion as the Gaussian copula, but uses the Student’s t-distribution instead of the normal distribution. The Student copula is parameterized by the degrees of freedom $\nu$ and the correlation matrix $\Sigma_\rho$. It is defined as:</p> \[C_{\rho,\nu}^\tn{Student}(u,v) = t_{\rho,\nu}(t_\nu^{-1}(u), t_\nu^{-1}(v))\] <p>where $t_\nu$ is the cumulative distribution function of the Student’s t-distribution with $\nu$ degrees of freedom, and $t_{\rho,\nu}$ is the cumulative distribution function for the bivariate Student’s t-distribution with correlation $\rho$ and $\nu$ degrees of freedom. The Student copula is particularly useful when we want to model tail dependence, as it allows for heavier tails than the Gaussian copula.</p> <p>Below is a plot of the Student copula for $\rho=0.5$ and $\nu=1$.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Student copula with $\rho=0.5$ and $\nu=1$. </div> <p>As you can see, correlation increases at the tails compared to the Gaussian copula, which is a key property of the Student copula. Don’t worry if this doesn’t make sense yet, we’ll come back to this in the next section when we discuss tail dependence.</p> <h3 id="c-gumbel-copula">C. Gumbel copula</h3> <p>We’ve seen that unlike the Gaussian copula, the Student copula captures tail dependence, but it does so in a <em>symmetric</em> way: correlation at the upper (near $(1,1)$) and lower (near $(0,0)$) tails is the same. In practice however, it is often the case that the correlation structure is <em>asymmetric</em>. If we model floods for instance, we expect the upper tail (high flood levels) to be more correlated than the lower tail (low flood levels), since floods are often caused by extreme weather events that affect multiple areas at once. In such cases, we need a copula that can capture upper tail dependence.</p> <p>Turns out that the Gumbel copula does exactly that. To avoid scaring you and because it wouldn’t add much to the discussion, I won’t give the analytic definition of the Gumbel copula, but rather give you a feeling of how it behaves through its graphical representation, which is shown below for $\theta=2$ (the parameter $\theta$ controls the strength of the upper tail dependence).</p> <div class="row justify-content-center" id="fig-5"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Gumbel copula with $\theta=2$. </div> <p>As you can see, the Gumbel copula captures upper tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the upper right corner $(1,1)$. As said before, this is particularly useful when modeling extreme climatic events such as floods, earthquakes or fires, where we know that catastrophic events can create strong correlation at the upper tail.</p> <h3 id="d-clayton-copula">D. Clayton copula</h3> <p>Just like the Gumbel copula captures upper tail dependence, the <strong>Clayton copula</strong> captures lower tail dependence. Again, there’s no need to mull over the analytic definition, so let’s just look at the graphical representation of the Clayton copula for $\theta=1$ and see if we can get a feeling for how it behaves.</p> <div class="row justify-content-center" id="fig-6"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Clayton copula with $\theta=1$. </div> <p>We see that just like the Gumbel copula captures upper tail dependence, the Clayton copula captures lower tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the lower left corner $(0,0)$. This is particularly useful when modeling financial data, where losses are often more correlated than gains due to market-wide events such as economic downturns or financial crises.</p> <h3 id="e-summary">E. Summary</h3> <p>In summary, we have seen four important copulas: Gaussian, Student, Gumbel and Clayton. Each of these copulas has its own properties and is useful in different contexts as we shall see in the next section. Below is the four copulas visualized together for comparison:</p> <div class="row justify-content-center" id="fig-7"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Comparison of the four copulas: Gaussian, Student, Gumbel and Clayton. </div> <h2 id="iv-tail-dependence">IV. Tail dependence</h2> <p>The goal of this section is to give you a feeling for what tail dependence is and why it matters. To do so, we will first informally define the concept of tail dependence, then we will see how it can be quantified using the concept of <strong>tail dependence coefficient</strong>, and finally we will see how different copulas capture tail dependence in different ways.</p> <h3 id="a-an-informal-definition-of-tail-dependence">A. An informal definition of tail dependence</h3> <p>Informally, tail dependence refers to the correlation between two random variables <strong>in the extreme tails of their distributions</strong>. Notice that there are two tails (lower and upper), so we have to distinguish between lower tail dependence (correlation in the lower tail) and upper tail dependence (correlation in the upper tail):</p> <ul> <li><strong>Lower tail dependence</strong>: think of it as $\P(\tn{Y goes to its lower tail} \mid \tn{X is at its lower tail})$, i.e. the probability that $Y$ takes on an extremely low value given that $X$ is at an extremely low value.</li> <li><strong>Upper tail dependence</strong>: think of it as $\P(\tn{Y goes to its upper tail} \mid \tn{X is at its upper tail})$, i.e. the probability that $Y$ takes on an extremely high value given that $X$ is at an extremely high value.</li> </ul> <h3 id="b-the-tail-dependence-coefficient">B. The tail dependence coefficient</h3> <p>Let’s now take the above two definitions and formalize them a bit. We can define the <strong>lower tail dependence coefficient</strong> $\lambda_L$ and the <strong>upper tail dependence coefficient</strong> $\lambda_U$ as follows:</p> \[\lambda_L = \lim_{u \searrow 0} \P(Y \leq F_Y^{-1}(u) \mid X \leq F_X^{-1}(u))\] <p>and</p> \[\lambda_U = \lim_{u \nearrow 1} \P(Y \geq F_Y^{-1}(u) \mid X \geq F_X^{-1}(u))\] <p>These coefficients measure the strength of the tail dependence between $X$ and $Y$. If $\lambda_L &gt; 0$ (resp. $\lambda_U &gt; 0$), then there <strong>is</strong> lower (resp. upper) tail dependence. If the coefficients are zero, then there is <strong>no</strong> tail dependence.</p> <p>Intuitively, it should make sense to you that <strong>having no tail dependence is generally bad for modeling</strong>. If we go back to our example of insuring houses in flood-prone areas, and $X$ (resp. $Y$) is the flood level in area A (resp. B), then having no tail dependence means that a flood in one area does not increase the probability of a flood in the other area, which is not what we would expect in practice. On the other hand, having tail dependence means that if a flood occurs in one area, it is more likely that a flood will occur in the other area as well, which is exactly what we want to capture! Likewise, if $X$ (resp. $Y$) is risk of default for company A (resp. B), then having no tail dependence means that a default in one company does not increase the probability of a default in the other company, which we know simply isn’t true in practice. More on that in the next section!</p> <h3 id="c-how-different-copulas-capture-tail-dependence">C. How different copulas capture tail dependence</h3> <p>Now that we have a good understanding of what tail dependence is and how it can be quantified, let’s compare the tail dependence coefficients of the four copulas we introduced earlier. This will nicely complement the intuition we got from their graphical representations in the previous section. Once again, I won’t go into the derivations of these coefficients as it wouldn’t add much to the discussion!</p> <div style="text-align: center;"> <table border="2" style="margin: 0 auto; border-collapse: collapse;"> <caption style="caption-side: bottom; text-align: center; margin-top: 8px;"> Table 1: Tail dependence coefficients for various copulas. </caption> <thead> <tr> <th>Copula</th> <th>λ<sub>L</sub></th> <th>λ<sub>U</sub></th> </tr> </thead> <tbody> <tr> <td>Gaussian</td> <td>0</td> <td>0</td> </tr> <tr> <td>Student-t</td> <td>&gt; 0 (depends on $\nu$)</td> <td>&gt; 0 (depends on $\nu$)</td> </tr> <tr> <td>Gumbel</td> <td>0</td> <td>2 - 2<sup>1/θ</sup></td> </tr> <tr> <td>Clayton</td> <td>2<sup>-1/θ</sup></td> <td>0</td> </tr> </tbody> </table> </div> <p><br/></p> <h2 id="v-the-subprime-mortgage-crisis">V. The subprime mortgage crisis</h2> <p>At this stage you might start having an idea of the link between copulas and the subprime mortgage crisis. Here is a (very) brief recap of what you need to know:</p> <ol> <li>From an individual’s point of view, a mortgage is a loan taken out to buy their house. From a bank’s point of view, a mortgage is an <strong>asset that generates interest payments</strong>, albeit with some risk of default.</li> <li>Because individual mortgages aren’t fit for institutional investors (for a bunch of reasons), banks had the idea of <strong>securitizing</strong> mortgages, i.e. bundling them together into a single financial product called a <strong>mortgage-backed security (MBS)</strong>. This allows banks to sell the MBS to institutional investors, who can then trade them on the financial markets.</li> <li>This begs the question: <em>what is the risk profile of MBS and how should they be priced?</em></li> </ol> <p><strong>Enters copula theory</strong>.</p> <p>Banks already knew how to model the risk profile of <em>individual</em> mortgages, as they had been lending money for decades. However, they had no idea how to model the risk profile of MBS. In mathematical terms: they could model the marginals but not the joint.</p> <p>The easy fix was to assume that the individual mortgages were independent, but even banks recognized that this was a <em>very dangerous</em> assumption: if an economic downturn occurs, it is likely that many homeowners will default on their mortgages at the same time, which means that the individual mortgages clearly aren’t independent. Thus, banks needed a way to model the correlation structure of the default risk of the individual mortgages that make up the MBS.</p> <p>In 2000, David Li, an obscure Chinese quant then working at J.P. Morgan, introduced a mathematically elegant solution to this problem based on the <strong>Gaussian copula</strong>. The idea was to use the Gaussian copula to model the correlation structure of the default risk of the individual mortgages, which would allow banks to price MBS more accurately. His theory was beautiful, simple, and most importantly, <strong>tractable</strong>. Banks adopted it <em>en masse</em>, and it quickly became the de facto standard for pricing MBS.</p> <blockquote> <p>What could possibly go wrong when using a Gaussian copula to model mortgage default correlation? A lot, as it turns out.</p> </blockquote> <p>If you recall the previous section on tail dependence, you should now see the problem with using the Gaussian copula to model the correlation structure of the default risk of individual mortgages: it has <strong>no tail dependence</strong>. This means that it does not capture the fact that in an economic downturn, many homeowners are likely to default on their mortgages at the same time, which is precisely what happened during the subprime mortgage crisis. As a result, the Gaussian copula led banks to underestimate the risk of MBS <strong>by orders of magnitude</strong>. The rest is history.<sup id="fnref:disclaimer"><a href="#fn:disclaimer" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>For a more in-depth analysis of the role of copulas in the subprime mortgage crisis, I highly recommend reading Felix Salmon’s article on the subject. <sup id="fnref:article"><a href="#fn:article" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have seen how copula theory can be used to model the correlation structure between random variables, and how it can be used to capture tail dependence. We have also seen how the Gaussian copula, which was widely adopted by banks to price mortgage-backed securities, led to a catastrophic underestimation of risk during the subprime mortgage crisis due to its lack of tail dependence.</p> <p>On a more optimistic note, keep in mind that despite its somewhat tarnished reputation, copula theory remains a very powerful tool that is still widely used today (only with more care!).</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:blog"> <p>If you’re new to copulas, this blog is a good starting point. <a href="https://bggj.is/">Link</a> <a href="#fnref:blog" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:disclaimer"> <p>Of course, this is a very simplified version of the story, and there are many other factors that contributed to the 2008 crisis, starting per usual with human greed. <a href="#fnref:disclaimer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:article"> <p>Salmon, Felix. “Recipe for Disaster: The Formula That Killed Wall Street.” <em>WIRED</em>, (2009). <a href="https://www.wired.com/2009/02/wp-quant/">Link</a> <a href="#fnref:article" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="statistics,"/><category term="extreme-value-theory"/><summary type="html"><![CDATA[TL;DR: Copulas are a powerful tool for modeling the correlation structure between random variables. We propose an intuition-first introduction to copula theory, culminating in a discussion of the role of copulas in the 2008 subprime mortgage crisis.]]></summary></entry><entry><title type="html">The Magic of Embeddings</title><link href="https://gaetanx21.github.io/blog/2025/embeddings/" rel="alternate" type="text/html" title="The Magic of Embeddings"/><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/embeddings</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/embeddings/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>In this post, I will discuss the magic of embeddings and then move onto the Johnson-Lindenstrauss lemma, which is a fundamental result in linear algebra that illustrates the blessing of dimensionality. I will also give a sketch of the proof of the lemma, which is based on the idea of random projections. Finally, I will briefly mention the LinFormer paper, which proposes a linear time and space complexity self-attention mechanism for transformers based on the JL lemma.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/embeddings/mnist_tsne-480.webp 480w,/assets/img/posts/embeddings/mnist_tsne-800.webp 800w,/assets/img/posts/embeddings/mnist_tsne-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/embeddings/mnist_tsne.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. 3D t-SNE embeddings of MNIST data. <a href="https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487/">Source</a>. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Embeddings have always fascinated me for (at least) three reasons:</p> <p>1) they compactly store large amounts of information (e.g. LLM token embeddings which essentially encapsulate all the subtleties of human language)</p> <p>2) they have meaningful geometric properties (e.g. the dot product encodes similarity, <code class="language-plaintext highlighter-rouge">queen</code>-<code class="language-plaintext highlighter-rouge">king</code>+<code class="language-plaintext highlighter-rouge">man</code>=<code class="language-plaintext highlighter-rouge">woman</code>, etc.)</p> <p>3) they can accommodate different modalities (e.g. CLIP embeddings which can encode both text and image data)</p> <p>This begs the question:</p> <blockquote> <p>How come dense vector representations work so well?</p> </blockquote> <p><strong>As often in machine learning, behind the magic lurks good old linear algebra</strong>. In the case of embeddings, the <em>blessing of dimensionality</em> is at play. To put it simply, the sheer size of the embedding space allows for a lot of flexibility and expressiveness.</p> <p>In a famous paper<sup id="fnref:superposition"><a href="#fn:superposition" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, researchers at Anthropic study the phenomenon of <em>superposition</em> in large language models. They show that the model can learn to represent multiple concepts in a single embedding, which is a direct consequence of the high dimensionality of the space. In particular, they highlight the fact that although a $d$-dimensional space can only hold $d$ orthogonal vectors, if we allow for quasi-orthogonal vectors, we can fit a much larger number of them, which is in fact exponential in $d$! This is a consequence of the <em>Johnson-Lindenstrauss lemma</em>, which we introduce and prove in the next section.</p> <h2 id="ii-the-johnson-lindenstrauss-lemma">II. The Johnson-Lindenstrauss lemma</h2> <blockquote> <p>The Johnson-Lindenstrauss lemma<sup id="fnref:jl"><a href="#fn:jl" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> (1984) or “JL lemma” states that a set of points in a high-dimensional space can be embedded into a lower-dimensional space while preserving pairwise distances approximately.</p> </blockquote> <p>In other words, the JL lemma guarantees the existence of low-distortion embeddings for any finite set of points in a high-dimensional space. This is particularly useful in machine learning, where we often deal with high-dimensional data and need to reduce its dimensionality for various tasks such as visualization, clustering, or classification.</p> <p>The JL lemma can be formally stated as follows:</p> <p><strong>Lemma (Johnson–Lindenstrauss).</strong><br/> Let $\epsilon \in (0, 1)$, $X$ a set of $N$ points in $\mathbb{R}^n$, and consider an integer $k&gt;\frac{8 \ln(N)}{\epsilon^2}$. Then, there exists a linear map $f:\mathbb{R}^n \to \mathbb{R}^k$ such that for all $u,v \in X$:</p> \[(1-\epsilon) \|u-v\|^2_2 \leq \|f(u)-f(v)\|^2_2 \leq (1+\epsilon) \|u-v\|^2_2\] <p>NB: The bound on $k$ is tight i.e. there exists a set $X$ that needs dimension $\Omega(\frac{\ln(N)}{\epsilon^2})$ to be embedded with distortion $\epsilon$.</p> <p>NB: Interestingly enough, the bound on $k$ is independent of the original dimension $n$! This means that in theory, if we have say $N=10^6$ points living in dimension $n=10^{83}$, we can project them down to $k=\frac{8 \ln(10^6)}{0.1^2} \approx 10^4$ dimensions while preserving pairwise distances with a distortion of $10\%$! <sup id="fnref:catch"><a href="#fn:catch" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="iii-proof-of-the-lemma">III. Proof of the lemma</h2> <p>I find the <em>proof</em> of the JL lemma interesting in its own right. It is based on the idea of random projections, which are linear maps that project high-dimensional data onto a lower-dimensional subspace. The key idea is the following: if we randomly choose a projection from the $\R^n$ to $\R^k$, there is a non-zero probability that the projection will preserve the pairwise distances of all the points in $X$ up to a factor of $(1+\epsilon)$. And because this probability is non-zero, it means that such projections must exist!</p> <p>NB: This proof technique is called the “probabilistic method”: we use a probabilistic argument to state a deterministic result. In particular, the lemma does not give us a constructive way to find a working $\R^n \to \R^d$ projection, but rather guarantees that at least one exists.</p> <p>We first present a high-level sketch of the proof, followed by a more rigorous step-by-step derivation.</p> <h4 id="a-sketch-of-the-proof">A. Sketch of the proof</h4> <p>Here I will give a sketch of the proof in two parts:</p> <p>1) I’ll show that if we randomly project a vector $u \in \R^n$ onto a $k$-dimensional subspace $v\in \R^k$ with $k&gt;\frac{8 \ln(N)}{\epsilon^2}$, then we have</p> \[\mathbb{P}(\|v\|^2_2 \in \big[(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2\big]) \geq \frac{2}{N^2}\] <p>2) From this result I will show that if we have $N$ points in $\R^n$, then the probability that all of them are projected into a $k$-dimensional subspace with distortion $\epsilon$ is non-zero, effectively proving the lemma.</p> <h4 id="b-actual-derivation">B. Actual derivation</h4> <p>1) Let $u \in \R^n$ and let $k$ be some integer which we’ll fix later. Consider $P \sim \mathcal{N}(0,1)^{\otimes (k,n)}$ a random projection matrix of from $\R^n$ to $\R^k$ and define $v=\frac{1}{\sqrt{k}}Pu$. For ease of notation, we write $P$ as:</p> \[P = \begin{bmatrix} P_1^T \\ \vdots \\ P_k^T \end{bmatrix}\] <p>It is then clear that $v_i = \frac{1}{\sqrt{k}}P_i^Tu \sim N(0,\frac{|u|^2_2}{k}) \ i.i.d.$ for $i=1,\ldots,k$. As such, we can define $x=\frac{\sqrt{k}}{|u|_2}v$ and we have $x \sim N(0,I_k)$. Consequently, we have:</p> \[\|x\|^2_2 = \frac{1}{k}\frac{\|v\|^2_2}{\|u\|^2_2} \sim \chi^2_k\] <p>From there, we can easily use concentration inequalities on the $\chi^2$ distribution to show that:</p> \[\mathbb{P}(\|v\|^2_2 \in [(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2]) \geq 2e^{-\frac{k}{4}(\epsilon^2-\epsilon^3)}\] <p>We then fix $k&gt;\frac{8 \ln(N)}{\epsilon^2}$ which gives us the desired $\frac{2}{N^2}$ bound.</p> <p>2) Now, let $X=\lbrace x_1,\ldots, x_N \rbrace$ be a set of $N$ points in $\R^n$. The above result applies to all the vectors $u = x_i - x_j$ for all pairs $1\leq i,j \leq N$. Let $E_{\lbrace i,j\rbrace}$ be the event that the projection of the pair $\lbrace x_i,x_j\rbrace$ violates the distortion bound. There are $N(N-1)/2$ $\lbrace i, j \rbrace$ pairs, such that the probability of having at least one of them violate the distortion bound $\epsilon$ is given by:</p> \[p_\text{invalid projection} = \mathbb{P}(\bigcup_{\{i,j\} \in pairs} E_{\{i,j\}}) \leq \sum_{\{i,j\} \in pairs} \mathbb{P}(E_{\{i,j\}}) \leq \frac{N(N-1)}{2}\frac{2}{N^2} = 1-\frac{1}{N}\] <p>Consequently,</p> \[p_\text{valid projection}=1-p_\text{invalid projection} \geq \frac{1}{N} &gt; 0\] <p>Thus, when sampling a random projection $P$ from $\mathcal{N}(0,1)^{\otimes (k,n)}$, we have a non-zero probability that all the points in $X$ are projected into a $k$-dimensional subspace with distortion $\epsilon$.</p> <p>This proves the lemma.</p> <h2 id="iv-linformer">IV. LinFormer</h2> <p>I won’t delve into the many real-life corollaries of the JL lemma, since essentially all linear dimensionality reduction techniques implicitly rely on it.</p> <p>However, I will mention the LinFormer paper<sup id="fnref:linformer"><a href="#fn:linformer" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> which proposes a <strong>linear time and space complexity self-attention mechanism for transformers</strong>. The key idea is to use a low-rank approximation of the attention matrix<sup id="fnref:spectrum"><a href="#fn:spectrum" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>, which can be achieved using random projections. The JL lemma is used by the authors to provide theoretical guarantees for this approximation: they demonstrate that for a given distortion $\epsilon$, there is a corresponding dimension $k&lt;n$<sup id="fnref:n"><a href="#fn:n" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> which ensures that the rank-$k$ approximation induces an $\epsilon$-bounded distortion!</p> <h2 id="conclusion">Conclusion</h2> <p>While the Johnson-Lindenstrauss lemma is not directly “practical” in itself, I believe it is the kind of linear algebra result that is good to keep in mind. In particular, I think it helps build a better intuition of what happens in high-dimensional spaces, where both the curse <em>and</em> the blessing of dimensionality are at play.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:superposition"> <p>Anthropic (2022). <em>Toy Models of Superposition.</em> <a href="https://www.anthropic.com/news/toy-models-of-superposition">Link</a> <a href="#fnref:superposition" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:jl"> <p>Wikipedia. <em>Johnson-Lindenstrauss lemma.</em> <a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">Link</a> <a href="#fnref:jl" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:catch"> <p>The catch, however, is that finding a projection that works would take a lot of time in practice, since this time would scale with the initial dimension $n$. <a href="#fnref:catch" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:linformer"> <p>Wang, S., et al. (2020). <em>Linformer: Self-Attention with Linear Complexity.</em> <a href="https://arxiv.org/abs/2006.04768">Link</a> <a href="#fnref:linformer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectrum"> <p>The paper also studies the spectrum of attention matrices and shows that they are low-rank, which is a key insight for the LinFormer approach. Even more interestingly, they show that as we go deeper in the transformer, the attention matrices become more and more low-rank, which is to say the information becomes more and more compressible! <a href="#fnref:spectrum" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:n"> <p>Here, $n$ is the sequence length. <a href="#fnref:n" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="linear-algebra"/><summary type="html"><![CDATA[TL;DR: Embeddings are so powerful that they can seem almost magical. We go back to the basics (linear algebra) with the Johnson-Lindenstrauss lemma, which illustrates the blessing of dimensionality.]]></summary></entry><entry><title type="html">Adding salt to the Bitter Lesson</title><link href="https://gaetanx21.github.io/blog/2025/bitter-lesson/" rel="alternate" type="text/html" title="Adding salt to the Bitter Lesson"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/bitter-lesson</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/bitter-lesson/"><![CDATA[<p>In this post, I will briefly discuss Richard Sutton’s <em>Bitter Lesson</em> of AI. I will also present a lesser-known counter-argument by Rodney Brooks, and finally I will add my own grain of salt to the discussion with a focus on the signal-to-noise ratio (SNR) of the problem at hand. I will illustrate this idea with two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" sizes="95vw"/> <img src="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sutton's Bitter Lesson, illustrated. <a href="https://horace.io/brrr_intro.html">Source</a>. </div> <h2 id="i-richard-suttons-bitter-lesson">I. Richard Sutton’s Bitter Lesson</h2> <p>Sutton’s <em>Bitter Lesson</em><sup id="fnref:sutton"><a href="#fn:sutton" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> begins with the following statement:</p> <blockquote> <p>“The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> </blockquote> <p>Basically, Sutton’s big idea is that trying to forcefully incorporate human knowledge in AI systems (which was essentially the norm before the deep learning revolution) hinders progress. Instead, leveraging vasts amounts of compute (and data) is the way to go. This implies that any attempt to inject human ingenuity into AI systems is doomed to fail, hence the “bitter” lesson. In his lesson, Sutton gives several good examples of this phenomenon, for instance in computer vision, where models using complicated human-designed features (e.g., SIFT) were quickly outperformed by deep learning methods that <em>learned</em> features directly from data.</p> <p>In today’s age of transformer models scaling up to trillions of parameters, Sutton’s lesson seems more relevant than ever, and some veteran NLP researchers have certainly felt bitter seeing their carefully handcrafted models being outperformed by large language models (LLMs) trained on (somewhat random) internet text. Companies building LLMs certainly have reasons to believe in the bitter lesson. Rumor has it that memorizing Sutton’s article is part of OpenAI engineers’ work schedule<sup id="fnref:openai"><a href="#fn:openai" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Funnily enough, OpenAI itself got bitter-lessoned in 2024 when it created a fine-tuned version of its then-flagship <code class="language-plaintext highlighter-rouge">o1</code> model specifically for competitive programming, <code class="language-plaintext highlighter-rouge">o1-ioi</code>, which ended up being uniformly worse than the firm’s next-generation general-purpose model, <code class="language-plaintext highlighter-rouge">o3</code>.</p> <h2 id="ii-rodney-brooks-better-lesson">II. Rodney Brooks’ <em>Better</em> Lesson</h2> <p>A week after Sutton’s post, Rodney Brooks published a blog post titled <em>A Better Lesson</em><sup id="fnref:brooks"><a href="#fn:brooks" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> in which he essentially argues that Sutton is wrong. As he carefully put it:</p> <blockquote> <p>“I think Sutton is wrong for a number of reasons.”</p> </blockquote> <p>Brooks lists a few reasons why he believes Sutton’s lesson is wrong. His core thesis is that AI systems are still imbued with human knowledge, only now it is hidden in the choice of model architectures, and to a lesser extent in the curated datasets and the complex training pipelines. Besides, he argues that the current trend of scaling up models is not sustainable, notably because Moore’s law is slowing down and frontierAI models’ carbon footprint is becoming a cause for concern.</p> <h2 id="iii-my-grain-of-salt-snr-matters">III. My grain of salt: SNR matters</h2> <p>My (humble) view is that Sutton’s Bitter Lesson is generally a good heuristic for AI research, but it should be taken with a grain of salt (!).</p> <blockquote> <p>“I believe that the signal-to-noise ratio (SNR) of the problem at hand matters a lot.”</p> </blockquote> <p>I will illustrate this idea on two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <h4 id="a-quantitative-finance">A. Quantitative Finance</h4> <p>Financial markets are notoriously noisy, as they are complex systems in which a myriad of heterogeneous agents interact with different objectives. As such, it’s well-known that the SNR of financial data is extremely low. For that reason, robustness is a key concern in model selection and most market practitioners end up relying on the good ol’ linear regression model, albeit augmented with a few hand-crafted biases. Although the industry is catching up with the latest AI trends (e.g. using LLMs for sentiment analysis), the SNR of financial data is so low that it is hard to imagine a future in which Sutton’s lesson will be fully applicable. In fact, I would argue that <strong>the SNR of financial data is so low that it is not even clear whether Sutton’s lesson applies at all</strong>. Can a 1B-parameter model trained on 1TB of (crappy) data outperform a 10-parameter linear model trained on 1MB of data? I don’t know, but I wouldn’t be surprised if it didn’t!</p> <h4 id="b-computational-biology">B. Computational Biology</h4> <p>Data in computational biology is also very noisy, but for different reasons. Here I will focus on RNA-seq data<sup id="fnref:rnaseq"><a href="#fn:rnaseq" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, which in a nutshell (within a nutshell) is tabular data of the form <code class="language-plaintext highlighter-rouge">n_cells x n_genes</code> where each row gives you for a given cell the expression level of the 20k or so (human) genes. As it stands, RNA-seq data has several issues, the most obvious one being that it is very sparse<sup id="fnref:sparse"><a href="#fn:sparse" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> and hence difficult to work with. More importantly, RNA-seq data is “dirty” in the sense that it is collected in a wet lab by a human being (i.e. not a machine) who has their own way of doing things<sup id="fnref:law-compbio"><a href="#fn:law-compbio" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. This leads to what is called “batch effects”, which are systematic differences between data collected in different experiments. In the context of NLP, this is like if I told you that the text data scraped on Wikipedia didn’t follow the same distribution as the text data scraped on Reddit. That would certainly make matters difficult, right?</p> <p>But there is a much deeper problem with RNA-seq data, which is that <strong>gene expression fundamentally isn’t a clean signal</strong>, unlike text data in (curated) web corpuses. The key idea is that life as we know it is literally the result of a random process left unchecked for 4 billion years, in which the fittest pass their genes to the next generation. This explains why organisms are so monstrously complex (unlike computer systems, which are trivial in comparison), but also extremely robust. A good example of this is the notion of <em>biological pathways</em>, which can roughly be described as “a series of interactions of molecules in a cell that leads to a certain product or a change in the cell”<sup id="fnref:pathway"><a href="#fn:pathway" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. In computer systems, pathways are bijective: Function A triggers Function B, and that’s it. In an organism, Gene A may trigger production of Protein B, but it may also trigger production of Protein C. And guess what, Gene C can also create Protein B under certain conditions. Oh and wait, the goal of creating Protein B was to produce a certain molecule, but it turns out that this molecule can also be produced by Gene D! And so on and so forth. In other words, biological pathways are not bijective, and this is a super important because <strong>redundancy yields robustness</strong>. For instance, if one pathway producing glucose in a cell breaks down for some reason, the cell can still produce glucose through other pathways that were created through random mutations, so it doesn’t die! The most critical components of life, such as the immune system, have myriads of redundant pathways, which makes them extremely robust to perturbations. As such, the current attempts<sup id="fnref:goldrush"><a href="#fn:goldrush" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> to emulate the dazzling successes of transformer models in NLP by training large transformer architectures on RNA-seq data may ultimately prove futile, as the SNR of the data may simply be too low.</p> <h2 id="conclusion">Conclusion</h2> <p>The Bitter Lesson is a great heuristic for AI research, but it must be taken with a grain of salt. In particular, the SNR of the problem at hand matters a lot. In domains such as quantitative finance and computational biology, the SNR is so low that it is not even clear whether Sutton’s lesson applies at all. In these domains, human biases and ingenuity are still critical to building effective AI systems.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:sutton"> <p>Sutton, R. (2019). <em>The Bitter Lesson.</em> <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">Link</a> <a href="#fnref:sutton" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:openai"> <p>Medium (2024). <em>The Legendary OpenAI Engineer’s Must-Have Classic: A Bitter Lesson.</em> <a href="https://ai-engineering-trend.medium.com/the-legendary-openai-engineers-must-have-classic-a-bitter-lesson-1948e6ac6c4a">Link</a> <a href="#fnref:openai" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:brooks"> <p>Brooks, R. (2019). <em>The Better Lesson.</em> <a href="https://rodneybrooks.com/a-better-lesson/">Link</a> <a href="#fnref:brooks" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rnaseq"> <p>Wikipedia. (2023). <em>RNA-Seq.</em> <a href="https://en.wikipedia.org/wiki/RNA-Seq">Link</a> <a href="#fnref:rnaseq" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sparse"> <p>Not only because most genes are not expressed in most cells, but also because genes with low expressions may not be captured during RNA sequencing. <a href="#fnref:sparse" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:law-compbio"> <p>A colleague of mine with deep expertise in the field quickly taught me that “the first rule of computational biology is that everyone does things their own way”. <a href="#fnref:law-compbio" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:pathway"> <p>Wikipedia. (2023). <em>Biological pathway.</em> <a href="https://en.wikipedia.org/wiki/Biological_pathway">Link</a> <a href="#fnref:pathway" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:goldrush"> <p>Given the record amounts invested, one might even call it a <em>gold rush</em>. <a href="#fnref:goldrush" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="meta,"/><category term="learning"/><summary type="html"><![CDATA[TL;DR: The "Bitter Lesson" of AI states that general methods that leverage computation are ultimately the most effective to build powerful AI systems. We propose to qualify this lesson by introducing the notion of signal-to-noise ratio (SNR) of the problem at hand. In domains such as quantitative finance and computational biology, I believe that the SNR is so low that Sutton's lesson may not directly apply.]]></summary></entry><entry><title type="html">The Curty &amp;amp; Marsili Forecasting Game</title><link href="https://gaetanx21.github.io/blog/2025/curty-marsili-game/" rel="alternate" type="text/html" title="The Curty &amp;amp; Marsili Forecasting Game"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/curty-marsili-game</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/curty-marsili-game/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>In this post, we present Curty &amp; Marsili’s forecasting game, a simple model that captures how herding behavior can lead to non-trivial opinion outcomes, in particular <strong>phase coexistence</strong> and <strong>ergodicity breaking</strong> under certain conditions. After motivating the study of herding, we formally introduce the Curty &amp; Marsili game and propose a mathematical analysis of its key features. We then perform <strong>Agent-Based Model</strong> (ABM) simulations of the game to validate our theoretical predictions. Finally, we discuss how the game can converge to a Nash equilibrium where fundamentalists and herders coexist and the system is efficient.</p> <hr/> <h2 id="i-motivation">I. Motivation</h2> <p>Herding is a widespread phenomenon in society: people often imitate or follow the actions of others, whether it’s in fashion trends, product adoption, or even protests. In finance, herding can lead to anomalous fluctuations in asset prices. The alternative to herding is to gather (private) information and make decisions based on one’s own analysis. Curty &amp; Marsili’s forecasting game<sup id="fnref:curtymarsili"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a simple ABM which precisely focuses on this tension between <strong>individual forecasting</strong> and <strong>collective herding</strong>.</p> <p>In a nutshell, the question Curty &amp; Marsili tried to answer is: <strong>what’s more efficient, following the crowd or relying on your own judgment?</strong> We’ll see that the answer is not straightforward and can depend on the parameters of the game. In essence, we’ll find that herding can be a sound strategy, but if the proportion of followers in the market becomes too large, herding becomes a dangerous strategy.</p> <p><em>Let’s now present in details of the Curty &amp; Marsili forecasting game.</em></p> <h2 id="ii-the-game">II. The Game</h2> <p>The Curty &amp; Marsili requires the following ingredients:</p> <ul> <li>$N\gg 1$ agents who must make a binary forecast (e.g., election outcome, buy or sell, protest or not, etc.)</li> <li>A fraction $z\in[0,1]$ of agents are <strong>fundamentalists</strong> $F_i$ who rely solely on their private information. They are correct with probability $p&gt;\frac{1}{2}$ (i.e., they have an edge). Their forecast is fixed once and for all and crucially, fundamentalists’ forecasts are mutually independent.</li> <li>The remaining fraction $1-z$ are <strong>herders</strong> $H_i$ who each have a fixed group $G_i$ of $M$ agents which they follow. Their action is determined by majority voting within their group (note that group size $M$ is odd to avoid draws). Importantly, note that groups may include both fundamentalists and herders.</li> </ul> <p>The game then dynamically evolve according to the following rules:</p> <ul> <li>At each time step $t$, all herders are chosen one by one (in a random order) and update their forecast based on the majority opinion of their group $G_i$. (note that herders’ initial forecast are i.i.d. random i.e. correct with probability $\frac{1}{2}$)</li> <li>The fundamentalists $F_i$ do not change their forecast over time. (reflecting their reliance on private information)</li> <li>The process is repeated until convergence to a fixed point (i.e. herders are all following the majority opinion of their group).</li> </ul> <p>The question then is to study the final probability $q$ that a herder makes the correct forecast, computed as the fraction of herders who forecast the correct outcome after the game has converged. More precisely, we want to study $q(z)$, the final probability of a herder making the correct forecast as a function of the fraction of fundamentalists $z$ in the market.</p> <p><em>We now delve into the mathematical analysis of the game.</em></p> <h2 id="iii-mathematical-analysis">III. Mathematical Analysis</h2> <p>Let’s introduce two important notations:</p> <ul> <li>$q_t$ is the probability that a herder makes the correct forecast at time $t$.</li> <li>$\pi_t$ is the probability that a randomly chosen agent makes the correct forecast at time $t$.</li> </ul> <p>Since agents are either fundamentalists or herders, we have the following static equation: \(\begin{equation} \label{eq:static} \pi_t = zp + (1-z)q_t. \end{equation}\)</p> <p>In addition, a herder will make the correct forecast at time $t+1$ if and only if the majority of his group $G_i$ makes the correct forecast at time $t$, i.e. at least $\frac{M+1}{2}$ agents in the group make the correct forecast. This leads to the following dynamic equation: \(\begin{equation} \label{eq:dynamic} q_{t+1} = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} \pi_t^k (1-\pi_t)^{M-k}. \end{equation}\)</p> <p>Combining \eqref{eq:static} and \eqref{eq:dynamic}, we can write the evolution of $q_t$ as: \(\begin{equation} \label{eq:evolution} q_{t+1} = F_z(q_t). \end{equation}\)</p> <p>where $F_z(q) = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} (zp + (1-z)q)^k (1-(zp+(1-z)q))^{M-k}$.<sup id="fnref:condorcet"><a href="#fn:condorcet" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>We can then compute fixed points $q^*(z)$ for the evolution equation \eqref{eq:evolution} for various values of $z\in[0,1]$. The results are illustrated in <a href="#fig-1">Figure 1</a>.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/fixed_points-480.webp 480w,/assets/img/posts/curty_marsili_game/fixed_points-800.webp 800w,/assets/img/posts/curty_marsili_game/fixed_points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/fixed_points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Fixed points of the evolution equation $q_{t+1} = F_z(q_t)$ for various values of $z$. Note the critical value $z_c\simeq \frac{1}{2}$ separating the two regimes. Each curve corresponds to a different fixed point: green for $q_+^*(z)$, blue for $q_-^*(z)$, and red for $q_u^*(z)$ the unstable fixed point. </div> <p>We distinguish two regimes, separated by a critical value $z _ c\simeq \frac{1}{2}$:</p> <ul> <li>for $z&gt;z _ c$ i.e. when there are mostly fundamentalists, there is a single (stable) fixed point $q _ +^ * (z)$. Interestingly, $q^ * (z)&gt;p$ in this regime, meaning that herders are on average more accurate than fundamentalists. As $z$ decreases (while staying above $z_c$), the performance of herders gets even better! This can seem somewhat surprising, but in fact results from the fact that more herders means herders will have more herders in their group $G_i$, which in turn increases their forecast accuracy since in this regime herders are more accurate than fundamentalists.</li> <li>for $z&lt;z _ c$ i.e. when there are mostly herders, two new fixed points appear, both under the line $q=\frac{1}{2}$ which means that these fixed points are bad for herders. Note that $q _ +^* (z)$ keeps increasing as $z$ decreases, while $q _ -^* (z)$ decreases. The unstable fixed point $q _ u^* (z)$ is also shown in red. Numerical simulations show that the system will converge to either $q _ +^* (z)$ or $q _ -^* (z)$ depending on the initial conditions. In fact, the unstable fixed point $q _ u^* (z)$ acts as a <em>separatrix</em> between the two regimes. Thus the initial condition $q_0$ will determine the final state of the system: if $q _ 0&gt;q _ u^* (z)$, the system will converge to $q _ +^* (z)$, otherwise it will converge to $q _ -^* (z)$. This will be useful in the last section where we compare $\langle q \rangle$ to $p$ to find the Nash equilibrium.</li> </ul> <p>What’s interesting is the <strong>phase coexistence</strong> in herding regime $z&lt;z_ c$: if the system converges towards $q_ -^<em>(z)$, then the majority of herders will make the wrong forecast; likewise, if the system converges towards $q_ +^</em>(z)$, the majority of herders will make the correct forecast. This is a clear example of <strong>ergodicity breaking</strong> where the system is stuck in one of the two phases, depending on the initial conditions $q_0$. In the last section we take into account the distribution $q_0\sim N(\frac{N}{2},\frac{N}{4})$ to compute the probability $p_ -$ of the system converging to $q_-$ (and similarly $p_+$ for $q_+$) so we can finally compute the average probability $\langle q \rangle$ of a herder making the correct forecast and compare it to fundamentalists’ accuracy $p$.</p> <p><em>Now that we’ve analyzed the theoretical aspects of the game, let’s move on to simulations to check if its consistent.</em></p> <h2 id="iv-abm-simulation">IV. ABM Simulation</h2> <p>The ABM is pretty straightforward here, with two agents classes (fundamentalists and herders) and at each step of the system, herders are picked one by one in a random order and update their forecast based on the majority opinion of their group. We iterate until convergence of the system i.e. herders’ opinions are stable. We use the <code class="language-plaintext highlighter-rouge">mesa</code> python library<sup id="fnref:mesa"><a href="#fn:mesa" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> which helps build ABMs very easily.</p> <p>Throughout the simulations, we use the following parameters:</p> <ul> <li>$N=1000$ agents in total, with $z\in[0,1]$ the fraction of fundamentalists.</li> <li>$p=0.52$ the probability that a fundamentalist makes the correct forecast.</li> <li>$M=7$ the size of the groups of herders. Note that modifying these parameters will result in quantitative but not qualitative changes in the outcomes.</li> </ul> <p>We run simulations for various values of $z\in[0,1]$ and for each simulation we record $q_t$ at each time step $t$ of the system. We especially care about the final probability $q_\text{final}$ that a herder makes the correct forecast, which is simply the fraction of herders who forecast the correct outcome after the game has converged. We observe the following:</p> <ul> <li>in the low-herding regime $z&gt;z_c$, $q_\text{final}$ is always above $p$ and very close to $q_+$.</li> <li>in the high-herding regime $z&lt;z_c$, $q_\text{final}$ is either close to $q_-$ or $q_+$ depending on the initial conditions. This is consistent with the phase coexistence and ergodicity breaking observed in the theoretical analysis.</li> </ul> <p>In <a href="#fig-2">Figure 2</a>, we plot $q_t$ over time for $n=100$ simulations for various values of $z$. We observe that the system converges to a fixed point after a few time steps, and the final probability $q_\text{final}$ is consistent with the theoretical predictions. As expected, we find that:</p> <ul> <li>in the high-herding regime $z&lt;z_c$, the system can converge to either $q_-$ or $q_+$ depending on the initial conditions, and we have $q_- \simeq 0$ and $q_+ \simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>.</li> <li>in the low-herding regime $z&gt;z_c$, the system converges but toward a wider spectrum of values, which are above $p$ on average i.e. $\langle q_\text{final} \rangle &gt; p$.</li> </ul> <p>We see in particular that the richness of the phase transition towards $z\simeq z_c$ is well captured by the ABM simulations.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/runs-480.webp 480w,/assets/img/posts/curty_marsili_game/runs-800.webp 800w,/assets/img/posts/curty_marsili_game/runs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/runs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Evolution of $q_t$ over time for $n=100$ simulations for various values of $z$. The system converges to a fixed point after a few time steps. The ensemble final probability $q_\text{final}$ is indicated by the y-tick on the right. </div> <p>We notice that $q_\text{final}(z)$ seems to increase with $z&lt;z_c$ then decrease with $z&gt;z_c$. To investigate this behavior, we plot the average final probability $\langle q_\text{final} \rangle$ over $n=100$ simulations for various values of $z$. The results are given in In <a href="#fig-3">Figure 3</a>. Interestingly, we see that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$, except near the $z=0$. This implies that herding is always a better strategy than being a fundamentalist… <strong>except when there are too many herders in the system!</strong></p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/q_z-480.webp 480w,/assets/img/posts/curty_marsili_game/q_z-800.webp 800w,/assets/img/posts/curty_marsili_game/q_z-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/q_z.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Average final probability $\langle q_\text{final} \rangle$ over $n=1000$ simulations for various values of $z$. Note that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$ except near $z=0$, meaning that herding is a better strategy than being a fundamentalist except when virtually everyone adopts the herding strategy! </div> <p><em>Let’s finish by showing how letting $z$ fluctuate naturally leads to a Nash equilibrium.</em></p> <h2 id="v-nash-equilibrium">V. Nash Equilibrium</h2> <p>We have seen that if there aren’t too many herders (i.e. if $z$ isn’t too low), then $\langle q_\text{final}(z) \rangle &gt; p$, i.e. herders are more accurate than fundamentalists on average. In this case, it is rational for fundamentalist agents to become herders, which means that $z$ will decrease. However there cannot be too many herders, since in the limit $z\to 0$ we have $q_\text{final}=\frac{1}{2}$ as all agents are herders and thus there is not information (“edge”) in the system. We thus expect the system to self-organize until the proportion $z$ of fundamentalists fluctuates around a critical value $z^\dagger$ such that $\langle q_\text{final}(z^\dagger) \rangle = p$. This is the Nash equilibrium of the system, where fundamentalists and herders coexist and the system is efficient. (or arbitrage-free in the context of financial markets)</p> <p>We can show <sup id="fnref:curtymarsili:1"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> that the Nash equilibrium $z^\dagger$ is given by $z^\dagger \sim N^{1/2}$ where $N$ is the total number of agents. This means that most agents are followers and there is a little minority of $\sqrt{N}$ fundamentalists feeding information (“edge”) to the system.</p> <p>Additionally, note that since $z^\dagger$ is very small, we have $q_-\simeq 0$ and $q_+\simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>. Then, if we denote $p_-$ (resp $p_+$) the probability that the system converges to $q_-$ (resp $q_+$) given the initial conditions, we have $\langle q_\text{final} \rangle = p_- q_- + p_+ q_+$, which at the Nash equilibrium rewrites $p=p_+$, meaning that the probability of the herder mass to converge to the truth ($q_+$) is $p$, as if they represented a single fundamentalist agent!</p> <h2 id="conclusion">Conclusion</h2> <p>Despite its simplicity, the Curty &amp; Marsili game suffices to display non-trivial behavior such as phase coexistence and ergodicity breaking. The game is a good illustration of how herding can be a good strategy… until too many agents adopt it and the whole herding population starts behaving like a single agent which is correct with probability $\langle q_\text{final} \rangle$. Finally, if we let agents switch strategy, $z$ will naturally converge to the efficient state $z^\dagger$ where $\langle q_\text{final} \rangle = p$ such that no strategy has an edge over the other. We find that $z^\dagger \sim N^{1/2}$, meaning that <strong>it is optimal (in a game-theoretic sense) that most agents are followers and a little minority of fundamentalists is feeding information to the system</strong>.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:curtymarsili"> <p><em>Phase coexistence in a forecasting game.</em> Curty, P. &amp; Marsili, M. (2008) <a href="https://wrap.warwick.ac.uk/id/eprint/1769/1/WRAP_Curty_fwp05-15.pdf">PDF</a> <a href="#fnref:curtymarsili" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:curtymarsili:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:condorcet"> <p>Note the similarity with the <strong>Condorcet Jury Theorem</strong>, where the probability of a correct decision by a majority vote increases with the number of jurors and their individual accuracy. <a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem">wikipedia</a> <a href="#fnref:condorcet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mesa"> <p><em>Mesa: An Agent-Based Modeling Framework in Python.</em> <a href="https://mesa.readthedocs.io/">mesa.readthedocs.io</a> <a href="#fnref:mesa" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="agent-based-model,"/><category term="game-theory"/><summary type="html"><![CDATA[TL;DR: When faced with a forecasting task, one can either seek information or follow the crowd. The Curty & Marsili game stacks fundamentalists against herders in a binary forecasting task, revealing phase coexistence and ergodicity breaking under certain conditions. We propose a theoretical study of the game's behavior and validate it through ABM simulations.]]></summary></entry><entry><title type="html">Listening to the Market Mode</title><link href="https://gaetanx21.github.io/blog/2025/market-mode/" rel="alternate" type="text/html" title="Listening to the Market Mode"/><published>2025-02-12T00:00:00+00:00</published><updated>2025-02-12T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/market-mode</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/market-mode/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>We first motivate the use of Principal Component Analysis (PCA) on returns to extract the market mode in equities. This mode is crucial for understanding the market risk and comparing it against other risks. We then do a (very) quick recap on Random Matrix Theory (RMT), which provides a theoretical framework for understanding the eigenspectrum of random matrices. Finally, we apply PCA on S&amp;P 500 components to extract the market mode and we monitor its evolution over time, drawing a comparison with the VIX index.</p> <h2 id="motivation">Motivation</h2> <p>Among the various asset classes (e.g., equities, bonds, commodities), equities tend to provide the highest returns in absolute terms (i.e. not adjusted for risk). Equities are exposed to a multitude of risk factors, with <strong>market risk</strong> being the dominant one<sup id="fnref:CAPM"><a href="#fn:CAPM" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. As such, understanding the market risk and how much of the variance it explains is crucial for risk management and portfolio construction.</p> <p>In a nutshell, the question we want to answer is the following: <strong>can we measure the market risk and compare its weight against other risks?</strong></p> <p>Perhaps the simplest approach to gauge market risk is to look at ready-made proxies such as the <strong>VIX index</strong><sup id="fnref:VIX"><a href="#fn:VIX" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. However, the method of computing the VIX is debatable and may not capture the market risk accurately. A more data-driven approach is to extract the market mode from market returns using PCA, as explained in the next section.</p> <h2 id="pca-on-returns--statistical-factor-model">PCA on Returns = Statistical Factor Model</h2> <p>In a nutshell, a <strong>factor model</strong> describes the variance observed in a set of correlated variables (in our case, stock returns) using a smaller number of <strong>unobserved factors</strong>, which we hope to be more or less independent &amp; more or less interpretable. The idea is to decompose the observed variables $X_i$ as linear combinations of the factors $F_k$ plus some idiosyncratic noise $\varepsilon_i$:</p> \[X_i = \sum_k \beta_k^{(i)} F_k + \varepsilon_i\] <p>where $\beta_k^{(i)}$ is the (factor) loading of the $i$-th asset on the $k$-th factor.</p> <p>Now, the hard part is to find <strong>good factors</strong>. One approach is to simply purchase them from vendors like MSCI (Barra models) who gather a lot of data and knowledge to build these factors. Another (cheaper &amp; more transparent) approach is to extract them via PCA. In this case, the factors are obtained as the eigenvectors of the correlation matrix of returns. This is nice for several reasons:</p> <ul> <li>eigenvectors are orthogonal<sup id="fnref:spectral"><a href="#fn:spectral" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, meaning our factors are uncorrelated;</li> <li>eigenvalues directly give us the amount of variance explained by each factor;</li> <li>the factors are interpretable as they are linear combinations of the original variables.</li> </ul> <p>Note that since the factors are obtained in a purely data-driven fashion (without any human/economic prior), we call this approach a <strong>statistical</strong> factor model, as opposed to classical factor models like the CAPM or the Fama-French Three-Factor Model.</p> <p><em>Before trying out PCA on real data, let’s quickly brush up on Random Matrix Theory (RMT), which provides a neat theoretical framework for understanding the eigenspectrum of correlation matrices.</em></p> <h2 id="brush-up-on-random-matrix-theory-rmt">Brush Up on Random Matrix Theory (RMT)</h2> <p>Consider a $T \times N$ matrix $X$ filled with i.i.d. Gaussian entries $X_{ij} \sim \N(0,\sigma^2)$. Typically, $X$ is called the <strong>design matrix</strong> and each one of the $T$ rows corresponds to one observation of the $N$ variables of interest. In our case, the variables are the daily close-to-close returns of the $N=500$ stocks in the S&amp;P 500 index.</p> <p>If we want to study the correlation between the variables, we first compute a standardized version of the design matrix $\tilde{X}$ by subtracting the mean and dividing by the standard deviation for each column. The sample correlation matrix is then given by $C = \frac{1}{T} \tilde{X}^T \tilde{X}$.</p> <p>Finally, $C$ is real symmetric so we know from the spectral theorem that it can be diagonalized in an orthonormal basis of eigenvectors. In fact $C$ is also positive semi-definite, so all its eigenvalues are non-negative.</p> <h4 id="marchenko-pastur-theorem">Marchenko-Pastur Theorem</h4> <p>RMT studies the behavior of the eigenvalues of $C$ when $T,N \to \infty$ with $Q = T/N$ fixed. The main result is the <strong>Marchenko-Pastur (MP) theorem</strong>:</p> \[L_N(\lambda) = \frac{1}{N} \sum_{i=1}^N \delta(\lambda - \lambda_i) \xrightarrow[N,T \to \infty]{\mathcal{W}} \mathbb{P}_\text{MP}(\lambda) = \frac{Q}{2\pi \sigma^2} \frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda} \mathbb{1}_{[\lambda_-, \lambda_+]}(\lambda)\] <p>where $\lambda_{\pm} = \sigma^2(1\pm\sqrt{\frac{1}{Q}})^2$ are the limiting bounds of the spectrum.</p> <p>The MP theorem tells us that the empirical spectral density of the correlation matrix $L _ N(\lambda)$ converges (weakly in distribution) toward the MP distribution $\mathbb{P} _ \text{MP}$ as $T,N \to \infty$.</p> <p>This is quite remarkable: we could have expected the eigenvalues to be unbounded as $T,N \to \infty$, but RMT tells us that they are actually bounded and gives us the exact form of the limiting distribution.</p> <p>In fact, we can relax some hypotheses and the MP theorem will still hold, though convergence may be (significantly) slower. For example, the entries of $X$ don’t have to be Gaussian. This is important in our case because we know that Gaussianity is a strong assumption for financial data. In practice returns have fat tails and are often skewed. A Student-distribution is already a much better model for returns. <em>Good news, MP still holds for Student-distributed entries!</em></p> <h4 id="link-with-pca">Link with PCA</h4> <p>Now, what does this have to do with PCA? Well, the MP theorem tells us that the eigenvalues of the correlation matrix are bounded and distributed according to a known law. This is useful because it allows us to detect the presence of <strong>signal</strong> in the data. If the eigenvalues are significantly larger than the MP bounds, then we can say that the data contains some structure that is not due to randomness. On the contrary, eigenvalues inside the “noise band” defined by the MP law are considered to be due to randomness. Thus, <strong>we can use the MP theorem to filter out noise and extract only the significant factors from the data</strong>. In particular, when doing PCA on market returns, one eigenvalue will stand out from all the others: it represents the dominant variance component due to the market, and as such we call it the <strong>market mode</strong>. It is approximately equally distributed between all the stocks and is the most important factor in the data.</p> <p>In this last section, we illustrate the above ideas through an experiment on US equities. Specifically, we compute rolling PCAs on the correlation matrix of S&amp;P 500 components and analyze the behavior of the market mode over time.</p> <h2 id="experiment-on-us-equities">Experiment on US Equities</h2> <p>To run our experiment, we first need some data. We chose US equities because they are the most liquid and it’s easy to get clean data. We fetched the daily close-to-close returns of the S&amp;P 500 components from Yahoo Finance using the <code class="language-plaintext highlighter-rouge">yfinance</code> Python package. We considered the period 2000–2025<sup id="fnref:sp500"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <h4 id="pca-on-2020-2024-returns">PCA on 2020-2024 returns</h4> <p>Let’s begin by computing the correlation matrix of the daily returns of the S&amp;P 500 components for the period 2020–2024. We then compute the eigenvalues of the correlation matrix and plot them against the MP bounds. The results are shown in <a href="#fig-1">Figure 1</a>. Importantly, <u>the largest eigenvalues are outside the plot</u> for better visibility. There is only ~10 of them, but they are much larger than the rest. <strong>In particular, the first principal component stands out from all the others: it represents the market mode.</strong> The other significant eigenvalues are due to sectoral correlations, which are also interesting to study but outside the scope of this post.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_true-480.webp 480w,/assets/img/posts/market-mode/mp_true-800.webp 800w,/assets/img/posts/market-mode/mp_true-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_true.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024. The largest eigenvalues are outside the plot. </div> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/dist_eigvec_mode-480.webp 480w,/assets/img/posts/market-mode/dist_eigvec_mode-800.webp 800w,/assets/img/posts/market-mode/dist_eigvec_mode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/dist_eigvec_mode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Distribution of the eigenvector of the market mode as well as a noisy mode for the period 2020–2024. Note how the market mode is approximately equally distributed between all the stocks whereas the noisy mode follows a normal distribution, which makes sense because it has no information and thus must maximize entropy. </div> <p>As an extra step, we can shuffle the returns to destroy the correlation structure and then recompute the eigenvalues. The results are shown in <a href="#fig-3">Figure 3</a>. Notice how all the eigenvalues are now neatly inside the MP bounds, which confirms that the structure in the data is due to correlations and not randomness.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_shuffled-480.webp 480w,/assets/img/posts/market-mode/mp_shuffled-800.webp 800w,/assets/img/posts/market-mode/mp_shuffled-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_shuffled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024 after shuffling the returns. All the eigenvalues are now inside the MP bounds and indeed follow the MP distribution. </div> <h4 id="rolling-pca-on-2000-2024-returns">Rolling PCA on 2000-2024 returns</h4> <p>Now that we’ve seen how PCA works on a single sample of market returns, let’s apply it to a rolling window of the daily returns of the S&amp;P 500 components for a large period of time. The idea is to monitor the evolution of the ratio $\lambda_\text{max} / \sum_i \lambda_i$ over time, where $\lambda_\text{max}$ is the largest eigenvalue (market mode) and $\lambda_i$ are the other eigenvalues. This ratio gives us an idea of how much of the variance is explained by the market mode. Intuitively, we expect this ratio to be high during times of uncertainty / fear / crisis as the market becomes even more important in driving returns. To check this hypothesis, we compare the ratio to the VIX index.</p> <p>We’ll be looking at the period 2000–2024, which includes the 2008 financial crisis and the 2020 COVID-19 pandemic. We will take 6-month rolling windows with a 1-month step size and compute the ratio $\lambda_\text{max} / \sum_i \lambda_i$ for each window. Note that for technical reasons<sup id="fnref:sp500:1"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, we only consider the top 420 companies in the S&amp;P 500 index (ranked by daily trading volume) instead of the full 500. However taking the top 420 companies or top 500 companies doesn’t change the results significantly<sup id="fnref:proof"><a href="#fn:proof" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.</p> <p>The results are shown in <a href="#fig-4">Figure 4</a>. The ratio $\lambda_\text{max} / \sum_i \lambda_i$ is plotted in green and the VIX index is plotted in red. We can see that the two series are quite correlated, which confirms our intuition. In particular, we see that the ratio spikes during the 2008 financial crisis, and the 2020 COVID-19 pandemic. This is a nice result as it shows that PCA can indeed capture the market mode and that it is a good proxy for market risk.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/TOP420-480.webp 480w,/assets/img/posts/market-mode/TOP420-800.webp 800w,/assets/img/posts/market-mode/TOP420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/TOP420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Ratio $\lambda_\text{max} / \sum_i \lambda_i$ (green) and VIX index (red) over the period 2000–2024. The two series are quite correlated, which confirms our intuition that the market mode is a good proxy for market risk. </div> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen how PCA can be used to extract the market mode from equities’ return data. This mode is crucial for understanding the market risk and weighing it against other risks. We’ve also seen how Random Matrix Theory provides a theoretical framework for understanding the eigenspectrum of correlation matrices. Finally, we’ve applied PCA on S&amp;P 500 individual returns to extract the market mode and we’ve analyzed its behavior over time, drawing a comparison with the VIX index.</p> <p>Note that the market mode is not the only factor that matters. If we stick to the PCA approach (statistical factor model), there are several other eigenvalues outside the MP noise band. These eigenvalues and the corresponding eigenvectors correspond to sectors of the US economy (e.g., tech, finance, utilities). They too are risk factors, albeit less important than the market mode. In practice, depending on the context, one may want to consider these factors, for instance to build a sector-neutral portfolio.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:CAPM"> <p>The Capital Asset Pricing Model (CAPM) is the most simple factor model as it relies on the market factor only. In a nutshell, it posits that the expected return of an asset is linearly related to the expected return of the market depending on the asset’s correlation with the market, known as the beta coefficient. <a href="#fnref:CAPM" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:VIX"> <p>The VIX index is a measure of the market’s expectation of volatility over the next 30 days. It is calculated using the implied volatility of S&amp;P 500 options and is often referred to as the “fear gauge” as it tends to spike during market downturns. <a href="#fnref:VIX" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectral"> <p>The eigenvectors of a real symmetric matrix are orthogonal. This is a consequence of the spectral theorem, which states that a real symmetric matrix can be diagonalized by an orthonormal basis of eigenvectors. <a href="#fnref:spectral" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sp500"> <p>Note that the composition of the S&amp;P 500 index changes over time as companies are added or removed. We use the current components of the index for each year. Sometimes too many components change and this causes problems. One simple solution is to only consider the top 420 companies (instead of 500), which are more stable. (NB: we rank the companies by daily trading volume.) <a href="#fnref:sp500" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:sp500:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:proof"> <p>One way to confirm this intuition is to re-run the experiment but looking at the top 100 companies instead of the top 420. The results are very similar, which shows that the market mode is indeed robust to the number of companies considered (as long as it’s not too small of course). <a href="#fnref:proof" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="random-matrix-theory,"/><category term="linear-algebra,"/><category term="quant-finance"/><summary type="html"><![CDATA[TL;DR: Performing PCA on returns amounts to constructing a statistical factor model. The largest eigenvalue corresponds to the market mode and far outweighs the other factors. Thus, one can perform rolling PCA on equities' returns to monitor the market risk over time.]]></summary></entry><entry><title type="html">Jeffreys’ Prior in Bayesian Inference</title><link href="https://gaetanx21.github.io/blog/2025/jeffreys-prior/" rel="alternate" type="text/html" title="Jeffreys’ Prior in Bayesian Inference"/><published>2025-02-07T00:00:00+00:00</published><updated>2025-02-07T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/jeffreys-prior</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/jeffreys-prior/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}} \newcommand{\mathcalL}{\mathcal{L}}\] <p>We first motivate the need for “objective” priors in Bayesian inference by highlighting the limitations of uniform priors. We then introduce Jeffreys’ prior, which is invariant under reparametrization and provides a principled way to assign priors in Bayesian inference. We prove its invariance under reparametrization and illustrate its use in a coin flip problem. Note that throughout this post we restrict ourselves to the one-dimensional case for simplicity.</p> <h2 id="introduction">Introduction</h2> <p>In Bayesian inference, prior distributions encode our initial beliefs about an unknown parameter $\theta$ before observing data $x$. We can then update these beliefs using Bayes’ theorem to obtain a posterior distribution. Namely: <em>posterior = likelihood x prior</em>, which can be rewritten as $p(\theta | x) \propto p(x | \theta) p(\theta)$.</p> <p>Choosing priors usually involves a trade-off between incorporating prior knowledge and maintaining objectivity. Depending on the context and how much we know about the problem, we might have different beliefs about the parameter, or no beliefs at all. For instance, if we’re doing linear regression on standardized data ($y _ i = \beta^T x _ i + \varepsilon _ i$), we may feel like our prior for $\beta$ should be centered around zero. But if we’re doing a coin flip experiment ($X_i \sim B(\theta)$), we might not have any strong prior beliefs about the bias of the coin. So how do we choose the prior in this case? One naive approach would be to use a flat prior $p(\theta) \sim U([0, 1])$. This prior seems uninformative but it really isn’t. To see why, let’s consider the same coin flip experiment but this time we want to estimate the odds ratio $\phi = \frac{\theta}{1 - \theta}$. We may again naively choose a flat prior $p(\phi) \propto 1$<sup id="fnref:improper"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. But this flat prior on $\phi$ induces a non-flat prior on $\theta$! In fact, since $\phi$ is uniform on $\R_+$<sup id="fnref:improper:1"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and as such biased towards arbitrarily large values, $\theta = \frac{\phi}{1 + \phi}$ is highly biased towards $1$, as illustrated on <a href="#fig-1">Figure 1</a>. Thus, choosing a flat prior for $\phi$ is not the same as choosing a flat prior for $\theta$! That is why the seemingly objective choice of a flat prior is not always the best choice.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/theta-480.webp 480w,/assets/img/posts/jeffreys_prior/theta-800.webp 800w,/assets/img/posts/jeffreys_prior/theta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/theta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="theta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sketch of the prior distribution on $\theta$ induced by a flat prior on $\phi=\frac{\theta}{1-\theta}$. Clearly, the prior is not flat and is biased towards $\theta=1$. </div> <p>Likewise, choosing a flat prior in high-dimensional spaces assigns way too much mass to unimportant regions of the parameter space, so it is informative, but in a bad way!</p> <p>With this in mind, we see that <strong>uniform priors are no silver bullet</strong>. Ideally, we would like a prior which does not depend on the parameterization of the problem. In other word, <strong>the information we encode in the prior should be invariant under reparametrization</strong>. If we go back to the example of the coin flip, we would like a prior that encodes the same prior information about the bias of the coin, regardless of whether we’re working with $\theta$ or $\phi$. Intuitively, such a prior should be based on the <strong>structure of the data model</strong> itself, rather than the parameterization we choose.</p> <p>Reparametrization invariance is exactly what Jeffreys’ prior achieves, as explained below.</p> <p><em>Note that intuitively, reparametrization invariance is a good heuristic for an “objective” prior.</em></p> <h2 id="definition">Definition</h2> <p>Jeffreys’ prior is defined using the Fisher information matrix. Given a likelihood function $\mathcalL(\theta | x)$ for a parameter $\theta$, the Fisher information is:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right].\] <p>Jeffreys’ prior is then given by:</p> \[\pi_J(\theta) \propto \sqrt{I(\theta)}.\] <p>The key property of Jeffrey’s prior is that it is invariant under reparametrization. In other words, if we try to estimate a different parameter $\phi = g(\theta)$, the Jeffrey’s prior for $\phi$ will be:</p> \[\pi_J(\phi) \propto \sqrt{I(\phi)} = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|\] <p>which is consistent with the transformation rule for probability densities.</p> <p><em>Note that Jeffrey’s prior is defined using the likelihood function. While this is convenient because it allows us to use the structure of the data model, it also goes against the Bayesian principle of choosing the prior independently of the data. This is a philosophical issue in Bayesian statistics, and different practitioners may have different views on this.</em></p> <h2 id="proof-of-invariance-under-reparametrization">Proof of Invariance Under Reparametrization</h2> <p>In this paragraph we demonstrate Jeffreys’ prior invariance under reparametrization. Suppose we have a parameter $\theta$ and a reparametrized parameter $\phi = g(\theta)$. We want to show that Jeffrey’s prior for $\phi$ is consistent with the transformation rule for probability densities.</p> <p>To begin with, note that the chain rule gives:</p> \[I(\phi) = I(\theta) \left( \frac{d\theta}{d\phi} \right)^2.\] <p>Taking the square root, we get:</p> \[\sqrt{I(\phi)} = \sqrt{I(\theta)} \left| \frac{d\theta}{d\phi} \right|\] <p>i.e., Jeffrey’s prior transforms as:</p> \[\pi_J(\phi) = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|.\] <p>We recognize the transformation rule for probability densities, which demonstrates that Jeffrey’s prior correctly transforms to maintain consistency, proving its invariance by reparametrization.</p> <h2 id="coin-flip-example">Coin flip example</h2> <p>Let’s compute Jeffreys’ prior for a simple coin flip problem to illustrate its use.</p> <p>Consider a simple example: estimating the bias $\theta$ of a coin, where $X \sim \text{Bin}(n, \theta)$. The likelihood function is:</p> \[\mathcal L(\theta | x) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i}.\] <p>We compute the Fisher information:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right] = \frac{n}{\theta (1 - \theta)}.\] <p>Thus, Jeffreys’ prior for $\theta$ is:</p> \[\pi_J(\theta) \propto \sqrt{\frac{n}{\theta (1 - \theta)}} \propto \frac{1}{\sqrt{\theta (1 - \theta)}}.\] <p>We recognize the <strong>Beta(1/2, 1/2)</strong> distribution, which is commonly used as an uninformative prior for bounded parameters. This is a nice result, as it shows that Jeffreys’ prior is consistent with our intuition of an uninformative prior in this case.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/beta-480.webp 480w,/assets/img/posts/jeffreys_prior/beta-800.webp 800w,/assets/img/posts/jeffreys_prior/beta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/beta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="beta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Jeffreys' prior for the bias of a coin flip experiment is the Beta(1/2, 1/2) distribution. </div> <h2 id="conclusion">Conclusion</h2> <p>Jeffreys’ prior provides a principled way to assign priors in Bayesian inference, ensuring invariance under reparametrization. We proved its reparametrization invariance and illustrated its use in a coin flip problem. Jeffreys’ prior is useful when no clear subjective prior information is available, for instance in astrophysics. We’ve limited ourselves to the one-dimensional case for simplicity, but Jeffreys’ prior can be extended to higher dimensions naturally by considering the Fisher information matrix and its determinant, such that $\pi_J(\theta) \propto \sqrt{\text{det}(I(\theta))}$. Finally, I want to stress that Jeffreys’ prior violates the Bayesian principle of choosing the prior independently of the data, which may be a concern for some practitioners.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:improper"> <p>The prior $p(\phi) \propto 1$ is called an <em>improper</em> prior since it doesn’t integrate to 1. This is a common pitfall when using flat priors. However using unnormalized priors is okay as long as we normalize the posterior distribution. <a href="#fnref:improper" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:improper:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="bayesian-ml"/><summary type="html"><![CDATA[TL;DR: Bayesian inference requires us to specify a prior distribution. When we're unsure what prior to pick and want to stay as objective as possible, one option is to use Jeffreys' prior, which leverages the Fisher information to provide a reparametrization-invariant prior.]]></summary></entry><entry><title type="html">Regression Dilution</title><link href="https://gaetanx21.github.io/blog/2025/regression-dilution/" rel="alternate" type="text/html" title="Regression Dilution"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/regression-dilution</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/regression-dilution/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}}\] <p>We first introduce the concept of <strong>attenuation bias</strong> in linear regression due to measurement error in the covariates. We derive the shrinkage effect in the one-dimensional case and extend it to the multivariate case. We do simulations to visualize the shrinkage effect as the signal-to-noise ratio (SNR) goes to zero.</p> <h2 id="introduction">Introduction</h2> <p>In classical linear regression, we assume a model of the form:</p> \[y = X\beta + \varepsilon\] <p>where $X$ is an $n \times p$ matrix of covariates $x_i \in \R^p$, $\beta$ is a $p \times 1$ vector of coefficients, and $\varepsilon$ is noise. <strong>Weak exogeneity</strong> is a key assumption in linear regression, which states that the covariates $X$ are fixed and non-random. In other words, the covariates are assumed to be measured without error. However, in many real-world scenarios, this hypothesis is violated: covariates themselves contain measurement noise:</p> \[\tilde{X} = X + U\] <p>where $U$ is an $n \times p$ matrix of noise $u_i \in \R^p$. This additional noise leads to a phenomenon known as <strong>attenuation bias</strong>, where the estimated coefficients shrink towards zero. Let’s first derive this effect in the one-dimensional case.</p> <p>Note that in what follows we make the following the classical assumptions:</p> <ul> <li>$x_i$ i.i.d. centered with variance $\sigma_x^2$ (or covariance $\Sigma_x$ in the multivariate case),</li> <li>$u_i$ i.i.d. centered with variance $\sigma_u^2$ (or covariance $\Sigma_u$ in the multivariate case),</li> <li>$\varepsilon_i$ i.i.d. centered with variance $\sigma_\varepsilon^2$,</li> <li>$x_i, u_i, \varepsilon_i$ are independent of each other</li> </ul> <h2 id="one-dimensional-case">One-dimensional case</h2> <p>Let’s first derive the attenuation bias in the one-dimensional case.</p> <p>Consider the simple case of a one-dimensional linear regression model:</p> \[y = \beta x + \varepsilon.\] <p>Now assume that we observe a noisy version of $x$: $\tilde{x} = x + u$, where $u$ is the noise term. The least squares estimator of $\beta$ using the noisy covariate $\tilde{x}$ is:</p> \[\hat{\beta} = \frac{\Cov(\tilde{x}, y)}{\Var(\tilde{x})} = \frac{\Cov(x + u, \beta x + \varepsilon)}{\Var(x + u)} = \frac{\beta \Var(x)}{\Var(x) + \Var(u)} = \frac{\beta \sigma_x^2}{\sigma_x^2 + \sigma_u^2} = \lambda \beta\] <p>where $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}&lt;1$ is the attenuation factor or shrinkage factor.</p> <p>Thus the estimated coefficient $\hat{\beta}$ is a scaled version of the true coefficient $\beta$, with the scaling factor $\lambda$ being less than 1. This implies that the estimated coefficient is biased towards zero due to the noise in the covariate.</p> <p>In particular, note that when $\sigma_u = 0$, we recover the unbiased estimator $\hat{\beta} = \beta$. Likewise, as $\sigma_u \to \infty$, the estimated coefficient $\hat{\beta} \to 0$ since the SNR goes to zero.</p> <h2 id="multivariate-case">Multivariate case</h2> <p>The multivariate case can be derived similarly, though the algebra is slightly more involved.</p> <p>If we use the noisy covariates $\tilde{X}$ instead of the true covariates $X$, the least squares estimator becomes:</p> \[\hat{\beta} = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T y.\] <p>Substituting $\tilde{X} = X + U$ and $y = X\beta + \varepsilon$ gives:</p> \[\hat{\beta} = [(X + U)^T (X + U)]^{-1} (X + U)^T (X\beta + \varepsilon)\] <p>We rewrite this expression so that the law of large numbers can be applied:</p> \[\hat{\beta} = \bigg[\frac{1}{n}(X^T X + X^T U + U^T X + U^T U)\bigg]^{-1} \bigg[\frac{1}{n}(X^T X\beta + X^T \varepsilon + U^T X\beta + U^T \varepsilon)\bigg]\] <p>Using the weak law of large numbers, we have</p> \[\begin{align*} \frac{1}{n}X^T X &amp;\to \E[x x^T] = \Sigma_x \\ \frac{1}{n}X^T U &amp;\to \E[x u^T] = 0 \\ \frac{1}{n}U^T X &amp;\to \E[u x^T] = 0 \\ \frac{1}{n}U^T U &amp;\to \E[u u^T] = \Sigma_u \end{align*}\] <p>and</p> \[\begin{align*} \frac{1}{n}X^T X\beta &amp;\to \E[x x^T]\beta = \Sigma_x \beta \\ \frac{1}{n}X^T \varepsilon &amp;\to \E[x \varepsilon_i^T] = 0 \\ \frac{1}{n}U^T X\beta &amp;\to \E[u x^T]\beta = 0 \\ \frac{1}{n}U^T \varepsilon &amp;\to \E[ \varepsilon_i^T] = 0 \end{align*}\] <p>where all the convergences are in probability<sup id="fnref:strong"><a href="#fn:strong" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Combining these results and applying Sluskty’s lemma and the continuous mapping theorem, we have:</p> \[\hat{\beta} \xrightarrow[]{\mathbb{P}} (\Sigma_x + \Sigma_u)^{-1} \Sigma_x \beta = \left(I + \Sigma_x^{-1} \Sigma_u \right)^{-1} \beta.\] <p>Note that in the multi-dimensional case, the shrinkage factor is not a scalar but a matrix $\Lambda = (I + \Sigma_x^{-1} \Sigma_u)^{-1}$. In particular, although $\Sigma_x$ and $\Sigma_u$ are positive definite matrices, $\Sigma_x^{-1} \Sigma_u$ is not positive definite in general. Therefore it is more difficult to interpret the shrinkage effect in the multivariate case.</p> <p>For simplicity, if we assume spherical noise on both covariates and response, i.e., $\Sigma_x = \sigma_x^2 I$ and $\Sigma_u = \sigma_u^2 I$, we recover the one-dimensional result with $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$. This makes sense because assuming spherical noise is like running the one-dimensional case independently for each covariate.</p> <p>Additionally, we recover the unbiased estimator $\hat{\beta} = \beta$ when $\Sigma_u = 0$, as expected.</p> <h2 id="visualizing-the-shrinkage-effect-as-snr-goes-to-zero">Visualizing the shrinkage effect as SNR goes to zero</h2> <p>We want to illustrate the gradual shrinkage of the estimated coefficients as the SNR gradually decreases. We stick to the one-dimensional case for simplicity.</p> <p>We simulate a linear regression model with a single covariate $x$ with $\sigma_x = 1$ and noise $u$ with $\sigma_u$ running from $0$ to $5 \sigma_x$. For each value of $\sigma_u$, we fit a linear regression model using the noisy covariate $x + u$ and record the estimated coefficient $\hat{\beta}$.</p> <p>We then plot the empirical shrinkage ratio $\frac{\hat{\beta}}{\beta}$ as a function of the SNR $\frac{\sigma_x}{\sigma_u}$. Additionally, we overlay the theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$.</p> <p>The results are shown in <a href="#fig-1">Figure 1</a>. As the SNR decreases, the estimated coefficients shrink towards zero, as expected. The empirical shrinkage ratio closely follows the theoretical shrinkage factor $\lambda$.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/regression_dilution/snr_shrinkage-480.webp 480w,/assets/img/posts/regression_dilution/snr_shrinkage-800.webp 800w,/assets/img/posts/regression_dilution/snr_shrinkage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/regression_dilution/snr_shrinkage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="snr shrinkage" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical shrinkage ratio as a function of the SNR. The theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$ is overlaid. </div> <h2 id="conclusion">Conclusion</h2> <p>When covariates are measured with noise, the estimated regression coefficients shrink towards zero, leading to bias. This is important in fields where measurement errors are common, such as economics and epidemiology. One way to mitigate this bias is to use <strong>error-in-variables models</strong>, which explicitly model the noise in the covariates. The simplest such model is probably Deming regression, which models a one-dimensional linear regression and assumes the SNR to be known. $\hat{\beta}$ is then found by minimizing a <em>weighted</em> sum of squared residual to account for the noise in $x$.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:strong"> <p>The strong law of large numbers would require additional assumptions on the moments of the random variables involved. <a href="#fnref:strong" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="robust-ml"/><summary type="html"><![CDATA[TL;DR: When covariates in linear regression are subject to noise, the estimated regression coefficients shrink towards zero. We derive this effect mathematically and illustrate it with simulations.]]></summary></entry><entry><title type="html">A geodesic from cat to dog</title><link href="https://gaetanx21.github.io/blog/2024/ot-geodesic/" rel="alternate" type="text/html" title="A geodesic from cat to dog"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-geodesic</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-geodesic/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete entropy-regularized Kantorovich problem and show how it can be solved efficiently using the Sinkhorn algorithm. We then illustrate the usefulness of the Sinkhorn algorithm to compute Wasserstein distances, barycenters, and geodesics between probability distributions. We finally apply this to interpolate between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the 2-Wasserstein metric.</p> <h1 id="discrete-entropy-regularized-kantorovich-problem">Discrete Entropy-Regularized Kantorovich problem</h1> <p>The discrete entropy-regularized Kantorovich problem formulates as: \(\begin{equation} \label{eq:Kreg} \tag{$\tn{K}^\tn{reg}$} P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \langle C,P\rangle - \epsilon H(P) \end{equation}\) where $H(P)=\sum _ {i=1}^nP_{ij}(\log P _ {ij}-1)$ is the discrete entropy. Note that when $\epsilon=0$ one recovers classical discrete OT. Crucially, (\ref{eq:Kreg}) is strictly convex as soon as $\epsilon&gt;0$ and thus has a unique solution $P^{\epsilon,\star}$.</p> <p>Additionally, One can easily show that $\langle C,P \rangle - \epsilon H(P)= \tn{KL} (P|K)$, where $K=\exp(-\frac{C}{\epsilon})$ is called a Gibbs kernel. Thus (\ref{eq:Kreg}) can be seen as a projection problem w.r.t. to the KL divergence: (\ref{eq:Kreg}) rewrites as $P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \tn{KL}(P|K)$ i.e. $P^{\epsilon,\star}=\tn{Proj} _ {U(\alpha,\beta)}^\tn{KL}(K)$.</p> <p>The whole point of introducing the entropy is to relax the Kantorovich problem into a strictly convex problem which can be solved efficiently using the Sinkhorn algorithm, which we now introduce.</p> <h2 id="sinkhorns-algorithm">Sinkhorn’s algorithm</h2> <p>The most well-known method to solve (\ref{eq:Kreg}) is Sinkhorn’s algorithm, which uses the fact that $P^{\epsilon,\star}$ necessarily has the form $P^{\epsilon,\star}=\tn{Diag}(u)K\tn{Diag}(v)$ where $K$ is the Gibbs kernel. The conditions $P\mathbb{1} _ m=a$ and $P^T\mathbb{1} _ n=b$ thus rewrite as $u * (Kv) = a$ and $v * (K^Tu) = b$ respectively, where $*$ denotes the Hadamard product. One can thus iteratively solve these two equations until $u$ and $v$ converge, yielding the Sinkhorn algorithm:</p> \[\begin{align*} u^{l+1}&amp;\leftarrow\frac{a}{Kv^l}\\ v^{l+1}&amp;\leftarrow\frac{b}{K^Tu^{l+1}} \end{align*}\] <p>where we use the initialization $v^0=\mathbb{I} _ m$.</p> <p>In practice, Sinkhorn’s algorithm allows us to compute Wasserstein distances efficiently. In turn, we can use these distances to compute barycenters and geodesics between probability distributions.</p> <h2 id="wasserstein-barycenters-and-geodesics-on-probability-spaces">Wasserstein barycenters and geodesics on probability spaces</h2> <p>Using OT, one can define <em>distances</em> between probability distributions defined on the same space $X$. The most common is the $p$-Wasserstein distance $W_p$ defined for any real number $p&gt;0$: \(\begin{equation} \label{eq:Wp} \tag{$W_p$} W_p(\alpha,\beta)=\min_{P\in U(\alpha,\beta)} \langle P,C^p \rangle^{1/p} = \bigg(\sum_{1\leq i,j\leq n} d(x_i,y_j)^p P_{ij}\bigg)^{1/p} \end{equation}\) where $d$ is a distance on $X$. For instance if $X=\R^d$ one can use $d(x,y)=||x-y||$.</p> <p>Now that we have a distance on probability measures, we can use it to compute barycenters. For a fixed $p&gt;1$ and $R$ probability distributions $\alpha_1,\dots,\alpha_R \in \tn{P}(X)$, their $p$-Wasserstein barycenter with coefficients $(\lambda_r)_r$ is defined as: \(\begin{equation} \label{eq:barycenter} \tag{B} \beta = \arg \min_{\beta \in \tn{P}(X)} \sum_{r=1}^{R} \lambda_k W_p(\alpha_r,\beta) \end{equation}\)</p> <p>$p$-Wasserstein barycenters can in particular be used to compute geodesics for the $p$-Wasserstein metric of the form $t\in[0,1]\mapsto\mu_t\in\tn{P}(X)$ from $\alpha$ to $\beta$ as: \(\begin{equation} \mu_t = \arg \min_{\mu\in\tn{P}(X)} (1-t)W_p^p(\alpha,\mu_t) + tW_p^p(\beta,\mu_t) \end{equation}\) When $p=2$, we in fact have $\mu_t=\sum_{1\leq i,j\leq n}P_{ij}^\star \delta_{(1-t)x_i+ty_j}$ using the notations from my <a href="/blog/2024/ot-assignement-problem/">previous post</a>.</p> <h2 id="a-geodesic-from-cat-to-dog">A geodesic from cat to dog</h2> <p>Wasserstein barycenters can be used to interpolate between (grayscale) images using the following formalism: a grayscale image of dimension $N\times N$ can be seen as a distribution of “light” $\alpha\in \tn{P}(\R^{N\times N})$. Then, one can go from an image of a cat $\alpha$ to that of a dog $\beta$ using OT. This has little value in itself, but one can also consider the geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ from $\alpha$ to $\beta$ and thus see the gradual fade from the cat image to the dog image.</p> <p>For $0\leq i \leq 8$ we consider the barycenter coefficients $\lambda=(1-t_i,t_i)$ where $t_i=\frac{i}{8}$ and we plot the 9 corresponding 2-Wasserstein barycenters $b^i\in\tn{P}(\R^{N\times N})$ which intuitively interpolate between the cat and the dog. The pictures were found online, turned to grayscale, resized to $N\times N$ with $N=128$, smooth with a Gaussian kernel, and then each Wasserstein barycenter is computed using the <code class="language-plaintext highlighter-rouge">ot</code> Python library. The results are presented in <a href="#fig-1">Figure 1</a> and quite satisfying for such a simple approach!</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_geodesic/cat2dog-480.webp 480w,/assets/img/posts/ot_geodesic/cat2dog-800.webp 800w,/assets/img/posts/ot_geodesic/cat2dog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_geodesic/cat2dog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cat2dog" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ in $\R^{N\times N}$ computed using 2-Wasserstein barycenters. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that the entropy-regularized Kantorovich problem can be solved efficiently using the Sinkhorn algorithm. This allows us to efficiently compute Wasserstein distances, barycenters, and geodesics between probability distributions. We have illustrated this by interpolating between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the $W_2$ metric.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: Entropic regularization relaxes the Kantorovitch problem into a strictly convex problem which can be solved efficiently with the Sinkhorn algorithm. We can use this to efficiently compute Wasserstein distances, barycenters, and finally geodesics between distributions.]]></summary></entry></feed>