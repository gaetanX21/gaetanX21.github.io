<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-26T21:51:19+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">The Curty &amp;amp; Marsili Forecasting Game</title><link href="https://gaetanx21.github.io/blog/2025/curty-marsili-game/" rel="alternate" type="text/html" title="The Curty &amp;amp; Marsili Forecasting Game"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/curty-marsili-game</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/curty-marsili-game/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>In this post, we present Curty &amp; Marsili’s forecasting game, a simple model that captures how herding behavior can lead to non-trivial opinion outcomes, in particular <strong>phase coexistence</strong> and <strong>ergodicity breaking</strong> under certain conditions. After motivating the study of herding, we formally introduce the Curty &amp; Marsili game and propose a mathematical analysis of its key features. We then perform <strong>Agent-Based Model</strong> (ABM) simulations of the game to validate our theoretical predictions. Finally, we discuss how the game can converge to a Nash equilibrium where fundamentalists and herders coexist and the system is efficient.</p> <hr/> <h2 id="i-motivation">I. Motivation</h2> <p>Herding is a widespread phenomenon in society: people often imitate or follow the actions of others, whether it’s in fashion trends, product adoption, or even protests. In finance, herding can lead to anomalous fluctuations in asset prices. The alternative to herding is to gather (private) information and make decisions based on one’s own analysis. Curty &amp; Marsili’s forecasting game<sup id="fnref:curtymarsili"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a simple ABM which precisely focuses on this tension between <strong>individual forecasting</strong> and <strong>collective herding</strong>.</p> <p>In a nutshell, the question Curty &amp; Marsili tried to answer is: <strong>what’s more efficient, following the crowd or relying on your own judgment?</strong> We’ll see that the answer is not straightforward and can depend on the parameters of the game. In essence, we’ll find that herding can be a sound strategy, but if the proportion of followers in the market becomes too large, herding becomes a dangerous strategy.</p> <p><em>Let’s now present in details of the Curty &amp; Marsili forecasting game.</em></p> <h2 id="ii-the-game">II. The Game</h2> <p>The Curty &amp; Marsili requires the following ingredients:</p> <ul> <li>$N\gg 1$ agents who must make a binary forecast (e.g., election outcome, buy or sell, protest or not, etc.)</li> <li>A fraction $z\in[0,1]$ of agents are <strong>fundamentalists</strong> $F_i$ who rely solely on their private information. They are correct with probability $p&gt;\frac{1}{2}$ (i.e., they have an edge). Their forecast is fixed once and for all and crucially, fundamentalists’ forecasts are mutually independent.</li> <li>The remaining fraction $1-z$ are <strong>herders</strong> $H_i$ who each have a fixed group $G_i$ of $M$ agents which they follow. Their action is determined by majority voting within their group (note that group size $M$ is odd to avoid draws). Importantly, note that groups may include both fundamentalists and herders.</li> </ul> <p>The game then dynamically evolve according to the following rules:</p> <ul> <li>At each time step $t$, all herders are chosen one by one (in a random order) and update their forecast based on the majority opinion of their group $G_i$. (note that herders’ initial forecast are i.i.d. random i.e. correct with probability $\frac{1}{2}$)</li> <li>The fundamentalists $F_i$ do not change their forecast over time. (reflecting their reliance on private information)</li> <li>The process is repeated until convergence to a fixed point (i.e. herders are all following the majority opinion of their group).</li> </ul> <p>The question then is to study the final probability $q$ that a herder makes the correct forecast, computed as the fraction of herders who forecast the correct outcome after the game has converged. More precisely, we want to study $q(z)$, the final probability of a herder making the correct forecast as a function of the fraction of fundamentalists $z$ in the market.</p> <p><em>We now delve into the mathematical analysis of the game.</em></p> <h2 id="iii-mathematical-analysis">III. Mathematical Analysis</h2> <p>Let’s introduce two important notations:</p> <ul> <li>$q_t$ is the probability that a herder makes the correct forecast at time $t$.</li> <li>$\pi_t$ is the probability that a randomly chosen agent makes the correct forecast at time $t$.</li> </ul> <p>Since agents are either fundamentalists or herders, we have the following static equation: \(\begin{equation} \label{eq:static} \pi_t = zp + (1-z)q_t. \end{equation}\)</p> <p>In addition, a herder will make the correct forecast at time $t+1$ if and only if the majority of his group $G_i$ makes the correct forecast at time $t$, i.e. at least $\frac{M+1}{2}$ agents in the group make the correct forecast. This leads to the following dynamic equation: \(\begin{equation} \label{eq:dynamic} q_{t+1} = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} \pi_t^k (1-\pi_t)^{M-k}. \end{equation}\)</p> <p>Combining \eqref{eq:static} and \eqref{eq:dynamic}, we can write the evolution of $q_t$ as: \(\begin{equation} \label{eq:evolution} q_{t+1} = F_z(q_t). \end{equation}\)</p> <p>where $F_z(q) = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} (zp + (1-z)q)^k (1-(zp+(1-z)q))^{M-k}$.<sup id="fnref:condorcet"><a href="#fn:condorcet" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>We can then compute fixed points $q^*(z)$ for the evolution equation \eqref{eq:evolution} for various values of $z\in[0,1]$. The results are illustrated in <a href="#fig-1">Figure 1</a>.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/fixed_points-480.webp 480w,/assets/img/posts/curty_marsili_game/fixed_points-800.webp 800w,/assets/img/posts/curty_marsili_game/fixed_points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/fixed_points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Fixed points of the evolution equation $q_{t+1} = F_z(q_t)$ for various values of $z$. Note the critical value $z_c\simeq \frac{1}{2}$ separating the two regimes. Each curve corresponds to a different fixed point: green for $q_+^*(z)$, blue for $q_-^*(z)$, and red for $q_u^*(z)$ the unstable fixed point. </div> <p>We distinguish two regimes, separated by a critical value $z _ c\simeq \frac{1}{2}$:</p> <ul> <li>for $z&gt;z _ c$ i.e. when there are mostly fundamentalists, there is a single (stable) fixed point $q _ +^ * (z)$. Interestingly, $q^ * (z)&gt;p$ in this regime, meaning that herders are on average more accurate than fundamentalists. As $z$ decreases (while staying above $z_c$), the performance of herders gets even better! This can seem somewhat surprising, but in fact results from the fact that more herders means herders will have more herders in their group $G_i$, which in turn increases their forecast accuracy since in this regime herders are more accurate than fundamentalists.</li> <li>for $z&lt;z _ c$ i.e. when there are mostly herders, two new fixed points appear, both under the line $q=\frac{1}{2}$ which means that these fixed points are bad for herders. Note that $q _ +^* (z)$ keeps increasing as $z$ decreases, while $q _ -^* (z)$ decreases. The unstable fixed point $q _ u^* (z)$ is also shown in red. Numerical simulations show that the system will converge to either $q _ +^* (z)$ or $q _ -^* (z)$ depending on the initial conditions. In fact, the unstable fixed point $q _ u^* (z)$ acts as a <em>separatrix</em> between the two regimes. Thus the initial condition $q_0$ will determine the final state of the system: if $q _ 0&gt;q _ u^* (z)$, the system will converge to $q _ +^* (z)$, otherwise it will converge to $q _ -^* (z)$. This will be useful in the last section where we compare $\langle q \rangle$ to $p$ to find the Nash equilibrium.</li> </ul> <p>What’s interesting is the <strong>phase coexistence</strong> in herding regime $z&lt;z_ c$: if the system converges towards $q_ -^<em>(z)$, then the majority of herders will make the wrong forecast; likewise, if the system converges towards $q_ +^</em>(z)$, the majority of herders will make the correct forecast. This is a clear example of <strong>ergodicity breaking</strong> where the system is stuck in one of the two phases, depending on the initial conditions $q_0$. In the last section we take into account the distribution $q_0\sim N(\frac{N}{2},\frac{N}{4})$ to compute the probability $p_ -$ of the system converging to $q_-$ (and similarly $p_+$ for $q_+$) so we can finally compute the average probability $\langle q \rangle$ of a herder making the correct forecast and compare it to fundamentalists’ accuracy $p$.</p> <p><em>Now that we’ve analyzed the theoretical aspects of the game, let’s move on to simulations to check if its consistent.</em></p> <h2 id="iv-abm-simulation">IV. ABM Simulation</h2> <p>The ABM is pretty straightforward here, with two agents classes (fundamentalists and herders) and at each step of the system, herders are picked one by one in a random order and update their forecast based on the majority opinion of their group. We iterate until convergence of the system i.e. herders’ opinions are stable. We use the <code class="language-plaintext highlighter-rouge">mesa</code> python library<sup id="fnref:mesa"><a href="#fn:mesa" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> which helps build ABMs very easily.</p> <p>Throughout the simulations, we use the following parameters:</p> <ul> <li>$N=1000$ agents in total, with $z\in[0,1]$ the fraction of fundamentalists.</li> <li>$p=0.52$ the probability that a fundamentalist makes the correct forecast.</li> <li>$M=7$ the size of the groups of herders. Note that modifying these parameters will result in quantitative but not qualitative changes in the outcomes.</li> </ul> <p>We run simulations for various values of $z\in[0,1]$ and for each simulation we record $q_t$ at each time step $t$ of the system. We especially care about the final probability $q_\text{final}$ that a herder makes the correct forecast, which is simply the fraction of herders who forecast the correct outcome after the game has converged. We observe the following:</p> <ul> <li>in the low-herding regime $z&gt;z_c$, $q_\text{final}$ is always above $p$ and very close to $q_+$.</li> <li>in the high-herding regime $z&lt;z_c$, $q_\text{final}$ is either close to $q_-$ or $q_+$ depending on the initial conditions. This is consistent with the phase coexistence and ergodicity breaking observed in the theoretical analysis.</li> </ul> <p>In <a href="#fig-2">Figure 2</a>, we plot $q_t$ over time for $n=100$ simulations for various values of $z$. We observe that the system converges to a fixed point after a few time steps, and the final probability $q_\text{final}$ is consistent with the theoretical predictions. As expected, we find that:</p> <ul> <li>in the high-herding regime $z&lt;z_c$, the system can converge to either $q_-$ or $q_+$ depending on the initial conditions, and we have $q_- \simeq 0$ and $q_+ \simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>.</li> <li>in the low-herding regime $z&gt;z_c$, the system converges but toward a wider spectrum of values, which are above $p$ on average i.e. $\langle q_\text{final} \rangle &gt; p$.</li> </ul> <p>We see in particular that the richness of the phase transition towards $z\simeq z_c$ is well captured by the ABM simulations.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/runs-480.webp 480w,/assets/img/posts/curty_marsili_game/runs-800.webp 800w,/assets/img/posts/curty_marsili_game/runs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/runs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Evolution of $q_t$ over time for $n=100$ simulations for various values of $z$. The system converges to a fixed point after a few time steps. The ensemble final probability $q_\text{final}$ is indicated by the y-tick on the right. </div> <p>We notice that $q_\text{final}(z)$ seems to increase with $z&lt;z_c$ then decrease with $z&gt;z_c$. To investigate this behavior, we plot the average final probability $\langle q_\text{final} \rangle$ over $n=100$ simulations for various values of $z$. The results are given in In <a href="#fig-3">Figure 3</a>. Interestingly, we see that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$, except near the $z=0$. This implies that herding is always a better strategy than being a fundamentalist… <strong>except when there are too many herders in the system!</strong></p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/q_z-480.webp 480w,/assets/img/posts/curty_marsili_game/q_z-800.webp 800w,/assets/img/posts/curty_marsili_game/q_z-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/q_z.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Average final probability $\langle q_\text{final} \rangle$ over $n=1000$ simulations for various values of $z$. Note that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$ except near $z=0$, meaning that herding is a better strategy than being a fundamentalist except when virtually everyone adopts the herding strategy! </div> <p><em>Let’s finish by showing how letting $z$ fluctuate naturally leads to a Nash equilibrium.</em></p> <h2 id="v-nash-equilibrium">V. Nash Equilibrium</h2> <p>We have seen that if there aren’t too many herders (i.e. if $z$ isn’t too low), then $\langle q_\text{final}(z) \rangle &gt; p$, i.e. herders are more accurate than fundamentalists on average. In this case, it is rational for fundamentalist agents to become herders, which means that $z$ will decrease. However there cannot be too many herders, since in the limit $z\to 0$ we have $q_\text{final}=\frac{1}{2}$ as all agents are herders and thus there is not information (“edge”) in the system. We thus expect the system to self-organize until the proportion $z$ of fundamentalists fluctuates around a critical value $z^\dagger$ such that $\langle q_\text{final}(z^\dagger) \rangle = p$. This is the Nash equilibrium of the system, where fundamentalists and herders coexist and the system is efficient. (or arbitrage-free in the context of financial markets)</p> <p>We can show <sup id="fnref:curtymarsili:1"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> that the Nash equilibrium $z^\dagger$ is given by $z^\dagger \sim N^{1/2}$ where $N$ is the total number of agents. This means that most agents are followers and there is a little minority of $\sqrt{N}$ fundamentalists feeding information (“edge”) to the system.</p> <p>Additionally, note that since $z^\dagger$ is very small, we have $q_-\simeq 0$ and $q_+\simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>. Then, if we denote $p_-$ (resp $p_+$) the probability that the system converges to $q_-$ (resp $q_+$) given the initial conditions, we have $\langle q_\text{final} \rangle = p_- q_- + p_+ q_+$, which at the Nash equilibrium rewrites $p=p_+$, meaning that the probability of the herder mass to converge to the truth ($q_+$) is $p$, as if they represented a single fundamentalist agent!</p> <h2 id="conclusion">Conclusion</h2> <p>Despite its simplicity, the Curty &amp; Marsili game suffices to display non-trivial behavior such as phase coexistence and ergodicity breaking. The game is a good illustration of how herding can be a good strategy… until too many agents adopt it and the whole herding population starts behaving like a single agent which is correct with probability $\langle q_\text{final} \rangle$. Finally, if we let agents switch strategy, $z$ will naturally converge to the efficient state $z^\dagger$ where $\langle q_\text{final} \rangle = p$ such that no strategy has an edge over the other. We find that $z^\dagger \sim N^{1/2}$, meaning that <strong>it is optimal (in a game-theoretic sense) that most agents are followers and a little minority of fundamentalists is feeding information to the system</strong>.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:curtymarsili"> <p><em>Phase coexistence in a forecasting game.</em> Curty, P. &amp; Marsili, M. (2008) <a href="https://wrap.warwick.ac.uk/id/eprint/1769/1/WRAP_Curty_fwp05-15.pdf">PDF</a> <a href="#fnref:curtymarsili" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:curtymarsili:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:condorcet"> <p>Note the similarity with the <strong>Condorcet Jury Theorem</strong>, where the probability of a correct decision by a majority vote increases with the number of jurors and their individual accuracy. <a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem">wikipedia</a> <a href="#fnref:condorcet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mesa"> <p><em>Mesa: An Agent-Based Modeling Framework in Python.</em> <a href="https://mesa.readthedocs.io/">mesa.readthedocs.io</a> <a href="#fnref:mesa" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="agent-based-model,"/><category term="game-theory"/><summary type="html"><![CDATA[TL;DR: When faced with a forecasting task, one can either seek information or follow the crowd. The Curty & Marsili game stacks fundamentalists against herders in a binary forecasting task, revealing phase coexistence and ergodicity breaking under certain conditions. We propose a theoretical study of the game's behavior and validate it through ABM simulations.]]></summary></entry><entry><title type="html">Listening to the Market Mode</title><link href="https://gaetanx21.github.io/blog/2025/market-mode/" rel="alternate" type="text/html" title="Listening to the Market Mode"/><published>2025-02-12T00:00:00+00:00</published><updated>2025-02-12T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/market-mode</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/market-mode/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>We first motivate the use of Principal Component Analysis (PCA) on returns to extract the market mode in equities. This mode is crucial for understanding the market risk and comparing it against other risks. We then do a (very) quick recap on Random Matrix Theory (RMT), which provides a theoretical framework for understanding the eigenspectrum of random matrices. Finally, we apply PCA on S&amp;P 500 components to extract the market mode and we monitor its evolution over time, drawing a comparison with the VIX index.</p> <h2 id="motivation">Motivation</h2> <p>Among the various asset classes (e.g., equities, bonds, commodities), equities tend to provide the highest returns in absolute terms (i.e. not adjusted for risk). Equities are exposed to a multitude of risk factors, with <strong>market risk</strong> being the dominant one<sup id="fnref:CAPM"><a href="#fn:CAPM" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. As such, understanding the market risk and how much of the variance it explains is crucial for risk management and portfolio construction.</p> <p>In a nutshell, the question we want to answer is the following: <strong>can we measure the market risk and compare its weight against other risks?</strong></p> <p>Perhaps the simplest approach to gauge market risk is to look at ready-made proxies such as the <strong>VIX index</strong><sup id="fnref:VIX"><a href="#fn:VIX" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. However, the method of computing the VIX is debatable and may not capture the market risk accurately. A more data-driven approach is to extract the market mode from market returns using PCA, as explained in the next section.</p> <h2 id="pca-on-returns--statistical-factor-model">PCA on Returns = Statistical Factor Model</h2> <p>In a nutshell, a <strong>factor model</strong> describes the variance observed in a set of correlated variables (in our case, stock returns) using a smaller number of <strong>unobserved factors</strong>, which we hope to be more or less independent &amp; more or less interpretable. The idea is to decompose the observed variables $X_i$ as linear combinations of the factors $F_k$ plus some idiosyncratic noise $\varepsilon_i$:</p> \[X_i = \sum_k \beta_k^{(i)} F_k + \varepsilon_i\] <p>where $\beta_k^{(i)}$ is the (factor) loading of the $i$-th asset on the $k$-th factor.</p> <p>Now, the hard part is to find <strong>good factors</strong>. One approach is to simply purchase them from vendors like MSCI (Barra models) who gather a lot of data and knowledge to build these factors. Another (cheaper &amp; more transparent) approach is to extract them via PCA. In this case, the factors are obtained as the eigenvectors of the correlation matrix of returns. This is nice for several reasons:</p> <ul> <li>eigenvectors are orthogonal<sup id="fnref:spectral"><a href="#fn:spectral" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, meaning our factors are uncorrelated;</li> <li>eigenvalues directly give us the amount of variance explained by each factor;</li> <li>the factors are interpretable as they are linear combinations of the original variables.</li> </ul> <p>Note that since the factors are obtained in a purely data-driven fashion (without any human/economic prior), we call this approach a <strong>statistical</strong> factor model, as opposed to classical factor models like the CAPM or the Fama-French Three-Factor Model.</p> <p><em>Before trying out PCA on real data, let’s quickly brush up on Random Matrix Theory (RMT), which provides a neat theoretical framework for understanding the eigenspectrum of correlation matrices.</em></p> <h2 id="brush-up-on-random-matrix-theory-rmt">Brush Up on Random Matrix Theory (RMT)</h2> <p>Consider a $T \times N$ matrix $X$ filled with i.i.d. Gaussian entries $X_{ij} \sim \N(0,\sigma^2)$. Typically, $X$ is called the <strong>design matrix</strong> and each one of the $T$ rows corresponds to one observation of the $N$ variables of interest. In our case, the variables are the daily close-to-close returns of the $N=500$ stocks in the S&amp;P 500 index.</p> <p>If we want to study the correlation between the variables, we first compute a standardized version of the design matrix $\tilde{X}$ by subtracting the mean and dividing by the standard deviation for each column. The sample correlation matrix is then given by $C = \frac{1}{T} \tilde{X}^T \tilde{X}$.</p> <p>Finally, $C$ is real symmetric so we know from the spectral theorem that it can be diagonalized in an orthonormal basis of eigenvectors. In fact $C$ is also positive semi-definite, so all its eigenvalues are non-negative.</p> <h4 id="marchenko-pastur-theorem">Marchenko-Pastur Theorem</h4> <p>RMT studies the behavior of the eigenvalues of $C$ when $T,N \to \infty$ with $Q = T/N$ fixed. The main result is the <strong>Marchenko-Pastur (MP) theorem</strong>:</p> \[L_N(\lambda) = \frac{1}{N} \sum_{i=1}^N \delta(\lambda - \lambda_i) \xrightarrow[N,T \to \infty]{\mathcal{W}} \mathbb{P}_\text{MP}(\lambda) = \frac{Q}{2\pi \sigma^2} \frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda} \mathbb{1}_{[\lambda_-, \lambda_+]}(\lambda)\] <p>where $\lambda_{\pm} = \sigma^2(1\pm\sqrt{\frac{1}{Q}})^2$ are the limiting bounds of the spectrum.</p> <p>The MP theorem tells us that the empirical spectral density of the correlation matrix $L _ N(\lambda)$ converges (weakly in distribution) toward the MP distribution $\mathbb{P} _ \text{MP}$ as $T,N \to \infty$.</p> <p>This is quite remarkable: we could have expected the eigenvalues to be unbounded as $T,N \to \infty$, but RMT tells us that they are actually bounded and gives us the exact form of the limiting distribution.</p> <p>In fact, we can relax some hypotheses and the MP theorem will still hold, though convergence may be (significantly) slower. For example, the entries of $X$ don’t have to be Gaussian. This is important in our case because we know that Gaussianity is a strong assumption for financial data. In practice returns have fat tails and are often skewed. A Student-distribution is already a much better model for returns. <em>Good news, MP still holds for Student-distributed entries!</em></p> <h4 id="link-with-pca">Link with PCA</h4> <p>Now, what does this have to do with PCA? Well, the MP theorem tells us that the eigenvalues of the correlation matrix are bounded and distributed according to a known law. This is useful because it allows us to detect the presence of <strong>signal</strong> in the data. If the eigenvalues are significantly larger than the MP bounds, then we can say that the data contains some structure that is not due to randomness. On the contrary, eigenvalues inside the “noise band” defined by the MP law are considered to be due to randomness. Thus, <strong>we can use the MP theorem to filter out noise and extract only the significant factors from the data</strong>. In particular, when doing PCA on market returns, one eigenvalue will stand out from all the others: it represents the dominant variance component due to the market, and as such we call it the <strong>market mode</strong>. It is approximately equally distributed between all the stocks and is the most important factor in the data.</p> <p>In this last section, we illustrate the above ideas through an experiment on US equities. Specifically, we compute rolling PCAs on the correlation matrix of S&amp;P 500 components and analyze the behavior of the market mode over time.</p> <h2 id="experiment-on-us-equities">Experiment on US Equities</h2> <p>To run our experiment, we first need some data. We chose US equities because they are the most liquid and it’s easy to get clean data. We fetched the daily close-to-close returns of the S&amp;P 500 components from Yahoo Finance using the <code class="language-plaintext highlighter-rouge">yfinance</code> Python package. We considered the period 2000–2025<sup id="fnref:sp500"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <h4 id="pca-on-2020-2024-returns">PCA on 2020-2024 returns</h4> <p>Let’s begin by computing the correlation matrix of the daily returns of the S&amp;P 500 components for the period 2020–2024. We then compute the eigenvalues of the correlation matrix and plot them against the MP bounds. The results are shown in <a href="#fig-1">Figure 1</a>. Importantly, <u>the largest eigenvalues are outside the plot</u> for better visibility. There is only ~10 of them, but they are much larger than the rest. <strong>In particular, the first principal component stands out from all the others: it represents the market mode.</strong> The other significant eigenvalues are due to sectoral correlations, which are also interesting to study but outside the scope of this post.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_true-480.webp 480w,/assets/img/posts/market-mode/mp_true-800.webp 800w,/assets/img/posts/market-mode/mp_true-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_true.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024. The largest eigenvalues are outside the plot. </div> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/dist_eigvec_mode-480.webp 480w,/assets/img/posts/market-mode/dist_eigvec_mode-800.webp 800w,/assets/img/posts/market-mode/dist_eigvec_mode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/dist_eigvec_mode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Distribution of the eigenvector of the market mode as well as a noisy mode for the period 2020–2024. Note how the market mode is approximately equally distributed between all the stocks whereas the noisy mode follows a normal distribution, which makes sense because it has no information and thus must maximize entropy. </div> <p>As an extra step, we can shuffle the returns to destroy the correlation structure and then recompute the eigenvalues. The results are shown in <a href="#fig-3">Figure 3</a>. Notice how all the eigenvalues are now neatly inside the MP bounds, which confirms that the structure in the data is due to correlations and not randomness.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_shuffled-480.webp 480w,/assets/img/posts/market-mode/mp_shuffled-800.webp 800w,/assets/img/posts/market-mode/mp_shuffled-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_shuffled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024 after shuffling the returns. All the eigenvalues are now inside the MP bounds and indeed follow the MP distribution. </div> <h4 id="rolling-pca-on-2000-2024-returns">Rolling PCA on 2000-2024 returns</h4> <p>Now that we’ve seen how PCA works on a single sample of market returns, let’s apply it to a rolling window of the daily returns of the S&amp;P 500 components for a large period of time. The idea is to monitor the evolution of the ratio $\lambda_\text{max} / \sum_i \lambda_i$ over time, where $\lambda_\text{max}$ is the largest eigenvalue (market mode) and $\lambda_i$ are the other eigenvalues. This ratio gives us an idea of how much of the variance is explained by the market mode. Intuitively, we expect this ratio to be high during times of uncertainty / fear / crisis as the market becomes even more important in driving returns. To check this hypothesis, we compare the ratio to the VIX index.</p> <p>We’ll be looking at the period 2000–2024, which includes the 2008 financial crisis and the 2020 COVID-19 pandemic. We will take 6-month rolling windows with a 1-month step size and compute the ratio $\lambda_\text{max} / \sum_i \lambda_i$ for each window. Note that for technical reasons<sup id="fnref:sp500:1"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, we only consider the top 420 companies in the S&amp;P 500 index (ranked by daily trading volume) instead of the full 500. However taking the top 420 companies or top 500 companies doesn’t change the results significantly<sup id="fnref:proof"><a href="#fn:proof" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.</p> <p>The results are shown in <a href="#fig-4">Figure 4</a>. The ratio $\lambda_\text{max} / \sum_i \lambda_i$ is plotted in green and the VIX index is plotted in red. We can see that the two series are quite correlated, which confirms our intuition. In particular, we see that the ratio spikes during the 2008 financial crisis, and the 2020 COVID-19 pandemic. This is a nice result as it shows that PCA can indeed capture the market mode and that it is a good proxy for market risk.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/TOP420-480.webp 480w,/assets/img/posts/market-mode/TOP420-800.webp 800w,/assets/img/posts/market-mode/TOP420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/TOP420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Ratio $\lambda_\text{max} / \sum_i \lambda_i$ (green) and VIX index (red) over the period 2000–2024. The two series are quite correlated, which confirms our intuition that the market mode is a good proxy for market risk. </div> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen how PCA can be used to extract the market mode from equities’ return data. This mode is crucial for understanding the market risk and weighing it against other risks. We’ve also seen how Random Matrix Theory provides a theoretical framework for understanding the eigenspectrum of correlation matrices. Finally, we’ve applied PCA on S&amp;P 500 individual returns to extract the market mode and we’ve analyzed its behavior over time, drawing a comparison with the VIX index.</p> <p>Note that the market mode is not the only factor that matters. If we stick to the PCA approach (statistical factor model), there are several other eigenvalues outside the MP noise band. These eigenvalues and the corresponding eigenvectors correspond to sectors of the US economy (e.g., tech, finance, utilities). They too are risk factors, albeit less important than the market mode. In practice, depending on the context, one may want to consider these factors, for instance to build a sector-neutral portfolio.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:CAPM"> <p>The Capital Asset Pricing Model (CAPM) is the most simple factor model as it relies on the market factor only. In a nutshell, it posits that the expected return of an asset is linearly related to the expected return of the market depending on the asset’s correlation with the market, known as the beta coefficient. <a href="#fnref:CAPM" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:VIX"> <p>The VIX index is a measure of the market’s expectation of volatility over the next 30 days. It is calculated using the implied volatility of S&amp;P 500 options and is often referred to as the “fear gauge” as it tends to spike during market downturns. <a href="#fnref:VIX" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectral"> <p>The eigenvectors of a real symmetric matrix are orthogonal. This is a consequence of the spectral theorem, which states that a real symmetric matrix can be diagonalized by an orthonormal basis of eigenvectors. <a href="#fnref:spectral" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sp500"> <p>Note that the composition of the S&amp;P 500 index changes over time as companies are added or removed. We use the current components of the index for each year. Sometimes too many components change and this causes problems. One simple solution is to only consider the top 420 companies (instead of 500), which are more stable. (NB: we rank the companies by daily trading volume.) <a href="#fnref:sp500" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:sp500:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:proof"> <p>One way to confirm this intuition is to re-run the experiment but looking at the top 100 companies instead of the top 420. The results are very similar, which shows that the market mode is indeed robust to the number of companies considered (as long as it’s not too small of course). <a href="#fnref:proof" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="random-matrix-theory,"/><category term="linear-algebra,"/><category term="quant-finance"/><summary type="html"><![CDATA[TL;DR: Performing PCA on returns amounts to constructing a statistical factor model. The largest eigenvalue corresponds to the market mode and far outweighs the other factors. Thus, one can perform rolling PCA on equities' returns to monitor the market risk over time.]]></summary></entry><entry><title type="html">Jeffreys’ Prior in Bayesian Inference</title><link href="https://gaetanx21.github.io/blog/2025/jeffreys-prior/" rel="alternate" type="text/html" title="Jeffreys’ Prior in Bayesian Inference"/><published>2025-02-07T00:00:00+00:00</published><updated>2025-02-07T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/jeffreys-prior</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/jeffreys-prior/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}} \newcommand{\mathcalL}{\mathcal{L}}\] <p>We first motivate the need for “objective” priors in Bayesian inference by highlighting the limitations of uniform priors. We then introduce Jeffreys’ prior, which is invariant under reparametrization and provides a principled way to assign priors in Bayesian inference. We prove its invariance under reparametrization and illustrate its use in a coin flip problem. Note that throughout this post we restrict ourselves to the one-dimensional case for simplicity.</p> <h2 id="introduction">Introduction</h2> <p>In Bayesian inference, prior distributions encode our initial beliefs about an unknown parameter $\theta$ before observing data $x$. We can then update these beliefs using Bayes’ theorem to obtain a posterior distribution. Namely: <em>posterior = likelihood x prior</em>, which can be rewritten as $p(\theta | x) \propto p(x | \theta) p(\theta)$.</p> <p>Choosing priors usually involves a trade-off between incorporating prior knowledge and maintaining objectivity. Depending on the context and how much we know about the problem, we might have different beliefs about the parameter, or no beliefs at all. For instance, if we’re doing linear regression on standardized data ($y _ i = \beta^T x _ i + \varepsilon _ i$), we may feel like our prior for $\beta$ should be centered around zero. But if we’re doing a coin flip experiment ($X_i \sim B(\theta)$), we might not have any strong prior beliefs about the bias of the coin. So how do we choose the prior in this case? One naive approach would be to use a flat prior $p(\theta) \sim U([0, 1])$. This prior seems uninformative but it really isn’t. To see why, let’s consider the same coin flip experiment but this time we want to estimate the odds ratio $\phi = \frac{\theta}{1 - \theta}$. We may again naively choose a flat prior $p(\phi) \propto 1$<sup id="fnref:improper"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. But this flat prior on $\phi$ induces a non-flat prior on $\theta$! In fact, since $\phi$ is uniform on $\R_+$<sup id="fnref:improper:1"><a href="#fn:improper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and as such biased towards arbitrarily large values, $\theta = \frac{\phi}{1 + \phi}$ is highly biased towards $1$, as illustrated on <a href="#fig-1">Figure 1</a>. Thus, choosing a flat prior for $\phi$ is not the same as choosing a flat prior for $\theta$! That is why the seemingly objective choice of a flat prior is not always the best choice.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/theta-480.webp 480w,/assets/img/posts/jeffreys_prior/theta-800.webp 800w,/assets/img/posts/jeffreys_prior/theta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/theta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="theta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sketch of the prior distribution on $\theta$ induced by a flat prior on $\phi=\frac{\theta}{1-\theta}$. Clearly, the prior is not flat and is biased towards $\theta=1$. </div> <p>Likewise, choosing a flat prior in high-dimensional spaces assigns way too much mass to unimportant regions of the parameter space, so it is informative, but in a bad way!</p> <p>With this in mind, we see that <strong>uniform priors are no silver bullet</strong>. Ideally, we would like a prior which does not depend on the parameterization of the problem. In other word, <strong>the information we encode in the prior should be invariant under reparametrization</strong>. If we go back to the example of the coin flip, we would like a prior that encodes the same prior information about the bias of the coin, regardless of whether we’re working with $\theta$ or $\phi$. Intuitively, such a prior should be based on the <strong>structure of the data model</strong> itself, rather than the parameterization we choose.</p> <p>Reparametrization invariance is exactly what Jeffreys’ prior achieves, as explained below.</p> <p><em>Note that intuitively, reparametrization invariance is a good heuristic for an “objective” prior.</em></p> <h2 id="definition">Definition</h2> <p>Jeffreys’ prior is defined using the Fisher information matrix. Given a likelihood function $\mathcalL(\theta | x)$ for a parameter $\theta$, the Fisher information is:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right].\] <p>Jeffreys’ prior is then given by:</p> \[\pi_J(\theta) \propto \sqrt{I(\theta)}.\] <p>The key property of Jeffrey’s prior is that it is invariant under reparametrization. In other words, if we try to estimate a different parameter $\phi = g(\theta)$, the Jeffrey’s prior for $\phi$ will be:</p> \[\pi_J(\phi) \propto \sqrt{I(\phi)} = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|\] <p>which is consistent with the transformation rule for probability densities.</p> <p><em>Note that Jeffrey’s prior is defined using the likelihood function. While this is convenient because it allows us to use the structure of the data model, it also goes against the Bayesian principle of choosing the prior independently of the data. This is a philosophical issue in Bayesian statistics, and different practitioners may have different views on this.</em></p> <h2 id="proof-of-invariance-under-reparametrization">Proof of Invariance Under Reparametrization</h2> <p>In this paragraph we demonstrate Jeffreys’ prior invariance under reparametrization. Suppose we have a parameter $\theta$ and a reparametrized parameter $\phi = g(\theta)$. We want to show that Jeffrey’s prior for $\phi$ is consistent with the transformation rule for probability densities.</p> <p>To begin with, note that the chain rule gives:</p> \[I(\phi) = I(\theta) \left( \frac{d\theta}{d\phi} \right)^2.\] <p>Taking the square root, we get:</p> \[\sqrt{I(\phi)} = \sqrt{I(\theta)} \left| \frac{d\theta}{d\phi} \right|\] <p>i.e., Jeffrey’s prior transforms as:</p> \[\pi_J(\phi) = \pi_J(\theta) \left| \frac{d\theta}{d\phi} \right|.\] <p>We recognize the transformation rule for probability densities, which demonstrates that Jeffrey’s prior correctly transforms to maintain consistency, proving its invariance by reparametrization.</p> <h2 id="coin-flip-example">Coin flip example</h2> <p>Let’s compute Jeffreys’ prior for a simple coin flip problem to illustrate its use.</p> <p>Consider a simple example: estimating the bias $\theta$ of a coin, where $X \sim \text{Bin}(n, \theta)$. The likelihood function is:</p> \[\mathcal L(\theta | x) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i}.\] <p>We compute the Fisher information:</p> \[I(\theta) = \E \left[ \left( \frac{\partial}{\partial \theta} \log \mathcalL(\theta | x) \right)^2 \bigg| \theta \right] = \frac{n}{\theta (1 - \theta)}.\] <p>Thus, Jeffreys’ prior for $\theta$ is:</p> \[\pi_J(\theta) \propto \sqrt{\frac{n}{\theta (1 - \theta)}} \propto \frac{1}{\sqrt{\theta (1 - \theta)}}.\] <p>We recognize the <strong>Beta(1/2, 1/2)</strong> distribution, which is commonly used as an uninformative prior for bounded parameters. This is a nice result, as it shows that Jeffreys’ prior is consistent with our intuition of an uninformative prior in this case.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/jeffreys_prior/beta-480.webp 480w,/assets/img/posts/jeffreys_prior/beta-800.webp 800w,/assets/img/posts/jeffreys_prior/beta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/jeffreys_prior/beta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="beta" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Jeffreys' prior for the bias of a coin flip experiment is the Beta(1/2, 1/2) distribution. </div> <h2 id="conclusion">Conclusion</h2> <p>Jeffreys’ prior provides a principled way to assign priors in Bayesian inference, ensuring invariance under reparametrization. We proved its reparametrization invariance and illustrated its use in a coin flip problem. Jeffreys’ prior is useful when no clear subjective prior information is available, for instance in astrophysics. We’ve limited ourselves to the one-dimensional case for simplicity, but Jeffreys’ prior can be extended to higher dimensions naturally by considering the Fisher information matrix and its determinant, such that $\pi_J(\theta) \propto \sqrt{\text{det}(I(\theta))}$. Finally, I want to stress that Jeffreys’ prior violates the Bayesian principle of choosing the prior independently of the data, which may be a concern for some practitioners.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:improper"> <p>The prior $p(\phi) \propto 1$ is called an <em>improper</em> prior since it doesn’t integrate to 1. This is a common pitfall when using flat priors. However using unnormalized priors is okay as long as we normalize the posterior distribution. <a href="#fnref:improper" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:improper:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="bayesian-ml"/><summary type="html"><![CDATA[TL;DR: Bayesian inference requires us to specify a prior distribution. When we're unsure what prior to pick and want to stay as objective as possible, one option is to use Jeffreys' prior, which leverages the Fisher information to provide a reparametrization-invariant prior.]]></summary></entry><entry><title type="html">Regression Dilution</title><link href="https://gaetanx21.github.io/blog/2025/regression-dilution/" rel="alternate" type="text/html" title="Regression Dilution"/><published>2025-02-01T00:00:00+00:00</published><updated>2025-02-01T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2025/regression-dilution</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2025/regression-dilution/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\text{Var}} \newcommand{\Cov}{\text{Cov}} \newcommand{\R}{\mathbb{R}}\] <p>We first introduce the concept of <strong>attenuation bias</strong> in linear regression due to measurement error in the covariates. We derive the shrinkage effect in the one-dimensional case and extend it to the multivariate case. We do simulations to visualize the shrinkage effect as the signal-to-noise ratio (SNR) goes to zero.</p> <h2 id="introduction">Introduction</h2> <p>In classical linear regression, we assume a model of the form:</p> \[y = X\beta + \varepsilon\] <p>where $X$ is an $n \times p$ matrix of covariates $x_i \in \R^p$, $\beta$ is a $p \times 1$ vector of coefficients, and $\varepsilon$ is noise. <strong>Weak exogeneity</strong> is a key assumption in linear regression, which states that the covariates $X$ are fixed and non-random. In other words, the covariates are assumed to be measured without error. However, in many real-world scenarios, this hypothesis is violated: covariates themselves contain measurement noise:</p> \[\tilde{X} = X + U\] <p>where $U$ is an $n \times p$ matrix of noise $u_i \in \R^p$. This additional noise leads to a phenomenon known as <strong>attenuation bias</strong>, where the estimated coefficients shrink towards zero. Let’s first derive this effect in the one-dimensional case.</p> <p>Note that in what follows we make the following the classical assumptions:</p> <ul> <li>$x_i$ i.i.d. centered with variance $\sigma_x^2$ (or covariance $\Sigma_x$ in the multivariate case),</li> <li>$u_i$ i.i.d. centered with variance $\sigma_u^2$ (or covariance $\Sigma_u$ in the multivariate case),</li> <li>$\varepsilon_i$ i.i.d. centered with variance $\sigma_\varepsilon^2$,</li> <li>$x_i, u_i, \varepsilon_i$ are independent of each other</li> </ul> <h2 id="one-dimensional-case">One-dimensional case</h2> <p>Let’s first derive the attenuation bias in the one-dimensional case.</p> <p>Consider the simple case of a one-dimensional linear regression model:</p> \[y = \beta x + \varepsilon.\] <p>Now assume that we observe a noisy version of $x$: $\tilde{x} = x + u$, where $u$ is the noise term. The least squares estimator of $\beta$ using the noisy covariate $\tilde{x}$ is:</p> \[\hat{\beta} = \frac{\Cov(\tilde{x}, y)}{\Var(\tilde{x})} = \frac{\Cov(x + u, \beta x + \varepsilon)}{\Var(x + u)} = \frac{\beta \Var(x)}{\Var(x) + \Var(u)} = \frac{\beta \sigma_x^2}{\sigma_x^2 + \sigma_u^2} = \lambda \beta\] <p>where $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}&lt;1$ is the attenuation factor or shrinkage factor.</p> <p>Thus the estimated coefficient $\hat{\beta}$ is a scaled version of the true coefficient $\beta$, with the scaling factor $\lambda$ being less than 1. This implies that the estimated coefficient is biased towards zero due to the noise in the covariate.</p> <p>In particular, note that when $\sigma_u = 0$, we recover the unbiased estimator $\hat{\beta} = \beta$. Likewise, as $\sigma_u \to \infty$, the estimated coefficient $\hat{\beta} \to 0$ since the SNR goes to zero.</p> <h2 id="multivariate-case">Multivariate case</h2> <p>The multivariate case can be derived similarly, though the algebra is slightly more involved.</p> <p>If we use the noisy covariates $\tilde{X}$ instead of the true covariates $X$, the least squares estimator becomes:</p> \[\hat{\beta} = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T y.\] <p>Substituting $\tilde{X} = X + U$ and $y = X\beta + \varepsilon$ gives:</p> \[\hat{\beta} = [(X + U)^T (X + U)]^{-1} (X + U)^T (X\beta + \varepsilon)\] <p>We rewrite this expression so that the law of large numbers can be applied:</p> \[\hat{\beta} = \bigg[\frac{1}{n}(X^T X + X^T U + U^T X + U^T U)\bigg]^{-1} \bigg[\frac{1}{n}(X^T X\beta + X^T \varepsilon + U^T X\beta + U^T \varepsilon)\bigg]\] <p>Using the weak law of large numbers, we have</p> \[\begin{align*} \frac{1}{n}X^T X &amp;\to \E[x x^T] = \Sigma_x \\ \frac{1}{n}X^T U &amp;\to \E[x u^T] = 0 \\ \frac{1}{n}U^T X &amp;\to \E[u x^T] = 0 \\ \frac{1}{n}U^T U &amp;\to \E[u u^T] = \Sigma_u \end{align*}\] <p>and</p> \[\begin{align*} \frac{1}{n}X^T X\beta &amp;\to \E[x x^T]\beta = \Sigma_x \beta \\ \frac{1}{n}X^T \varepsilon &amp;\to \E[x \varepsilon_i^T] = 0 \\ \frac{1}{n}U^T X\beta &amp;\to \E[u x^T]\beta = 0 \\ \frac{1}{n}U^T \varepsilon &amp;\to \E[ \varepsilon_i^T] = 0 \end{align*}\] <p>where all the convergences are in probability<sup id="fnref:strong"><a href="#fn:strong" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Combining these results and applying Sluskty’s lemma and the continuous mapping theorem, we have:</p> \[\hat{\beta} \xrightarrow[]{\mathbb{P}} (\Sigma_x + \Sigma_u)^{-1} \Sigma_x \beta = \left(I + \Sigma_x^{-1} \Sigma_u \right)^{-1} \beta.\] <p>Note that in the multi-dimensional case, the shrinkage factor is not a scalar but a matrix $\Lambda = (I + \Sigma_x^{-1} \Sigma_u)^{-1}$. In particular, although $\Sigma_x$ and $\Sigma_u$ are positive definite matrices, $\Sigma_x^{-1} \Sigma_u$ is not positive definite in general. Therefore it is more difficult to interpret the shrinkage effect in the multivariate case.</p> <p>For simplicity, if we assume spherical noise on both covariates and response, i.e., $\Sigma_x = \sigma_x^2 I$ and $\Sigma_u = \sigma_u^2 I$, we recover the one-dimensional result with $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$. This makes sense because assuming spherical noise is like running the one-dimensional case independently for each covariate.</p> <p>Additionally, we recover the unbiased estimator $\hat{\beta} = \beta$ when $\Sigma_u = 0$, as expected.</p> <h2 id="visualizing-the-shrinkage-effect-as-snr-goes-to-zero">Visualizing the shrinkage effect as SNR goes to zero</h2> <p>We want to illustrate the gradual shrinkage of the estimated coefficients as the SNR gradually decreases. We stick to the one-dimensional case for simplicity.</p> <p>We simulate a linear regression model with a single covariate $x$ with $\sigma_x = 1$ and noise $u$ with $\sigma_u$ running from $0$ to $5 \sigma_x$. For each value of $\sigma_u$, we fit a linear regression model using the noisy covariate $x + u$ and record the estimated coefficient $\hat{\beta}$.</p> <p>We then plot the empirical shrinkage ratio $\frac{\hat{\beta}}{\beta}$ as a function of the SNR $\frac{\sigma_x}{\sigma_u}$. Additionally, we overlay the theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$.</p> <p>The results are shown in <a href="#fig-1">Figure 1</a>. As the SNR decreases, the estimated coefficients shrink towards zero, as expected. The empirical shrinkage ratio closely follows the theoretical shrinkage factor $\lambda$.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/regression_dilution/snr_shrinkage-480.webp 480w,/assets/img/posts/regression_dilution/snr_shrinkage-800.webp 800w,/assets/img/posts/regression_dilution/snr_shrinkage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/regression_dilution/snr_shrinkage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="snr shrinkage" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical shrinkage ratio as a function of the SNR. The theoretical shrinkage factor $\lambda = \frac{1}{1 + \frac{\sigma_u^2}{\sigma_x^2}}$ is overlaid. </div> <h2 id="conclusion">Conclusion</h2> <p>When covariates are measured with noise, the estimated regression coefficients shrink towards zero, leading to bias. This is important in fields where measurement errors are common, such as economics and epidemiology. One way to mitigate this bias is to use <strong>error-in-variables models</strong>, which explicitly model the noise in the covariates. The simplest such model is probably Deming regression, which models a one-dimensional linear regression and assumes the SNR to be known. $\hat{\beta}$ is then found by minimizing a <em>weighted</em> sum of squared residual to account for the noise in $x$.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:strong"> <p>The strong law of large numbers would require additional assumptions on the moments of the random variables involved. <a href="#fnref:strong" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="robust-ml"/><summary type="html"><![CDATA[TL;DR: When covariates in linear regression are subject to noise, the estimated regression coefficients shrink towards zero. We derive this effect mathematically and illustrate it with simulations.]]></summary></entry><entry><title type="html">A geodesic from cat to dog</title><link href="https://gaetanx21.github.io/blog/2024/ot-geodesic/" rel="alternate" type="text/html" title="A geodesic from cat to dog"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-geodesic</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-geodesic/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete entropy-regularized Kantorovich problem and show how it can be solved efficiently using the Sinkhorn algorithm. We then illustrate the usefulness of the Sinkhorn algorithm to compute Wasserstein distances, barycenters, and geodesics between probability distributions. We finally apply this to interpolate between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the 2-Wasserstein metric.</p> <h1 id="discrete-entropy-regularized-kantorovich-problem">Discrete Entropy-Regularized Kantorovich problem</h1> <p>The discrete entropy-regularized Kantorovich problem formulates as: \(\begin{equation} \label{eq:Kreg} \tag{$\tn{K}^\tn{reg}$} P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \langle C,P\rangle - \epsilon H(P) \end{equation}\) where $H(P)=\sum _ {i=1}^nP_{ij}(\log P _ {ij}-1)$ is the discrete entropy. Note that when $\epsilon=0$ one recovers classical discrete OT. Crucially, (\ref{eq:Kreg}) is strictly convex as soon as $\epsilon&gt;0$ and thus has a unique solution $P^{\epsilon,\star}$.</p> <p>Additionally, One can easily show that $\langle C,P \rangle - \epsilon H(P)= \tn{KL} (P|K)$, where $K=\exp(-\frac{C}{\epsilon})$ is called a Gibbs kernel. Thus (\ref{eq:Kreg}) can be seen as a projection problem w.r.t. to the KL divergence: (\ref{eq:Kreg}) rewrites as $P^{\epsilon,\star}= \arg \min _ {P\in U(\alpha,\beta)} \tn{KL}(P|K)$ i.e. $P^{\epsilon,\star}=\tn{Proj} _ {U(\alpha,\beta)}^\tn{KL}(K)$.</p> <p>The whole point of introducing the entropy is to relax the Kantorovich problem into a strictly convex problem which can be solved efficiently using the Sinkhorn algorithm, which we now introduce.</p> <h2 id="sinkhorns-algorithm">Sinkhorn’s algorithm</h2> <p>The most well-known method to solve (\ref{eq:Kreg}) is Sinkhorn’s algorithm, which uses the fact that $P^{\epsilon,\star}$ necessarily has the form $P^{\epsilon,\star}=\tn{Diag}(u)K\tn{Diag}(v)$ where $K$ is the Gibbs kernel. The conditions $P\mathbb{1} _ m=a$ and $P^T\mathbb{1} _ n=b$ thus rewrite as $u * (Kv) = a$ and $v * (K^Tu) = b$ respectively, where $*$ denotes the Hadamard product. One can thus iteratively solve these two equations until $u$ and $v$ converge, yielding the Sinkhorn algorithm:</p> \[\begin{align*} u^{l+1}&amp;\leftarrow\frac{a}{Kv^l}\\ v^{l+1}&amp;\leftarrow\frac{b}{K^Tu^{l+1}} \end{align*}\] <p>where we use the initialization $v^0=\mathbb{I} _ m$.</p> <p>In practice, Sinkhorn’s algorithm allows us to compute Wasserstein distances efficiently. In turn, we can use these distances to compute barycenters and geodesics between probability distributions.</p> <h2 id="wasserstein-barycenters-and-geodesics-on-probability-spaces">Wasserstein barycenters and geodesics on probability spaces</h2> <p>Using OT, one can define <em>distances</em> between probability distributions defined on the same space $X$. The most common is the $p$-Wasserstein distance $W_p$ defined for any real number $p&gt;0$: \(\begin{equation} \label{eq:Wp} \tag{$W_p$} W_p(\alpha,\beta)=\min_{P\in U(\alpha,\beta)} \langle P,C^p \rangle^{1/p} = \bigg(\sum_{1\leq i,j\leq n} d(x_i,y_j)^p P_{ij}\bigg)^{1/p} \end{equation}\) where $d$ is a distance on $X$. For instance if $X=\R^d$ one can use $d(x,y)=||x-y||$.</p> <p>Now that we have a distance on probability measures, we can use it to compute barycenters. For a fixed $p&gt;1$ and $R$ probability distributions $\alpha_1,\dots,\alpha_R \in \tn{P}(X)$, their $p$-Wasserstein barycenter with coefficients $(\lambda_r)_r$ is defined as: \(\begin{equation} \label{eq:barycenter} \tag{B} \beta = \arg \min_{\beta \in \tn{P}(X)} \sum_{r=1}^{R} \lambda_k W_p(\alpha_r,\beta) \end{equation}\)</p> <p>$p$-Wasserstein barycenters can in particular be used to compute geodesics for the $p$-Wasserstein metric of the form $t\in[0,1]\mapsto\mu_t\in\tn{P}(X)$ from $\alpha$ to $\beta$ as: \(\begin{equation} \mu_t = \arg \min_{\mu\in\tn{P}(X)} (1-t)W_p^p(\alpha,\mu_t) + tW_p^p(\beta,\mu_t) \end{equation}\) When $p=2$, we in fact have $\mu_t=\sum_{1\leq i,j\leq n}P_{ij}^\star \delta_{(1-t)x_i+ty_j}$ using the notations from my <a href="/blog/2024/ot-assignement-problem/">previous post</a>.</p> <h2 id="a-geodesic-from-cat-to-dog">A geodesic from cat to dog</h2> <p>Wasserstein barycenters can be used to interpolate between (grayscale) images using the following formalism: a grayscale image of dimension $N\times N$ can be seen as a distribution of “light” $\alpha\in \tn{P}(\R^{N\times N})$. Then, one can go from an image of a cat $\alpha$ to that of a dog $\beta$ using OT. This has little value in itself, but one can also consider the geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ from $\alpha$ to $\beta$ and thus see the gradual fade from the cat image to the dog image.</p> <p>For $0\leq i \leq 8$ we consider the barycenter coefficients $\lambda=(1-t_i,t_i)$ where $t_i=\frac{i}{8}$ and we plot the 9 corresponding 2-Wasserstein barycenters $b^i\in\tn{P}(\R^{N\times N})$ which intuitively interpolate between the cat and the dog. The pictures were found online, turned to grayscale, resized to $N\times N$ with $N=128$, smooth with a Gaussian kernel, and then each Wasserstein barycenter is computed using the <code class="language-plaintext highlighter-rouge">ot</code> Python library. The results are presented in <a href="#fig-1">Figure 1</a> and quite satisfying for such a simple approach!</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_geodesic/cat2dog-480.webp 480w,/assets/img/posts/ot_geodesic/cat2dog-800.webp 800w,/assets/img/posts/ot_geodesic/cat2dog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_geodesic/cat2dog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cat2dog" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Geodesic $\mu^{\tn{cat}\rightarrow \tn{dog}}$ in $\R^{N\times N}$ computed using 2-Wasserstein barycenters. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that the entropy-regularized Kantorovich problem can be solved efficiently using the Sinkhorn algorithm. This allows us to efficiently compute Wasserstein distances, barycenters, and geodesics between probability distributions. We have illustrated this by interpolating between grayscale images of a cat and a dog, effectively computing a geodesic in the space of grayscale $N\times N$ images for the $W_2$ metric.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: Entropic regularization relaxes the Kantorovitch problem into a strictly convex problem which can be solved efficiently with the Sinkhorn algorithm. We can use this to efficiently compute Wasserstein distances, barycenters, and finally geodesics between distributions.]]></summary></entry><entry><title type="html">Solving the assignement problem using Optimal Transport</title><link href="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/" rel="alternate" type="text/html" title="Solving the assignement problem using Optimal Transport"/><published>2024-11-15T00:00:00+00:00</published><updated>2024-11-15T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/ot-assignement-problem</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/ot-assignement-problem/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce the discrete Kantorovich problem and show that in the uniform case it amounts to solving the permutation problem. We then illustrate this with a student internship assignement problem. We run Monte Carlo simulations for different cost functions and show that the choice of cost function crucially impacts the optimal assignement.</p> <h2 id="the-discrete-kantorovich-problem">The discrete Kantorovich Problem</h2> <p>Let $X, Y$ be two measurable spaces (for simplicity, $X=Y=\R^d$). Consider two discrete distributions (i.e. weighted point clouds) $\alpha\in \tn{P}(X), \ \beta\in\tn{P}(Y)$ given by \(\begin{equation} \label{eq:def} \alpha = \sum_{i=1}^n a_i \delta_{x_i}, \quad \beta = \sum_{j=1}^m b_j \delta_{y_j}, \end{equation}\) and a cost function $c:X\times Y \rightarrow \R^+$.</p> <p>The discrete Kantorovich problem then formulates as: \(\begin{equation} \label{eq:K} \tag{K} P^\star = \arg \min_{P\in U(\alpha,\beta)} \langle C,P\rangle \end{equation}\) where $C=\big(c(x_i,y_j)\big)_{i,j} \in \R^{n\times m}$ and $U(\alpha,\beta)=\lbrace P\in \R^{n\times m} | P\geq 0, P\mathbb{1}_m=a, P^T \mathbb{1}_n=b \rbrace$.</p> <p>Notice that $P\mapsto \langle C,P \rangle$ is a convex functional and $U(\alpha,\beta)$ is a convex subset of $\R^{n\times m}$, such that (\ref{eq:K}) is a convex problem.</p> <p>Even better, it is a linear programming (LP) problem since $P\mapsto \langle C,P \rangle$ is linear and $U(\alpha,\beta)$ encodes linear constraints.</p> <p>Thus, in the discrete case, Optimal Transport (OT) can be seen as an LP problem, and thus solved with off-the-shelf LP solvers such as the <code class="language-plaintext highlighter-rouge">cvxpy</code> Python library.</p> <h2 id="the-uniform-case">The Uniform Case</h2> <p>Let’s consider the uniform case i.e. $n=m$ and $a_i=b_j=\frac{1}{n} \ \forall i,j$.</p> <p>In that scenario, one can show that there exists at least one OT coupling $P^\star$ which is a permutation matrix. This comes from the fact that the extremal points of the polytope $U(1,1)$ are permutation matrices.</p> <p>Thus, in the uniform case there exists a permutation $\sigma^\star \in S_n$ such that $P^\star=P _ {\sigma^\star}=\big( \mathbb{1} _ {\sigma^\star(i)=j} \big) _ {i,j}$. In particular, $\sigma^\star$ solves the permutation problem \(\begin{equation} \label{eq:permutation-problem} \tag{PP} \sigma^\star = \arg \min_{\sigma\in S_n} \sum_{i=1}^n C_{i,\sigma(j)} \end{equation}\)</p> <h2 id="student-internship-assignment">Student Internship Assignment</h2> <p>To illustrate the method described, let’s apply the uniform case, which solves the permutation problem, to assign $n$ students $x_i$ to $n$ internships $y_j$ in a <em>optimal</em> manner.</p> <p>Let’s consider that each student $x_i$ expresses their preference through a ranking $\sigma_i$ of the internships where $\sigma_i(j)$ is the ranking of internship $y_j$ according to student $x_i$ (i.e. $\sigma_i(j)=1$ for $x_i$’s dream internship and $\sigma_i(j)=n$ for $x_i$’s least desired internship).</p> <p>There are many possible choices for the cost function $c$, but it must clearly be an increasing function of $\sigma_i(j)$. The most natural is probably $c(x_i,y_j)=\sigma_i(j)$ i.e. a linear penalization of the integer distance between the student’s favorite ($c=1$) and least wanted internship ($c=n$). However, the optimal assignment $P^\star=P_{\sigma^\star}$ depends crucially on the choice of $c$! Intuitively, rapidly increasing function e.g. quadratic cost $c(x_i,y_j)=\sigma_i(j)^2$ will prevent any student from being attributed an internship deemed too undesirable. This means no student will get an awful internship, the hidden cost being that presumably fewer student will get their first wish. On the contrary, a slowly increasing function e.g. log cost $c(x_i,y_j)=\log\sigma_i(j)$ will only slightly penalize poor internship attributions, and thus we except to see lots of students get their first wish alongside a handful of students getting very low-ranked internships.</p> <p>We test those intuitions by running Monte Carlo simulations for each of the aforementioned cost functions (linear, quadratic, log). More precisely, for a given cost function $c$, we run $M$ simulations, each with $n$ students. Each simulation returns a integer array <code class="language-plaintext highlighter-rouge">ranks</code> of length $n$ where <code class="language-plaintext highlighter-rouge">ranks[i]</code> is student i’s ranking of the internship they were attributed. For each cost function $c$, We concatenate the $M$ <code class="language-plaintext highlighter-rouge">ranks</code> arrays and then plot a histogram of their distribution.</p> <p>The results are presented in <a href="#fig-assignment">Figure 1</a> and confirm our intuition, although there is no difference between linear and quadratic cost. We used $n=20$ students and ran $M=100$ iterations for each cost function $c$.</p> <div class="row justify-content-center" id="fig-assignment"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/ot_permutation_problem/ranks-480.webp 480w,/assets/img/posts/ot_permutation_problem/ranks-800.webp 800w,/assets/img/posts/ot_permutation_problem/ranks-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/ot_permutation_problem/ranks.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ranks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Empirical distribution of students’ ranking of their obtained internship for different cost functions $c$. The log penalty increases slowly such that it’s tolerable to highly disappoint a handful of students if that can help the majority obtain their first wish. This is not the case for the linear and quadratic penalties, which penalize highly the worst attributions. </div> <h2 id="conclusion">Conclusion</h2> <p>We have shown that discrete OT amounts to a LP problem. However, LP problems do not scale well. This motivates the introduction of entropic regularization, which makes (\ref{eq:K}) much easier and faster to solve when $n$ becomes too large for a LP approach. We will discuss this in a future post.</p>]]></content><author><name></name></author><category term="optimal-transport"/><summary type="html"><![CDATA[TL;DR: The discrete Kantorovich problem amounts to a LP problem. In the uniform case, the solution is a permutation matrix which in fact solves the assignement problem.]]></summary></entry><entry><title type="html">Intuitions behind Benford’s Law</title><link href="https://gaetanx21.github.io/blog/2024/benford-law/" rel="alternate" type="text/html" title="Intuitions behind Benford’s Law"/><published>2024-09-20T00:00:00+00:00</published><updated>2024-09-20T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/benford-law</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/benford-law/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first present a quick overview of Benford’s Law. We then provide three different intuitions behind this phenomenon and illustrate them with simulations.</p> <h2 id="introduction-to-benfords-law">Introduction to Benford’s Law</h2> <p>Benford’s Law states that the distribution of the first digit of many real-world datasets is not uniform, but instead verifies $\mathbb{P}(d) \simeq \log_{10}(1 + \frac{1}{d})$ for $d \in \lbrace 1, \ldots, 9\rbrace$. <a href="#fig-1">Figure 1</a> plots the theoretical distribution of the first digit according to Benford’s Law, alongside the uniform distribution for comparison.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/naive_vs_benford-480.webp 480w,/assets/img/posts/benford_law/naive_vs_benford-800.webp 800w,/assets/img/posts/benford_law/naive_vs_benford-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/naive_vs_benford.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="naive vs benford" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Benford's Law alongside uniform distribution. </div> <h4 id="history">History</h4> <p>Benford’s law was actually discovered in the 19th century by Simon Newcomb, who noticed that the first pages of logarithm tables were more worn out than the last ones. Some 50 years later, Frank Benford published a paper where he advanced explanations of this anomaly, which is why the law is now named after him.</p> <h4 id="examples">Examples</h4> <p>Many real-world datasets follow Benford’s Law, such as the populations of countries, the lengths of rivers, stock prices, the numbers in tax returns, etc. Note however that not all datasets follow Benford’s Law. In particular, datasets that do not span several orders of magnitude are unlikely to follow it, for reasons that will become clear in the following sections.</p> <h4 id="applications">Applications</h4> <p>Benford’s law can be used to detect made-up data, often generated by humans, since they tend to distribute the first digits uniformly. In particular, it can be used to detect fraud in accounting, elections, academic papers. For instance, the macroeconomic data the Greek government provided to the European Union before entering the Eurozone did not follow Benford’s Law (though we found out years later only…)</p> <h4 id="literature">Literature</h4> <p>The literature on Benford’s law is somewhat scattered and rife with pseudo-explanations. The phenomenon is still not fully understood, and I do not pretend to provide a definitive answer here. Instead, my goal is to provide three different intuitions behind Benford’s Law, which I will try to explain in simple terms and illustrate with simulations.</p> <h2 id="first-intuition-geometric-growth">First intuition: geometric growth</h2> <p>Perhaps the most intuitive explanation for Benford’s Law is that many real-world variables grow geometrically. For instance, the population of a country grows at a certain rate each year, the stock price of a company goes up or down a percent of so each day, etc. When you think of it, any variable that spans several orders of magnitude should be suspected to grow more or less geometrically!</p> <p>Let’s first consider a variable $X_t$ that grows geometrically at some unknown constant rate $r&gt;0$, starting at $X_0=1$. Intuitively, we feel that going from 1 to 2 (a +100% increase) will take more time than going from 9 to 10 (a +11% increase). And once we get to 10, we feel that going from 10 to 20 (a +100% increase) will take more time than going from 90 to 100 (a +11% increase). And so on. In fact, we can formalize this intuition by writing $X_t = (1+r)^t$, such that going from $d\cdot 10^n$ to $(d+1)\cdot 10^n$ takes time $\Delta t = \frac{1}{1+r} \log _ {10} \left(\frac{d+1}{d}\right) \propto \log _ {10}\left(1 + \frac{1}{d}\right)$. We thus recover Benford’s Law in the constant geometric growth setting.</p> <p>What if we relax our assumptions and add some noise to the growth rate? Intuitively we feel like we should still observe Benford’s Law, perhaps with some deviations due to the noise.</p> <p>To confirm this hypothesis, let’s consider the process $\frac{dX_t}{X_t}=\mu dt + \sigma dW_t$, where $\mu$ is the drift, $\sigma$ is the volatility, and $dW_t$ is a Brownian motion. It is one (very simple) way of modeling stock prices. The math is a bit trickier to deal with, so instead of an analytical solution we’ll simulate the process and plot the distribution of the first digit of $X_t$ at different times $t$. <a href="#fig-2">Figure 2</a> shows that the distribution of the first digit of $X_t$ indeed matches Benford’s Law. The result is robust to the choice of parameters, with better results as we increase the number of simulations $N$ and the time horizon $T$.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/geometric_growth-480.webp 480w,/assets/img/posts/benford_law/geometric_growth-800.webp 800w,/assets/img/posts/benford_law/geometric_growth-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/geometric_growth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="geometric growth" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Benford Law vs. empirical distribution of the first digit of $X_t$. We used parameters $N=1,000,000, T=1,000, \mu=0.1$, $\sigma=0.2$, and $X_0=1$. </div> <h2 id="second-intuition-clt-on-the-logarithm">Second intuition: CLT on the logarithm</h2> <p>Another closely related intuition comes from the Central Limit Theorem (CLT). The core hypothesis is that variables that follow Benford’s law really are <em>products</em> of many factors, which are more or less independent. For instance, the price of a brick of butter is the product of its length, width, height, and the price of the milk.</p> <p>Let’s thus consider a positive random variable $X$ which is made up of $P$ underlying factors i.e. $X = \prod_{i=1}^P X_i$ where we assume the $X_i$ to be i.i.d. positive random variables. We have $\log_{10}(X) = \sum_{i=1}^P \log_{10}(X_i)$. By the CLT, $\log_{10}(X)$ should be approximately normally distributed, with variance scaling linearly with $P$. Thus as P grows to infinity, so does the variance of the normal distribution that models $\log_{10}(X)$.</p> <p>Let’s now look at the random variable $\lbrace \log_{10}(X) \rbrace$ where $\lbrace \cdot \rbrace$ denotes the fractional part. Using the result $\lbrace \sigma Z \rbrace \xrightarrow[\sigma^2\xrightarrow\infty]{d} U([0,1])$ where $Z$ is normally distributed, we have $\mathcal{L}(\lbrace \log_{10}(X) \rbrace) \simeq U([0,1])$.</p> <p>Finally, note that we can rewrite $X$ as $10^{\lbrace \log_{10}(X) \rbrace} \times 10^{\lfloor \log_{10}(X) \rfloor} = \tn{significand} \times \tn{order of magnitude}$. The probability $p_d$ of $X$ having first digit $d$ is then given by \(\begin{align*} p_d &amp;= \mathbb{P}(d \leq \tn{significand} &lt; d+1) \ &amp;= \mathbb{P}(\log_{10}(d) \leq \lbrace \log_{10}(X) \rbrace &lt; \log_{10}(d+1)) \ &amp;= \log_{10}(d+1) - \log_{10}(d) = \log_{10}(1 + \frac{1}{d}) \\). We thus recover Benford’s Law.</p> <p>On <a href="#fig-3">Figure 3</a>, we simulate $X$ as the product of $P=3$ i.i.d. random variables $X_i\sim U([1,10])$. Again, we observe that the distribution of the first digit of $X$ matches Benford’s Law. Note that this result is robust to the choice of the distribution of the $X_i$ and the number of factors $P&gt;1$, with better results as we increase $P$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/clt-480.webp 480w,/assets/img/posts/benford_law/clt-800.webp 800w,/assets/img/posts/benford_law/clt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/clt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="clt" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, P=3$, and $X_i\sim U([1,10])$. </div> <h2 id="third-intuition-scale-invariance">Third intuition: scale invariance</h2> <p>This last intuition is a bit different from the two previous ones. The goal here is to <em>think like a physician</em> (!). The key idea is that Benford’s law applies to variables which have a <em>dimension</em>, or to say it plainly, Benford’s law applies to numbers that need a unit after them (e.g. meters, euros, liters, etc.) If we look at countries’ area for instance, we can measure it in square kilometers or square miles and we’ll still observe Benford’s law. Likewise, stock prices in EUR, USD and JPY all display Benford’s law. And that makes sense right? Since units are arbitrary conventions, we don’t expect Benford’s law to fade away when we change them.</p> <p>Okay but how to turn this insight into a mathematical argument? The answer is <em>scale invariance</em>.</p> <p>Let’s consider a positive variable $X$ that follows Benford’s Law. Assume that $X$ has a probability measure with density $f$ w.r.t. the Lebesgue measure. Since changing units doesn’t break Benford’s law, we can multiply $X$ by some constant $k$ and still end up with the same distribution. In other words, there is some constant $C(k)$ that such $\forall x, f(kx)=C(k)f(x)$. This is the definition of scale invariance. We also need the probability mass to conserve here, i.e. $f(x)dx = f(kx)d(kx)$, i.e. $f(kx)=\frac{f(x)}{k}$. Differentiating with respect to $k$ and then setting $k=1$ yields the linear functional equation $f’(x) = -\frac{1}{x}f(x)$, with solution $f(x) = \frac{\lambda}{x}$. This isn’t technically a probability density function, since it cannot be normalized. In fact scale-invariant distributions are exactly of the form $p(x)\propto \frac{1}{x^{\alpha}}$ for $\alpha&gt;1$ (power law). Let’s thus consider $\alpha=1.01$ for instance, to bring us close to the ideal case of Benford’s Law.</p> <p>We simulate $X$ as a random variable with density $p(x)\propto \frac{1}{x^\alpha}$ and plot the distribution of the first digit of $X$ on <a href="#fig-4">Figure 4</a>. We observe that the distribution of the first digit of $X$ indeed matches Benford’s Law. Note that the result is <em>not</em> robust to the choice of $\alpha$: we only observe concordance with Benford’s Law for $\alpha$ close to 1.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/benford_law/scale_invariance-480.webp 480w,/assets/img/posts/benford_law/scale_invariance-800.webp 800w,/assets/img/posts/benford_law/scale_invariance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/benford_law/scale_invariance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="scale invariance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Benford Law vs. empirical distribution of the first digit of $X$. We used parameters $N=1,000,000, \alpha=1.01$. </div> <h2 id="conclusion">Conclusion</h2> <p>We provided three different intuitions behind Benford’s Law, which we illustrated with simulations.</p> <ol> <li>Geometric growth: Benford’s Law arises when variables grow geometrically.</li> <li>CLT on the logarithm: Benford’s Law arises when variables are products of many factors.</li> <li>Scale invariance: Benford’s Law arises when variables are scale-invariant.</li> </ol>]]></content><author><name></name></author><category term="misc"/><summary type="html"><![CDATA[TL;DR: Many real-world datasets follow Benford's Law, which states that distribution of the first digit is not uniform. We provide three different intuitions behind this phenomenon.]]></summary></entry><entry><title type="html">The case against leveraged ETFs</title><link href="https://gaetanx21.github.io/blog/2024/leveraged-etf/" rel="alternate" type="text/html" title="The case against leveraged ETFs"/><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/2024/leveraged-etf</id><content type="html" xml:base="https://gaetanx21.github.io/blog/2024/leveraged-etf/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>We first introduce leveraged ETFs: their purpose, daily rebalancing, and the common misconception around them. We then delve into the math behind leveraged ETFs, showing that they are not suitable for long-term investments due to 1) their extreme price swings 2) the volatility drag they incur. We illustrate our results on <code class="language-plaintext highlighter-rouge">TQQQ</code> and <code class="language-plaintext highlighter-rouge">SQQQ</code>, 3x and -3x leveraged ETFs on the Nasdaq 100 respectively. Our data comes from Yahoo Finance (through python module <code class="language-plaintext highlighter-rouge">yfinance</code>) and consists of the adjusted close prices from 2010 to 2024.</p> <h2 id="etfs-and-letfs">ETFs and LETFs</h2> <p>ETFs, or <em>exchange-traded funds</em>, have gained prominence over the past decades. In 1993, the S&amp;P 500 Trust ETF or SPY and its mere \$11M in assets gave birth to the ETF industry. Over the years, the number of ETFs and their assets under management (AUM) have ballooned, with over 12,000 ETFs and \$8T of AUM as of 2024<sup id="fnref:investopedia"><a href="#fn:investopedia" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. ETFs appeal to investors for their low fees and trading flexibility. Unlike mutual funds which can only be traded at market close, ETFs are traded continuously just like stocks. In addition, ETFs use in-kind rather than cash creation/redemption of units, making them a tax-efficient investment vehicle. The ETF industry has grown so much that it now represents a significant fraction of public float as well as daily trading volume. In the U.S., ETFs accounted for 12.7% of equities and 28.2% of trading as of Q3 2023<sup id="fnref:ishares"><a href="#fn:ishares" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. The increasing weight of ETFs in the stock market motivates the study of ETF-related order flows and potential flow-based trading strategies.</p> <p><em>Leveraged</em> ETFs (LETFs) are a specific kind of ETF which utilize leverage to increase exposure to the underlying. The underlying is most commonly a popular stock index, but it can also be a single stock, for instance <em>Direxion Daily NVDA Bull 2X</em> (<code class="language-plaintext highlighter-rouge">NVDU</code>). For long LETFs, the leverage ratio $\beta$ is often 2 and sometimes 3. For short LETFs, also known as <em>inverse</em> ETFs, $\beta$ can be -1, -2, and more rarely -3. Despite being relatively new, with the first fund opened in 2006<sup id="fnref:marketwatch"><a href="#fn:marketwatch" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, the LETF industry has experienced frantic AUM growth over the past two decades, even exceeding that of the regular ETF industry. As of June 2024, LETFs accounted for more than $100B in the U.S. alone. <code class="language-plaintext highlighter-rouge">TQQQ</code> (a 3x Nasdaq 100 LETF) is the largest LETF in the world, with over $23B under management as of June 2024.</p> <h2 id="intuition-behind-letfs">Intuition behind LETFs</h2> <h3 id="purpose-and-functioning">Purpose and functioning</h3> <p>The announced objective of LETFs is to magnify the returns of an underlying. The underlying is often a popular stock index, but it can also be a single stock, bonds, commodities, and more recently cryptos. Throughout the rest of the paper, we will only consider LETFs linked to stock indexes.</p> <p>In theory, a LETF with leverage $\beta$ ($\beta\in\lbrace -3,-2,-1,2,3 \rbrace$) will have daily close-to-close returns equal to $\beta$ times the underlying’s close-to-close returns. For instance, if the S&amp;P 500 index moves up 1% on a given day, then a corresponding 3x LETF (e.g. <code class="language-plaintext highlighter-rouge">SPXL</code>) will be up 3% on that day. Likewise, if the S&amp;P 500 is down 2%, then that LETF will be down 6%. In practice, the fund’s expense ratio and the cost of borrowing cash (resp. stocks) for long (resp. short) LETFs incur a small performance drag.</p> <p>To gain leverage, the fund manager can either buy/short the index’s individual components or enter into swap agreements. Regardless of the method used, the result is the same: daily LETF returns will be equal to the daily index returns multiplied by $\beta$. For the sake of simplicity, in the rest of this post we will assume that LETFs use swap agreements to gain exposure.</p> <h3 id="daily-rebalancing">Daily rebalancing</h3> <p>Non-leveraged ETFs tracking market capitalization-weighted indexes need to rebalance only when the underlying index itself undergoes a rebalance, which happens <em>quarterly</em> for most indexes. On the contrary, LETFs need to rebalance <em>daily</em>.</p> <p>To understand why, let’s consider a hypothetical 2x LETF for the S&amp;P 500 with \$100M under management on day 0. The index manager thus contracts $2\times100=\$200M$ worth of swap agreement to have an exposure of $\beta=2$. Now let’s assume the S&amp;P 500 is up 1% on day 1. The fund is up $2\times 1 = 2$% and the AUM is now $1.02\times 100=\$102M$, while our swap agreement exposure is now $1.01\times200=\$202M$. However, to maintain the leverage $\beta=2$, we now need a swap exposure of $2\times 102=\$204M$. Therefore we must contract another \$2M worth of swap agreements to go from our current \$202M exposure to the required \$204M exposure. Likewise, if the S&amp;P 500 goes down 1% on day 1, the new AUM is \$98M and the new exposure is \$198M. Since the new exposure must be $2\times 98=\$196M$, we need to sell \$2M worth of swap agreements. This example shows not only that a leveraged ETF must be rebalanced daily, but also that the fund manager always has to “buy high and sell low” since they increase (resp. decrease) their swap positions when the index is up (resp. down). Note that this is already a first hint of the adverse behavior of LETFs when held for more than a day.</p> <p>Let’s now formalize the reasoning above. Let us consider a LETF with leverage $\beta$ tracking a given index. We will denote $r_t$ the index’s daily return on day $t$. In addition, $A_t$ and $S_t$ will be the AUM and swap exposure on day $t$, respectively. By definition, we have</p> \[\begin{align*} A _ {t+1}&amp;=(1+\beta r _ t)A _ t \\ S _ {t+1}&amp;=(1+r _ t)S _ t=(1+r _ t)\beta A _ t \end{align*}\] <p>In order to maintain leverage $\beta$, our swap exposure at $t+1$ must be $\tilde{S} _ {t+1}=\beta A _ {t+1}=\beta(1+\beta r _ t)A _ t$, meaning that we need to update our swap positions according to \(\Delta _ t=\tilde{S} _ {t+1}-S _ {t+1}=\beta(\beta-1)r _ t A _ t\) We remark that the factor $\beta(\beta-1)$ is always positive for the values of $\beta$ considered (i.e. outside $[0,1]$), thus $\Delta _ t$ has the same sign as $r _ t$, which confirms that the LETF fund manager will be required to <strong>“buy high and sell low”</strong> every day as they rebalance their swap positions near market close.</p> <h3 id="a-common-misconception">A common misconception</h3> <p>A significant fraction of retail and sometimes professional investors seem not to understand the long-term behavior of LETFs. Indeed, buy-and-hold investors should stay clear of LETFs under most circumstances<sup id="fnref:sec"><a href="#fn:sec" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. As it turns out, the long-term performance of a LETF with leverage $\beta$ is (vastly) different from the performance of a static portfolio with leverage $\beta$<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>. Mathematically, this is simply because we have</p> \[\begin{align*} A _ T^\text{LETF}&amp;=A _ 0\prod _ {i=0}^{T-1}(1+\beta r _ t) \\ A _ T^\text{static}&amp;=\beta\bigg(A _ 0\prod _ {i=0}^{T-1}(1+ r _ t)\bigg) - (\beta - 1) \end{align*}\] <p>where $A_t$ is the AUM on day $t$<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>.</p> <p>To illustrate the difference in long-term performance between a LETF and a static portfolio with the same leverage, we consider the Nasdaq 100 index and its corresponding 3x LETF <code class="language-plaintext highlighter-rouge">TQQQ</code>. We compare the performance of <code class="language-plaintext highlighter-rouge">TQQQ</code> with a static portfolio holding QQQ with leverage 3. The results are shown in <a href="#fig-1">Figure 1</a>.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/leveraged_etf/tqqq_vs_leverage-480.webp 480w,/assets/img/posts/leveraged_etf/tqqq_vs_leverage-800.webp 800w,/assets/img/posts/leveraged_etf/tqqq_vs_leverage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/leveraged_etf/tqqq_vs_leverage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tqqq" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/leveraged_etf/tqqq_vs_leverage_log-480.webp 480w,/assets/img/posts/leveraged_etf/tqqq_vs_leverage_log-800.webp 800w,/assets/img/posts/leveraged_etf/tqqq_vs_leverage_log-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/leveraged_etf/tqqq_vs_leverage_log.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tqqq log" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Performance of TQQQ vs. a static portfolio holding QQQ with leverage 3 (left: linear, right: log). Clearly, the long-term performance is not the same. In particular, TQQQ is extremely more volatile than the static portfolio. </div> <p>To get an intuition behind the long-term divergence in performance between a LETF and the corresponding static portfolio, consider a hypothetical index whose daily returns alternate regularly between -0.9% and +1%. Over 2 days, this index will be up $\simeq0.09\%$ ($0.991\times1.01-1$) whereas a 2x LETF on that index will be up $\simeq0.16\%$ ($0.982\times1.02-1$). Compounded over a longer time period, the performance gap between the index and the corresponding 2x LETF widens: over 200 trading days, the index is up $\simeq9.5\%$ ($(0.991\times1.01)^{100}-1$) whereas the corresponding 2x LETF is up $\simeq17.8\%$ ($(0.982\times1.02)^{100}-1$). Likewise, a 3x LETF would be up roughly 24.5%.</p> <div style="text-align: center;" id="table-1"> <table style="margin: 0 auto;"> <thead> <tr> <th>Leverage $\beta$</th> <th>LETF</th> <th>Static portfolio</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>+9.5%</td> <td>+9.5%</td> </tr> <tr> <td>2</td> <td>+17.8%</td> <td>+19.0%</td> </tr> <tr> <td>3</td> <td>+24.5%</td> <td>+28.5%</td> </tr> <tr> <td>-1</td> <td>-10.3%</td> <td>-9.5%</td> </tr> <tr> <td>-2</td> <td>-21.0%</td> <td>-19.0%</td> </tr> <tr> <td>-3</td> <td>-31.7%</td> <td>-28.5%</td> </tr> </tbody> </table> </div> <p><strong>Table 1</strong>: Performance over 200 trading days for a hypothetical underlying index alternating -0.9% and +1% daily returns. Note that for the static portfolio we have assumed no costs to borrow cash/stocks.</p> <p><a href="#table-1">Table 1</a> illustrates the vast difference in performance between static portfolio and LETF for various values of $\beta$, still considering the same alternating index over 200 trading days. We remark that regardless of the leverage ratio, the LETF underperforms the corresponding static portfolio. Also, the performance drag worsens as $| \beta |$ increases. Let’s now study a simple model to understand and quantify the observed performance discrepancy.</p> <h2 id="dynamics-of-letfs">Dynamics of LETFs</h2> <h3 id="continuous-time-model">Continuous-time model</h3> <p>In order to explain the surprising return dynamics observed previously, we can use a simple model proposed by Avellaneda<sup id="fnref:avellaneda"><a href="#fn:avellaneda" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. In this model, we denote by $S_t$ and $L_t$ the value of the index and the NAV<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> of the corresponding LETF, respectively, on day $t$. In addition, let $r, f, \lambda$ be the risk-free rate, the fund’s expense ratio, and the cost of borrowing the index, respectively. In particular, we assume these three variables to be constant, which is at least true on the short-term.</p> <p>By construction of the LETF, we thus have<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> \(\begin{equation} \label{basic-ret} \frac{dL_t}{L_t}=\beta \frac{dS_t}{S_t} - (\beta-1)r dt - fdt + 1_{\beta&lt;0}\beta\lambda dt \end{equation}\) where the cost of borrowing the index $\beta\lambda dt$ is incurred only for inverse LETFs i.e. when $\beta&lt;0$.</p> <p>In order to move forward we now need a model for the daily index returns $\frac{dS_t}{S_t}$. We use an Itô process \(\begin{equation} \label{ito} \frac{dS_t}{S_t}=\mu_t dt + \sigma_t dW_t \end{equation}\)</p> <p>Plugging ($\ref{ito}$) in ($\ref{basic-ret}$) and separating the drift and noise terms, we obtain \(\begin{equation} \frac{dL_t}{L_t}=\big(\beta\mu_t-(\beta-1)r-f+1_{\beta&lt;0}\beta\lambda\big)dt + \beta\sigma_t dW_t \end{equation}\)</p> <p>We then use Itô’s lemma to find \(\begin{equation} d[\ln{L_t}]=\big(\beta\mu_t-(\beta-1)r-f+1_{\beta&lt;0}\beta\lambda - \frac{\beta^2 \sigma_t^2}{2}\big)dt + \beta\sigma_t dW_t \end{equation}\)</p> <p>Integrating from 0 to $T$ yields \(\begin{equation} \label{eq3} \ln \frac{L_T}{L_0} = \beta M_T - \big((\beta-1)r -f + 1_{\beta&lt;0}\beta\lambda\big)T - \frac{\beta^2}{2}V_T + \beta \sqrt{V_T}Z \end{equation}\) where $M_T=\int_0^T \mu_t dt$, $V_T=\int_0^T \sigma_t^2 dt$ and $Z=N(0,1)$.</p> <p>To simplify (\ref{eq3}), we remark that \(\begin{equation} \label{eq4} \ln \frac{S_T}{S_0} = M_T - \frac{V_T}{2} + \sqrt{V_T}Z \end{equation}\)</p> <p>Plugging (\ref{eq4}) in (\ref{eq3}), we obtain \(\begin{equation} \ln \frac{L_T}{L_0} = \beta \ln \frac{S_T}{S_0} + \frac{\beta-\beta^2}{2}V_T - \big((\beta-1)r -f + 1_{\beta&lt;0}\beta\lambda\big)T \end{equation}\)</p> <p>Finally, exponentiation yields \(\begin{equation} \frac{L_T}{L_0} = \bigg(\frac{S_T}{S_0}\bigg)^\beta \exp\bigg(\frac{\beta-\beta^2}{2}V_T - \big((\beta-1)r -f + 1_{\beta&lt;0}\beta\lambda\big)T\bigg) \end{equation}\)</p> <p>Neglecting $r, f, \lambda$, we end up with the following neat expression \(\begin{equation} \label{eq:final} \boxed{\frac{L_T}{L_0} = \bigg(\frac{S_T}{S_0}\bigg)^\beta \exp\bigg(-\frac{\beta^2-\beta}{2}V_T\bigg)} \end{equation}\)</p> <h3 id="interpretation-and-consequences">Interpretation and consequences</h3> <p>The first thing to note from ($\ref{eq:final}$) is that we clearly do not have the linear relationship $\frac{L_T}{L_0}-1=\beta(\frac{S_T}{S_0}-1)$ which we may naively expect. In fact, this linear relationship holds for the static portfolio only. Instead, we have a linear relationship between the <em>logarithms</em> of the LETF and the index, which is not the same thing at all! It means an <em>exponential</em> relationship between the LETF and the index, aka huge swings in the LETF’s value. This is the first reason why LETFs are not suitable for long-term investments.</p> <p>The second thing to note is the term $\exp\big(-\frac{\beta^2-\beta}{2}V_T\big)$ which is always strictly less than 1 since $\beta^2-\beta$ is positive for all the values of $\beta$ considered. Since $V_T=\int_0^T \sigma_t^2 dt$ is the realized volatility, we see that the LETF is adversely exposed to the index turbulence: there is a <strong>“volatility drag”</strong>. Thus, the higher the volatility, the larger $V_T$ and thus we need $\frac{S_T}{S_0}\gg 1$ to compensate. In particular, if we simply assume that $\forall t, \sigma_t=\sigma$, then $V_T=\sigma^2 T$ and thus $\frac{L_T}{L_0} = \bigg(\frac{S_T}{S_0}\bigg)^\beta \exp\bigg(-\frac{\beta^2-\beta}{2}\sigma^2 T\bigg)$ i.e. there is an exponential time decay in the long-term performance of the LETF.</p> <p>The bottom line is that LETFs are not adequate buy-and-hold investments for the two reasons above. Holding them for too long will not yield the hoped-for linear relationship $r_\text{total}^\text{LETF}=\beta \times r_\text{total}^\text{index}$ ; worse still, it will incur an inevitable volatility drag which scales more or less as an exponential time decay.</p> <h2 id="illustration-on-tqqq-and-sqqq">Illustration on TQQQ and SQQQ</h2> <p>Let’s illustrate the volatility decay of <code class="language-plaintext highlighter-rouge">TQQQ</code> and <code class="language-plaintext highlighter-rouge">SQQQ</code>, the 3x and -3x Nasdaq 100 LETFs respectively. We will use equation ($\ref{eq:final}$) to compute $-\frac{\beta^2-\beta}{2}V_T = \ln(\frac{L_T}{L_0})-\beta \ln(\frac{S_T}{S_0})$: we expect to find a negatively sloped line.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/leveraged_etf/tqqq-480.webp 480w,/assets/img/posts/leveraged_etf/tqqq-800.webp 800w,/assets/img/posts/leveraged_etf/tqqq-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/leveraged_etf/tqqq.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tqqq" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/leveraged_etf/sqqq-480.webp 480w,/assets/img/posts/leveraged_etf/sqqq-800.webp 800w,/assets/img/posts/leveraged_etf/sqqq-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/leveraged_etf/sqqq.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sqqq" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Exponential decay of TQQQ and SQQQ. </div> <p><a href="#fig-2">Figure 2</a> presents the exponential decay of <code class="language-plaintext highlighter-rouge">TQQQ</code> and <code class="language-plaintext highlighter-rouge">SQQQ</code> over the past 15 years. We obtain a negatively sloped line as expected. In addition, we find that the slope is steeper for <code class="language-plaintext highlighter-rouge">SQQQ</code> than for <code class="language-plaintext highlighter-rouge">TQQQ</code>, which is consistent with the fact that the volatility drag is proportional to $\beta^2-\beta$, which is larger for <code class="language-plaintext highlighter-rouge">SQQQ</code> than for <code class="language-plaintext highlighter-rouge">TQQQ</code>. We cannot say much more about the slopes and the intersects since we’ve neglected $r, f, \lambda$ in our model.</p> <h2 id="conclusion">Conclusion</h2> <p>We’ve shown that LETFs are more complex than they seem. In particular, buying a $\beta$ LETF is not the same as buying a static portfolio with leverage $\beta$. Equation ($\ref{eq:final}$) nicely sums up the double problem with LETFs: 1) they vary exponentially with the underlying index 2) they incur a volatility drag. We could dig deeper by taking into account $r, f, \lambda$ in our model, but the main point is already clear: LETFs are not suitable for long-term investments.</p> <p>You may wonder: who uses LETFs then? I have no definite answer, but certainly traders who wish to hedge their positions with less collateral may find LETFs useful. In addition, LETFs can be used to speculate on short-term market movements. The average detention period of <code class="language-plaintext highlighter-rouge">TQQQ</code> (~3 days) and <code class="language-plaintext highlighter-rouge">SQQQ</code> (&lt;1 day) is a clear indication that LETFs are not meant to be held for long periods.</p> <hr/> <p><strong>Footnotes &amp; References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:investopedia"> <p><em>A Brief History of Exchange-Traded Funds</em>. Simpson, S. <a href="investopedia.com/articles/exchangetradedfunds/12/brief-history-exchangetraded-funds.asp">Investopedia.com</a> <a href="#fnref:investopedia" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ishares"> <p><em>Global ETF Market Facts: Three things to know from Q3 2023.</em>. Cohen, S. <a href="ishares.com/us/insights/global-etf-facts">iShares.com</a> <a href="#fnref:ishares" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:marketwatch"> <p><em>ProFunds prepares first leveraged ETFs</em>. Spence, J. <a href="marketwatch.com/story/profunds-readies-first-leveraged-etfs">MarketWatch.com</a> <a href="#fnref:marketwatch" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sec"> <p><em>Updated Investor Bulletin: Leveraged and Inverse ETFs</em>. SEC Investor Alerts and Bulletins. <a href="sec.gov/resources-for-investors/investor-alertsbulletins/updated-investor-bulletin-leveraged-inverse-etfs">sec.gov</a> <a href="#fnref:sec" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1"> <p>Such static portfolio is obtained by borrowing cash/stocks on day 0 to get the desired level of leverage and then holding until exit. In particular, there is no daily rebalancing and the portfolio’s value can be negative. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>Note that formally, the static portfolio can reach negative AUM. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:avellaneda"> <p><em>Path-dependence of Leveraged ETF returns</em>. Avellaneda, M. <a href="#fnref:avellaneda" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>The Net Asset Value of a fund is the value of its assets divided by its number of shares outstanding. Technically, the NAV and the price per share are two distinct values, but in practice they remain very close for non-arbitrage reasons, and thus in this paper we will refer to LETF’s NAV or price interchangeably. In addition, we will always use split-adjusted NAVs/prices. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Technically, this formula holds true for daily returns only i.e. when $dt=\Delta t=1$ day, but we will assume it to be true on an infinitesimal time scale. Note that Avellaneda also proposed a discrete-time model equivalent to the one described here. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="quant-finance"/><summary type="html"><![CDATA[TL;DR: Leveraged ETFs amplify daily returns, which is not the same as basic leverage, especially in the long term. Digging into the math reveals that leveraged ETFs are not suitable buy-and-hold investments as they 1) exhibit huge price swings 2) incur a volatility drag.]]></summary></entry></feed>