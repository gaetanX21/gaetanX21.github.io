<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gaetanx21.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gaetanx21.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-18T11:31:36+00:00</updated><id>https://gaetanx21.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Fused &amp;amp; Furious: Sinkhorn Triton Kernels</title><link href="https://gaetanx21.github.io/blog/fused-and-furious/" rel="alternate" type="text/html" title="Fused &amp;amp; Furious: Sinkhorn Triton Kernels"/><published>2026-01-17T00:00:00+00:00</published><updated>2026-01-17T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/fused-and-furious</id><content type="html" xml:base="https://gaetanx21.github.io/blog/fused-and-furious/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\Rnn}{\mathbb{R}^{n\times n}} \newcommand{\D}{D_{model}} \newcommand{\x}{\mathbf{x}} \newcommand{\F}{\mathcal{F}} \newcommand{\Hpre}{\mathcal{H}^{pre}} \newcommand{\Hpost}{\mathcal{H}^{post}} \newcommand{\Hres}{\mathcal{H}^{res}} \newcommand{\B}{\mathcal{B}_n} \newcommand{\one}{\mathbf{1}_n} \newcommand{\T}{\text{Tr}} \newcommand{\KL}{\text{KL}}\] <p>In their latest paper, charmingly titled <em>Manifold-Constrained Hyper-Connections</em><sup id="fnref:mhc"><a href="#fn:mhc" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, DeepSeek improved upon preexisting research on <strong>residual stream scaling</strong> with two major contributions:</p> <ol> <li>an approach that’s more <strong>stable</strong> than previous papers, with demonstrated performance at scale</li> <li>an <strong>efficient</strong> infrastructure design that minimizes the overhead of hyper-connections compared to vanilla transformers</li> </ol> <p>My goal in this post is to delve deeper into the second part. More precisely, I want to focus on the Sinkhorn projection step of their architecture: why we need custom fused GPU kernels to implement it, and how to do it.</p> <p>This post will be in three parts:</p> <ol> <li>First, I’ll present increasingly complex residual connection paradigms, culminating in the mHC architecture.</li> <li>Then, I’ll provide some geometric intuition behind Sinkhorn’s algorithm and derive it.</li> <li>Finally, I’ll showcase increasingly fast Triton kernels for the Sinkhorn algorithm.</li> </ol> <p><a href="#fig-1">Figure 1</a> serves as a teaser of the crazy speedups we achieved. Let’s dive in!</p> <p><em>The code for this project can be found <a href="https://github.com/gaetanX21/sinkhorn-triton">here</a>.</em></p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/speedup-480.webp 480w,/assets/img/posts/fused_and_furious/speedup-800.webp 800w,/assets/img/posts/fused_and_furious/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Our optimized Triton kernel beats the compiled PyTorch alternative by 1 or 2 orders of magnitude. </div> <hr/> <h2 class="no_toc" id="table-of-contents">Table of Contents</h2> <ul id="markdown-toc"> <li><a href="#i-scaling-the-residual-stream" id="markdown-toc-i-scaling-the-residual-stream">I. Scaling the residual stream</a> <ul> <li><a href="#ia-residual-matrix-transformers" id="markdown-toc-ia-residual-matrix-transformers">I.A. Residual Matrix Transformers</a></li> <li><a href="#ib-hyper-connections" id="markdown-toc-ib-hyper-connections">I.B. Hyper-Connections</a></li> <li><a href="#ic-manifold-constrained-hyper-connections" id="markdown-toc-ic-manifold-constrained-hyper-connections">I.C. Manifold-Constrained Hyper-Connections</a></li> </ul> </li> <li><a href="#ii-sinkhorns-algorithm" id="markdown-toc-ii-sinkhorns-algorithm">II. Sinkhorn’s Algorithm</a> <ul> <li><a href="#iia-projecting-under-the-generalized-kl-divergence" id="markdown-toc-iia-projecting-under-the-generalized-kl-divergence">II.A. Projecting under the generalized KL divergence</a></li> <li><a href="#iib-deriving-the-algorithm" id="markdown-toc-iib-deriving-the-algorithm">II.B. Deriving the algorithm</a></li> </ul> </li> <li><a href="#iii-triton-kernels-for-sinkhorn" id="markdown-toc-iii-triton-kernels-for-sinkhorn">III. Triton kernels for Sinkhorn</a> <ul> <li><a href="#iiia-hardware-setup" id="markdown-toc-iiia-hardware-setup">III.A. Hardware setup</a></li> <li><a href="#iiib-mini-refresher-on-memory-hierarchy" id="markdown-toc-iiib-mini-refresher-on-memory-hierarchy">III.B. Mini-refresher on memory hierarchy</a></li> <li><a href="#iiic-implementations" id="markdown-toc-iiic-implementations">III.C. Implementations</a> <ul> <li><a href="#naive-pytorch-implementation" id="markdown-toc-naive-pytorch-implementation">Naive PyTorch implementation</a></li> <li><a href="#basic-fused-kernel" id="markdown-toc-basic-fused-kernel">Basic fused kernel</a></li> <li><a href="#loading-m-in-registers" id="markdown-toc-loading-m-in-registers">Loading $M$ in registers</a></li> <li><a href="#block-packing" id="markdown-toc-block-packing">Block packing</a></li> </ul> </li> <li><a href="#iiid-benchmark" id="markdown-toc-iiid-benchmark">III.D. Benchmark</a></li> <li><a href="#iiie-figures" id="markdown-toc-iiie-figures">III.E. Figures</a> <ul> <li><a href="#vanity-metric" id="markdown-toc-vanity-metric">Vanity metric</a></li> <li><a href="#comparing-implementations" id="markdown-toc-comparing-implementations">Comparing implementations</a></li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="i-scaling-the-residual-stream">I. Scaling the residual stream</h2> <p>A current trend in macro design of transformers is to scale the <strong>size</strong> of the residual stream. That is, instead of being a vector $\x\in\R^{\D}$, the per-token residual stream becomes a matrix $X\in\R^{n\times\D}$ where $n$ is a new scaling dimension (often a small value e.g. $n=4$). The motivation behind this architectural shift is that in vanilla transformers, model parameters and FLOPs scale <em>quadratically</em> with $\D$ whereas the residual stream scales <em>linearly</em>. Hence, as we keep increasing model size, the residual stream may become a <strong>bottleneck</strong>.</p> <h3 id="ia-residual-matrix-transformers">I.A. Residual Matrix Transformers</h3> <p>My <a href="/blog/residual-matrix-transformer/">previous post</a> discussed the <strong>Residual Matrix Transformer</strong> (RMT) architecture, which introduces a matrix representation of the per-token residual stream inspired by outer-product memories. The key idea is that using a matrix instead of a vector increases the effective storage capacity of the residual stream. The implementation of RMT is fairly simple, as the only difference compared to the classic transformer architecture is how we <em>read from</em> and <em>write to</em> the residual stream. Crucially, the micro design remains untouched (i.e. Attention and Feed-Forward blocks do not change), which implies that scaling the residual stream incurs negligible computational overhead, though it <em>does</em> cause memory access overhead if implemented naively (i.e. without fused kernels) as we will see.</p> <p>In a nutshell, RMT replaces the classic residual connection</p> \[\x_{l+1} = \x_l + \F_l(\x_l)\] <p>with</p> \[X_{l+1} = X_l + {\Hpost}^T \F_l(\Hpre X_l)\] <p>where $\x_l\in\R^{\D}$ (resp. $X_l\in\R^{n\times\D}$) is the $l$-th layer vector (resp. matrix) residual stream, and $\F_l$ is the $l$-th transformer block.</p> <p>$\Hpre_l, \Hpost_l \in \R^{1\times n}$ are <em>learned</em> vectors which correspond respectively to <code class="language-plaintext highlighter-rouge">READ</code> and <code class="language-plaintext highlighter-rouge">WRITE</code> operations on the residual stream memory store.</p> <h3 id="ib-hyper-connections">I.B. Hyper-Connections</h3> <p>Hyper-Connections<sup id="fnref:hc"><a href="#fn:hc" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> (HC) extend the idea of RMT by enabling <em>communication</em> between the $n$ channels of the residual stream.</p> <p>The motivation is to <strong>increase the topological complexity of the residual mapping without increasing the computational complexity</strong>. The main improvement over RMT is the introduction of $\Hres_l\in\Rnn$ which acts as a “stream-mixing matrix” allowing the $n$ channels to <strong>exchange information</strong> instead of simply evolving independently. Also, $\Hpre_l, \Hpost_l, \Hres_l$ are now both <em>static</em> (i.e. learned parameters) and <em>dynamic</em> (i.e. dependent on input $X_l$) to further increase the topological complexity of the connection network (see original paper for more details). Note that we will omit the dependence on $X_l$ to keep notations lightweight (e.g. we’ll write $\Hres_l$ instead of $\Hres_l(X_l)$)</p> <p>Thus, the residual connection in HC is even richer than in RMT thanks to $\Hres_l$:</p> \[X_{l+1} = \Hres_l X_l + {\Hpost}^T \F_l(\Hpre X_l)\] <h3 id="ic-manifold-constrained-hyper-connections">I.C. Manifold-Constrained Hyper-Connections</h3> <p>DeepSeek’s Manifold-Constrained Hyper-Connections (mHC) address one critical flaw in HC: instability as the network depth $L$ scales.</p> <p>Indeed, HC abandons the identity mapping of residual connections and replaces it with $\Hres_l$. Yet, the identity mapping is crucial to the training stability of deep neural network architectures<sup id="fnref:resnet"><a href="#fn:resnet" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>.</p> <p>It’s easy to see why replacing $\mathbb{I}_n$ with $\Hres_l$ is problematic: if we recursively extend the residual connection across multiple layers, we have:</p> <ol> <li>\(\x_{l+m} = \x_l + \sum_{k=0}^{m-1}\F_{k+l}(\x_{l+k})\) for the standard residual connection</li> <li>\(X_{l+m} = X_l + \sum_{k=0}^{m-1}{\Hpost_{l+k}}^T \F_{l+k}(\Hpre_{l+k} X_{l+k})\) for RMT</li> <li>\(X_{l+m} = \big(\overleftarrow{\prod}_{k=0}^{m-1}\Hres_{l+k}\big) X_l + \sum_{k=0}^{m-1} \big(\overleftarrow{\prod}_{j=k+1}^{m-1}\Hres_{l+j}\big) {\Hpost_{l+k}}^T \F_{l+k}(\Hpre_{l+k} X_{l+k})\) for HC</li> </ol> <p>where $\overleftarrow{\prod}$ denotes the reverse product i.e. $\overleftarrow{\prod} _ {k=0}^{m-1}\Hres_{l+k} = \Hres_{l+m-1}\Hres_{l+m-2}\cdots\Hres_{l+1}\Hres_{l}$.</p> <p>The matrix product $\Pi=\overleftarrow{\prod} _ {k=0}^{m-1}\Hres_{l+k}$ has no reason to behave nicely i.e. keep a spectral norm close to 1. In fact, mHC shows that instead of preserving the signal strength, $\Pi$ tends to amplify or attenuate it as depth increases, resulting in signal rescaling across <strong>several orders of magnitudes</strong>, which is problematic for both the forward pass (exploding/vanishing activations) and the backward pass (exploding/vanishing gradients). The propagation of instability as depth increases is illustrated in <a href="#fig-2">Figure 2</a>.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/mhc_forward_backward_gain-480.webp 480w,/assets/img/posts/fused_and_furious/mhc_forward_backward_gain-800.webp 800w,/assets/img/posts/fused_and_furious/mhc_forward_backward_gain-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/mhc_forward_backward_gain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 2.</b> This figure illustrates the propagation dynamics of (a) the single-layer mapping $\Hres_l$ and (b) the composite mapping $\prod_{i=1}^{L-1}\Hres_{L-i}$ within a 27B model. The layer index $l$ (x-axis) unrolls each standard Transformer block into two independent layers (Attention and FFN). The <i>Amax Gain Magnitude</i> (y-axis) is calculated as the maximum absolute row sum (for the forward signal) and column sum (for the backward gradient), averaged over all tokens in a selected sequence. Figure taken from DeepSeek mHC paper. </div> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/mhc_teaser-480.webp 480w,/assets/img/posts/fused_and_furious/mhc_teaser-800.webp 800w,/assets/img/posts/fused_and_furious/mhc_teaser-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/mhc_teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 3.</b> Illustrations of residual connection paradigms. This figure compares the structural design of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) mHC. Unlike the unconstrained HC, mHC focuses on optimizing the residual connection space by projecting the matrices onto a constrained manifold to ensure stability. Figure taken from DeepSeek mHC paper. </div> <p>DeepSeek’s solution to this instability problem is to constrain $\Hres_l$ to be a <strong>bistochastic matrix</strong> i.e. a square matrix with non-negative entries where each row and column sums to 1.</p> <p>Bistochastic matrices have three nice properties that make them ideal for stabilizing the residual stream:</p> <ol> <li>they have a spectral norm of 1, meaning that they preserve signal strength across layers</li> <li>they are closed under matrix multiplication, meaning that the product of multiple bistochastic matrices is also bistochastic</li> <li>they can be given a probabilistic / optimal transport interpretation as <em>soft permutations</em>, meaning that they allow each channel to attend to and exchange with every other channel, without collapsing to a single channel</li> </ol> <p>To enforce the bistochastic constraint, we must first compute $\Hres_l$ as in HC, and then <strong>project</strong> it onto the manifold of bistochastic matrices $\B$, also known as Birkhoff’s polytope. The next section goes into the details of this projection, which uses Sinkhorn’s algorithm.</p> <h2 id="ii-sinkhorns-algorithm">II. Sinkhorn’s Algorithm</h2> <p>Quite plainly, the goal of Sinkhorn’s algorithm is to project a given square matrix $M\in\Rnn$<sup id="fnref:positivity"><a href="#fn:positivity" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> onto the Birkhoff polytope</p> <p>\(\B=\lbrace P\in\Rnn \mid P\one=\one, \one^T P=\one^T, P\geq 0\rbrace\).</p> <p>Note that $\B$ is convex as we will use this property later. In fact it is the convex hull generated by the $n!$ permutation matrices in $\Rnn$, which are the extreme points of $\B$.<sup id="fnref:mhc-lite"><a href="#fn:mhc-lite" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p> <p>Let’s now present Sinkhorn’s algorithm and derive its formulation intuitively.</p> <h3 id="iia-projecting-under-the-generalized-kl-divergence">II.A. Projecting under the generalized KL divergence</h3> <p>We’ve been talking about projections, but to define a projection, one needs a metric. In other words, we need a sensible metric $d: \Rnn \times \Rnn \to \Rnn$ and then we can try to find \(\text{Proj}_d^{\B}(M) = \arg\min_{P \in \B} d(P,M)\) for a given matrix $M \in \Rnn$.</p> <p>One classic metric on $\Rnn$ is the one <em>induced</em> by the Frobenius norm ${\Vert M \Vert} _ F = \sqrt{\T (MM^T)} = \sqrt{ \sum_{1 \leq i,j \leq n} M_{ij}^2}$.</p> <p>However, using this metric would mean we’re doing a Euclidean ($L^2$) projection, which intuitively doesn’t feel right here given the probabilistic / optimal transport interpretation of bistochastic matrices. It would also require solving a linear programming problem in $O(n^3)$, which isn’t ideal.</p> <p>Instead, one much more sensible metric<sup id="fnref:divergence"><a href="#fn:divergence" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> is the KL divergence. Since we’re dealing with bistochastic matrices, we use the <em>generalized</em> KL divergence:</p> \[\KL(P\Vert M) = \sum_{1 \leq i,j \leq n} \left( P_{ij} \log\left(\frac{P_{ij}}{M_{ij}}\right) - P_{ij} + M_{ij} \right)\] <p>The extra terms serve a purpose: $P_{ij}$ ensures $\KL(P\Vert M)$ is minimized iff $P=M$, and $M_{ij}$ guarantees non-negativity.</p> <p>Thus, the problem we want to solve is:</p> \[\min_{P \in \B} \KL(P\Vert M) \quad (S)\] <p><strong>Crucially, $P \mapsto \KL(P\Vert M)$ is strictly convex and $\B$ is convex, meaning that $(S)$ has a unique solution!</strong></p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/proj_kl-480.webp 480w,/assets/img/posts/fused_and_furious/proj_kl-800.webp 800w,/assets/img/posts/fused_and_furious/proj_kl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/proj_kl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 4.</b> Two ways to project a matrix $M$ onto Birkhoff's polytope: using the L2 norm, which amounts to finding a Euclidian geodesic and yields linear programming problem, or using the generalized KL divergence, which means finding a geodesic for the generalized KL divergence and is solved with Sinkhorn's algorithm. </div> <h3 id="iib-deriving-the-algorithm">II.B. Deriving the algorithm</h3> <p>Now that we’ve defined the projection as solving $(S)$, let’s derive Sinkhorn’s algorithm. We’ll tackle this problem just like any optimization problem, using Lagrange’s multipliers.</p> <p>First, we introduce the Lagrangian:</p> \[\mathcal{L}(P,\mathbf{f},\mathbf{g})=\KL(P\Vert M) + \sum_i f_i\big(\sum_j P_{ij}-1\big) + \sum_j g_j\big(\sum_i P_{ij}-1\big)\] <p>where $\mathbf{f}, \mathbf{g} \in \R^n$ are Lagrange multipliers.</p> <p>Then, solving for $\frac{\partial \mathcal{L}}{\partial P_{ij}}=0$ yields $P_{ij}=e^{-f_i} M_{ij} e^{-g_j}=u_i M_{ij} v_j$ where we introduced $\mathbf{u}=\exp{(-\mathbf{f})}$ and $\mathbf{v}=\exp{(-\mathbf{g})}$.</p> <p>Next, plugging this into $\frac{\partial \mathcal{L}}{\partial f_i}=0$ yields $u_i = 1 / (\sum_j P_{ij}v_j)$, i.e. $\mathbf{u}=1 \oslash (M\mathbf{v})$.</p> <p>Likewise, we get $\mathbf{v}=1 \oslash (M^T\mathbf{u})$.</p> <p>Finally, we know that the (unique!) solution of $(S)$ is of the form</p> \[P= \text{diag}(\mathbf{u}) M \text{diag}(\mathbf{v})\] <p>where $\mathbf{u}, \mathbf{v}$ satisfy the above equations.</p> <p>This leads to the following iterative algorithm, known as Sinkhorn’s algorithm<sup id="fnref:stability"><a href="#fn:stability" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>:</p> \[\begin{array}{l} \hline \textbf{Algorithm: } \text{Sinkhorn} \\ \hline \text{1. Initialize } \mathbf{u} \leftarrow \mathbf{1}_n, \mathbf{v} \leftarrow \mathbf{1}_n \\ \text{2. }\textbf{for } k = 1 \text{ to } n_{iter} \textbf{ do}: \\ \quad \quad \mathbf{u} \leftarrow 1 \oslash (M\mathbf{v}) \\ \quad \quad \mathbf{v} \leftarrow 1 \oslash (M^T\mathbf{u}) \\ \text{3. }\textbf{return } P = \text{diag}(\mathbf{u}) M \text{diag}(\mathbf{v}) \\ \hline \end{array}\] <p>Note that $n_{iter}$ is a hyperparameter that controls the accuracy of the projection: the larger it is, the closer $P$ is to the true projection. In practice, $n_{iter}$ is often set between 10 and 20 ($n_{iter}=20$ in mHC).</p> <h2 id="iii-triton-kernels-for-sinkhorn">III. Triton kernels for Sinkhorn</h2> <p>What’s remarkable (and indeed remarked in the age of GPU scientific computing<sup id="fnref:cuturi"><a href="#fn:cuturi" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>) about Sinkhorn’s algorithm is its simplicity: it only requires matrix-vector multiplications and element-wise operations! This makes it extremely well-suited for GPU implementations, as these operations can be efficiently parallelized between the (tens of) thousands of cores of modern GPUs.</p> <p>However, the looping nature of the algorithm introduces <strong>severe memory-boundedness</strong> if implemented naively, as each iteration requires reading and writing the entire matrix $M$ as well as the vectors $\mathbf{u}, \mathbf{v}$. This can be mitigated by <strong>fusing</strong> the algorithm into a single kernel, which helps dramatically reduce memory access overhead and improve performance.</p> <p>Indeed, if we look at the memory access pattern of the Sinkhorn algorithm, we have:</p> <table> <thead> <tr> <th style="text-align: left"><strong>Phase</strong></th> <th style="text-align: left"><strong>Read Access (words)</strong></th> <th style="text-align: left"><strong>Write Access (words)</strong></th> <th style="text-align: left"><strong>Total Access (words)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Per Iteration</strong></td> <td style="text-align: left">$2n^2 + 4n$</td> <td style="text-align: left">$2n$</td> <td style="text-align: left">$2n^2 + 6n$</td> </tr> <tr> <td style="text-align: left"><strong>Final Write</strong></td> <td style="text-align: left">$n^2 + 2n$</td> <td style="text-align: left">$n^2$</td> <td style="text-align: left">$2n^2 + 2n$</td> </tr> <tr> <td style="text-align: left"><strong>Total</strong></td> <td style="text-align: left">$n_{iter}(2n^2 + 4n) + n^2 + 2n$</td> <td style="text-align: left">$2n_{iter}n + n^2$</td> <td style="text-align: left">$n_{iter}(2n^2 + 6n) + 2n^2 + 2n$</td> </tr> </tbody> </table> <p>Thus, excluding the final write, memory access <strong>scales with $n_{iter}$</strong>, which is very inefficient.</p> <table> <thead> <tr> <th style="text-align: left"><strong>Phase</strong></th> <th style="text-align: left"><strong>FLOPS</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Per Iteration</strong></td> <td style="text-align: left">$4n^2 + 2n$</td> </tr> <tr> <td style="text-align: left"><strong>Final Write</strong></td> <td style="text-align: left">$2n^2$</td> </tr> <tr> <td style="text-align: left"><strong>Total</strong></td> <td style="text-align: left">$n_{iter}(4n^2 + 2n) + 2n^2$</td> </tr> </tbody> </table> <p>Also, if we look at FLOPS, we see that each iteration requires only $4n^2+2n$ FLOPS for $2n^2+6n$ memory accesses. With 4 bytes per word since we’re using FP32 precision, this yields an arithmetic intensity of roughly 0.5 FLOPS/byte, which is terrible. Hence, <strong>Sinkhorn’s algorithm is completely memory-bound</strong>.</p> <p>Sinkhorn’s algorithm memory-boundedness can be addressed with kernel fusion. We will now explore increasingly complex implementations based on Triton kernels<sup id="fnref:backward"><a href="#fn:backward" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>.</p> <h3 id="iiia-hardware-setup">III.A. Hardware setup</h3> <p>We’re benchmarking on a <strong>NVIDIA RTX 4000 Ada Generation</strong> (released Aug 9th, 2023) <a href="https://www.content.shi.com/cms-content/accelerator/media/pdfs/pny/pny-052124-nvidia-rtx-4000-ada.pdf">with</a>:</p> <ul> <li>20GB of VRAM (GDDR6, not HBM)</li> <li>360 GB/s memory bandwidth and 160-bit memory bus</li> <li>48 Streaming Multiprocessors (SMs)</li> <li>192 Tensor Cores (4th gen, 4 per SM)</li> <li>6144 CUDA cores (Ada architecture, 128 per SM)</li> </ul> <p>Delivering a peak performance of:</p> <ul> <li>327 TFLOPS for Tensor Cores (Achtung: that’s in FP8 and “with sparsity”<sup id="fnref:sparsity"><a href="#fn:sparsity" class="footnote" rel="footnote" role="doc-noteref">10</a></sup>)</li> <li>26 TFLOPS for CUDA Cores (FP32)</li> </ul> <p>This isn’t some fancy dual-die GB300 or whatever, but it’s still enough to do some serious benchmarking. Let’s get to it!</p> <h3 id="iiib-mini-refresher-on-memory-hierarchy">III.B. Mini-refresher on memory hierarchy</h3> <p>I’ve said that Sinkhorn’s algorithm is <strong>memory-bound</strong>. This may seem unclear if you’re not familiar with “memory hierarchy”. I’ll recap it <em>very</em> briefly here, focusing on GPUs.</p> <p>Basically, GPUs have a <strong>hierarchical memory system</strong> with different types of memory at different levels of the hierarchy. While the full theory is quite complicated, a simple binary model of GPU memory is sufficient to grasp the main challenges. Thus, you can think of GPU memory as being split in two components:</p> <ol> <li><strong>SRAM</strong> where “S” stands for “Static”, in reference to the <em>static</em> nature of the stored information (uses a 6-transistor cell).</li> <li><strong>VRAM</strong> where “V” stands for “Video” since GPUs were mostly for video games &amp; animation historically. It’s also called DRAM because it <em>is</em> DRAM i.e. it uses <em>dynamic</em> memory cells (based on capacitors which must be refreshed frequently). Whenever we talk about HBM, we’re talking about this memory<sup id="fnref:hbm"><a href="#fn:hbm" class="footnote" rel="footnote" role="doc-noteref">11</a></sup>.</li> </ol> <p>The key is that SRAM is fast but small, whereas VRAM is slow but large. <em>One</em> goal of GPU kernels is to <strong>maximize the amount of computation done in SRAM</strong> (i.e. inside the registers) and <strong>minimize the amount of data exchanged with VRAM</strong>. That’s because moving data between VRAM and SRAM is extremely slow (orders of magnitude) compared to computation inside the registers.</p> <p>Below is a table summarizing the key differences between SRAM and VRAM. <a href="#fig-5">Figure 5</a> gives a more detailed view of the memory hierarchy of the NVIDIA H100 GPU.</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left">SRAM</th> <th style="text-align: left">VRAM (“HBM”)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Location</strong></td> <td style="text-align: left">On-chip</td> <td style="text-align: left">Off-chip</td> </tr> <tr> <td style="text-align: left"><strong>Capacity</strong></td> <td style="text-align: left">Small capacity (10s-100s MBs)</td> <td style="text-align: left">Large capacity (10s-100s GBs)</td> </tr> <tr> <td style="text-align: left"><strong>Latency</strong></td> <td style="text-align: left">Fast as hell</td> <td style="text-align: left">Not so fast</td> </tr> <tr> <td style="text-align: left"><strong>Bandwidth</strong></td> <td style="text-align: left">Massive</td> <td style="text-align: left">Large</td> </tr> <tr> <td style="text-align: left"><strong>Components</strong></td> <td style="text-align: left">Registers + Shared memory + L1/L2 cache</td> <td style="text-align: left">Global memory + “local memory”</td> </tr> </tbody> </table> <p><br/></p> <div class="row justify-content-center" id="fig-5"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/mem_hierarchy-480.webp 480w,/assets/img/posts/fused_and_furious/mem_hierarchy-800.webp 800w,/assets/img/posts/fused_and_furious/mem_hierarchy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/mem_hierarchy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 5.</b> Memory hierarchy of the H100 (SXM5) GPU. Taken from <a href="https://www.aleksagordic.com/blog/matmul">this</a> outstanding blog post. </div> <p><em>Note: despite its name, “local memory” actually lives in the GPU’s VRAM. This memory is accessed when registers are full but we need more storage capacity. Register spill over into local memory is something we really want to avoid as it completely kills performance due to the slow access time of VRAM.</em></p> <p><strong>The key thing to remember is that by default, data lives in VRAM until you want to do stuff with it and then it gets loaded into SRAM for computation, then the result is written back to VRAM.</strong></p> <p>This means that if you implement things naively, each operation costs you a back and forth between VRAM and SRAM. For example, imagine you have 2 matrices $A$ and $B$ and want to compute their product $C$. Three things happen sequentially:</p> <ol> <li>$A$ and $B$ are loaded into SRAM. (VRAM → SRAM)</li> <li>The computation is done in SRAM.</li> <li>The result $C$ is written back to VRAM. (SRAM → VRAM)</li> </ol> <p>This seems fair. But now imagine you have a matrix $A$ and want to do 2 things on it: first square it, then exponentiate it. If you do it naively, you’ll have to do 4 back and forths between VRAM and SRAM:</p> <ol> <li>$A$ is loaded into SRAM. (VRAM → SRAM)</li> <li>$A$ is squared in SRAM.</li> <li>The result $A^2$ is written back to VRAM. (SRAM → VRAM)</li> <li>$A^2$ is loaded into SRAM. (VRAM → SRAM)</li> <li>$A^2$ is exponentiated in SRAM.</li> <li>The result $\exp(A^2)$ is written back to VRAM. (SRAM → VRAM)</li> </ol> <p>This is clearly suboptimal. What if we could do both operations in one go? That’s where <strong>fused kernels</strong> come in. A fused kernel is a kernel that does multiple sequential operations in the registers, hence minimizing the number of back and forths between VRAM and SRAM. This is exactly what we’re going to do in the next section.</p> <p>Also, whenever we refer to <em>registers</em>, think of it as a more granular way to refer to SRAM. In practice, the SRAM is more than just registers (it also includes shared memory &amp; L1/L2 cache) but for the purpose of this blog post, you can equate these two concepts in your mind.</p> <p>There’s a lot more to be said about memory hierarchy but we’ll stop here for now. If you want to learn more, I recommend Aleksa Gordic’s <a href="https://www.aleksagordic.com/blog/matmul">deep dive</a>.</p> <hr/> <h3 id="iiic-implementations">III.C. Implementations</h3> <p>Now that the hardware is out of the way, let’s dive into the implementations. We’ll start with a naive PyTorch implementation, then move on to the Triton kernels!</p> <h4 id="naive-pytorch-implementation">Naive PyTorch implementation</h4> <p>We begin with a simple PyTorch implementation for reference.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">set_float32_matmul_precision</span><span class="p">(</span><span class="sh">"</span><span class="s">high</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># we want to leverage Tensor Cores!
</span>
<span class="k">def</span> <span class="nf">sinkhorn_pytorch</span><span class="p">(</span>
    <span class="n">log_M</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># logits
</span>    <span class="n">n_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># increase for better convergence
</span>    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># numerical stability
</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">PyTorch baseline for comparison with Triton kernels.</span><span class="sh">"""</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">log_M</span><span class="p">)</span>
    <span class="n">M_T</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># free transpose (view trick)
</span>    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="n">shape</span>

    <span class="c1"># initialize scalers
</span>    <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">M</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">M</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># loop
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">M</span> <span class="o">@</span> <span class="n">v</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># row normalization
</span>        <span class="n">v</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">M_T</span> <span class="o">@</span> <span class="n">u</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># column normalization
</span>
    <span class="c1"># final scaled matrix
</span>    <span class="k">return</span> <span class="n">u</span> <span class="o">*</span> <span class="n">M</span> <span class="o">*</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p>This version is <strong>highly inefficient</strong> because $M, \mathbf{u}, \mathbf{v}$ are each read from and written to global memory at each iteration, effectively scaling memory access with $n_{iter}$. We can be smarter!</p> <p>In addition, we define a slightly more optimized baseline, which technically <em>is</em> a fused kernel but uses poor auto-fusion heuristics provided by PyTorch’s <code class="language-plaintext highlighter-rouge">torch.compile</code>. We also use <code class="language-plaintext highlighter-rouge">torch.inference_mode</code> to drop the computational graph and tensor version tracking since we’re only interested in the forward pass. Thus, this second baseline has no unfair disadvantage compared to the Triton kernels coming next.</p> <p>Formally, we construct this second baseline as a wrapper of the first one:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sinkhorn_pytorch_compiled</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">()(</span><span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">sinkhorn_pytorch</span><span class="p">))</span>
</code></pre></div></div> <p>This second version is still inefficient, but it’s a good sanity check to ensure that our Triton kernels are actually faster.</p> <h4 id="basic-fused-kernel">Basic fused kernel</h4> <p>Currently we have two types of objects doing back and forths between global memory and registers:</p> <ol> <li>vector scalers $\mathbf{u}$, $\mathbf{v}$ of size $n$</li> <li>matrix $M$ of size $(n,n)$</li> </ol> <p>In my first kernel attempt, I decided to have $\mathbf{u}$ and $\mathbf{v}$ live in registers but keep $M$ in global memory for now. This gave a kernel looking something like the pseudo-code below:</p> \[\begin{array}{l} \hline \textbf{Algorithm: } \text{Fused Sinkhorn (Global Memory Access)} \\ \hline \textbf{Input: } M \in \mathbb{R}^{n \times n} \text{ (Global Memory)} \\ \textbf{Output: } P \in \mathbb{R}^{n \times n} \text{ (Global Memory)} \\ \textbf{Scalers: } \mathbf{u}, \mathbf{v}, \mathbf{t} \in \mathbb{R}^n \\ \hline \text{1. Initialize scalers in registers: } \mathbf{u} \leftarrow \mathbf{1}_n, \mathbf{v} \leftarrow \mathbf{1}_n \\ \text{2. }\textbf{for } k = 1 \text{ to } n_{iter} \textbf{ do}: \\ \quad \quad \text{// Update } \mathbf{u} \text{ (Row normalization)} \\ \quad \quad \textbf{for } i = 1 \text{ to } n \textbf{ do}: \\ \quad \quad \quad \text{// Read row } M_{i,:} \text{ from global memory} \\ \quad \quad \quad u_i \leftarrow 1 / \sum_j(M_{i,j}v_j) \\ \\ \quad \quad \text{// Update } \mathbf{v} \text{ (Column normalization)} \\ \quad \quad \mathbf{t} \leftarrow \mathbf{0}_n \\ \quad \quad \textbf{for } i = 1 \text{ to } n \textbf{ do}: \\ \quad \quad \quad \text{// Read row } M_{i,:} \text{ from global memory} \\ \quad \quad \quad \mathbf{t} \leftarrow \mathbf{t} + u_i M_{i,:} \quad \text{// Accumulate scaled rows} \\ \quad \quad \mathbf{v} \leftarrow 1 \oslash (\mathbf{t} + \epsilon) \\ \\ \text{3. }\textbf{Write to Global Memory}: \\ \quad \textbf{for } i = 1 \text{ to } n \textbf{ do}: \\ \quad \quad \text{// Read row } M_{i,:} \text{ from global memory} \\ \quad \quad P_{i,:} \leftarrow u_i M_{i,:} \odot \mathbf{v} \\ \hline \end{array}\] <p>Note that we use an accumulator $\mathbf{t}$ for the column normalization step. This is to avoid reading $M$ column-by-column, which is inefficient due to strided memory access<sup id="fnref:strided"><a href="#fn:strided" class="footnote" rel="footnote" role="doc-noteref">12</a></sup> (whereas reading $M$ row-by-row offers efficient coalesced memory access).</p> <p>This first kernel is a good start as we’ve reduced the I/O bandwidth caused by $\mathbf{u}$ and $\mathbf{v}$’s back-and-forths in global memory, but $M$ is still read twice per iteration, meaning that memory access <em>still</em> scales with $n_{iter}$. We can do much better!</p> <h4 id="loading-m-in-registers">Loading $M$ in registers</h4> <p>First of all, we need a bit of context: remember that we’re using Sinkhorn’s algorithm to project $\Hres_l$ onto Birkhoff’s polytope. But <strong>$\Hres_l$ is a tiny matrix</strong> since the scaling factor $n$ of the residual stream is a small integer value. For the sake of simplicity, we’ll stick with $n=4$ like in mHC. This means that $M$ is effectively a $4\times 4$ matrix, which can easily fit in registers! Thus, we can update the kernel to have $M$ live in the registers, which will save us a lot more I/O bandwidth!</p> <p>This gives us exactly the same algorithm as above, except that $M$ is loaded in the registers at the beginning of the kernel, so we don’t need to read it from memory at every row / column normalization step! One other difference: the <code class="language-plaintext highlighter-rouge">exp</code> operation on $M$ is now done on-the-fly inside the registers, which saves one back-and-forth between global memory and registers.</p> <h4 id="block-packing">Block packing</h4> <p>The previous solution may seem optimal, but we can in fact still do <em>much</em> better!</p> <p>I realized this by experimenting on my own and observing the unoptimized PyTorch solution beating my kernel for batch sizes of 2048 and above. The reason is simple: PyTorch is smart enough to pack small matrices together whenever it can, helping the GPU better saturate its cores. More precisely, PyTorch (via cuBLAS) uses highly optimized BMM (batch matrix multiplication) kernels that process multiple matrices per thread block to saturate the SMs. In contrast, our naive kernel assigns one block per $4\times4$ matrix, leaving the vast majority of threads in the warp idle.</p> <p>So, we too need to process our matrices in batches instead of one-by-one. To do so, we can pack our $B$ matrices into</p> \[N_{block} = \bigg\lceil \frac{B}{\text{BLOCK_SIZE}} \bigg\rceil\] <p>tensors of shape $(\text{BLOCK_SIZE},n,n)$, and then process a <em>full tensor</em> per block. This technique is known as <strong>block tiling</strong>, it’s useful when the data objects you’re processing are too small to saturate the GPU’s threads. See <a href="#fig-5">Figure 5</a> for an illustration.</p> <div class="row justify-content-center" id="fig-6"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/block_packing-480.webp 480w,/assets/img/posts/fused_and_furious/block_packing-800.webp 800w,/assets/img/posts/fused_and_furious/block_packing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/block_packing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 6.</b> Block packing: instead of using one block per matrix, we pack $B$ matrices into $N_{block}$ tensors of shape $(\text{BLOCK_SIZE}, n, n)$ and process a full tensor per block. Consequently, the threads dedicated to each block are better utilized (higher occupancy). In this example, $B=8, N_{block}=2, \text{BLOCK_SIZE}=4$. </div> <p>This is much more efficient than using one block per matrix, which is what the two previous kernels are doing. The reason for that is that <strong>a block has significant resources</strong>: 4 warps by default, each with 32 threads, meaning a total of 128 threads. Thus, assigning a single $4\times 4$ matrix per block means we’re effectively dedicating 8 threads per matrix element, which is <em>way</em> too much and leads to underutilization i.e. idle threads. This in turn greatly slows down the kernel as it hinders parallelism (since the idle threads cannot be used to process other matrices).</p> <p>Choosing the right $\text{BLOCK_SIZE}$ i.e. how many matrices to pack in a single block is a bit of an art. One easy method is to use on <code class="language-plaintext highlighter-rouge">triton.autotune</code> to benchmark different values. I did so for $n=4$ and found that $\text{BLOCK_SIZE}=64$ yielded the best results.<sup id="fnref:num-warps"><a href="#fn:num-warps" class="footnote" rel="footnote" role="doc-noteref">13</a></sup></p> <p>Our last kernel thus reuses the same layout as the previous one, except that we now process a full tensor of shape $(\text{BLOCK_SIZE},n,n)$ per block. For $n=4$, we thus end up using tensors of dimension $(64, 4, 4)$.</p> <h3 id="iiid-benchmark">III.D. Benchmark</h3> <p>We have presented in total <strong>5 implementations</strong> of Sinkhorn’s algorithm:</p> <ol> <li>naive PyTorch (<code class="language-plaintext highlighter-rouge">sinkhorn_pytorch</code>)</li> <li>naive Pytorch but auto-fused and with inference mode (<code class="language-plaintext highlighter-rouge">sinkhorn_pytorch_compiled</code>)</li> <li>Triton kernel with scalers $\mathbf{u}, \mathbf{v}$ in registers but $M$ in global memory (<code class="language-plaintext highlighter-rouge">sinkhorn_A_in_global_memory</code>)</li> <li>Triton kernel with $\mathbf{u}, \mathbf{v}, M$ in registers (<code class="language-plaintext highlighter-rouge">sinkhorn_A_in_registers</code>)</li> <li>Triton kernel with $\mathbf{u}, \mathbf{v}, M$ in registers and block packing (<code class="language-plaintext highlighter-rouge">sinkhorn_A_in_registers_block_packing</code>)</li> </ol> <p><em>Note: <code class="language-plaintext highlighter-rouge">A</code> refers to the input matrix $M$, as I happened to have used different naming conventions in the code.</em></p> <p>We will now benchmark these implementations on our NVIDIA RTX 4000 Ada Generation.</p> <p>We’ll stick to $n=4$ to have simple 2D plots instead of heatmaps / 3D plots.</p> <p>As for the batch size $B$, we will sweep from $1$ to $2^{24}\sim 16M$. The reason for sweeping to such large batch sizes is twofold:</p> <ol> <li>because $\Hres_l$ is defined at the <strong>token-level</strong>, it means that for a given network layer $l$, we need as many Sinkhorn projections as we have tokens in our batch. Thus, the relevant batch size $B$ to benchmark on is the microbatch size $mbs$. Today’s pretraining pipelines commonly use microbatch sizes in the range $mbs\in[4096, 16384]$<sup id="fnref:mbs"><a href="#fn:mbs" class="footnote" rel="footnote" role="doc-noteref">14</a></sup>, which justifies benchmarking at least up to $B=16k$.</li> <li>it allows us to escape the uninteresting <em>latency-bound</em> regime and get to the much cooler <em>memory-bound</em> regime (that’s why we push $B$ much further than the $16k$ required for pretraining)</li> </ol> <h3 id="iiie-figures">III.E. Figures</h3> <h4 id="vanity-metric">Vanity metric</h4> <p><a href="#fig-7">Figure 7</a> shows the speedup of the most optimized Triton kernel (<code class="language-plaintext highlighter-rouge">sinkhorn_A_in_registers_block_packing</code>) over the compiled PyTorch implementation (<code class="language-plaintext highlighter-rouge">sinkhorn_pytorch_compiled</code>). We can see that for batch sizes below 1k, both implementations operate in the latency-bound regime, hence the speedup is roughly constant (and scales with $n_{iter}$). Past this threshold, we enter the memory-bound regime where the Triton kernel’s I/O awareness shines!</p> <p>In any case, the speedup ranges from 20x to <strong>139x</strong>, which is pretty cool!</p> <div class="row justify-content-center" id="fig-7"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/speedup-480.webp 480w,/assets/img/posts/fused_and_furious/speedup-800.webp 800w,/assets/img/posts/fused_and_furious/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 7.</b> For batch sizes below 1k, both the naive PyTorch function and the optimized Triton kernel operate in the latency-bound regime, hence the speedup is roughly constant (and scales with <code>n_iter</code>). Past this threshold, we enter the memory-bound regime where the Triton kernel's I/O awareness shines. </div> <h4 id="comparing-implementations">Comparing implementations</h4> <p>The next three figures compare the implementations across three performance metrics:</p> <ol> <li><strong>Execution time</strong> (lower is better) on <a href="#fig-8">Figure 8</a></li> <li><strong>Memory bandwidth</strong> (higher is better) on <a href="#fig-9">Figure 9</a></li> <li><strong>Compute throughput</strong> (higher is better) on <a href="#fig-10">Figure 10</a></li> </ol> <p>Note that the execution time uses the <strong>median</strong> (instead of the mean) to avoid being skewed by outliers, since kernel execution times tend to exhibit positive skew.</p> <p>We also compute the 98% confidence interval (<code class="language-plaintext highlighter-rouge">q01</code> to <code class="language-plaintext highlighter-rouge">q99</code>) to ensure the <strong>statistical significance</strong> of the results.</p> <p>Also, the memory bandwidth and compute throughput are computed as follows:</p> <ul> <li><strong>Memory bandwidth</strong> = $\frac{\text{total bytes read from/written to global memory}}{\text{median execution time}}$</li> <li><strong>Compute throughput</strong> = $\frac{\text{total FLOPS}}{\text{median execution time}}$</li> </ul> <p>Across all three metrics, we observe an order relation: <code class="language-plaintext highlighter-rouge">sinkhorn_pytorch</code> &lt; <code class="language-plaintext highlighter-rouge">sinkhorn_pytorch_compiled</code> &lt; <code class="language-plaintext highlighter-rouge">sinkhorn_A_in_global_memory</code> &lt; <code class="language-plaintext highlighter-rouge">sinkhorn_A_in_registers</code> &lt; <code class="language-plaintext highlighter-rouge">sinkhorn_A_in_registers_block_packing</code>, as expected!</p> <p>Also, we consistently witness two regimes:</p> <ol> <li>$B\ll 1k$ is the <strong>latency-bound regime</strong>: the amount of data is not sufficient to keep all the GPU’s SMs busy, and as such execution time is roughly constant. In this regime, the GPU’s throughput scales linearly with batch size as more SMs are utilized, meaning we can effectively scale memory bandwidth and compute throughput “for free” thanks to increased parallelism.</li> <li>$B\gg 1k$ is the <strong>memory-bound regime</strong>: there’s now enough data to use all the GPU’s SMs, and as such execution time now scales linearly with batch size i.e. no more “free” performance from increased parallelism. That’s why global throughput mostly plateaus beyond $B=16k$.</li> </ol> <div class="row justify-content-center" id="fig-8"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/timing-480.webp 480w,/assets/img/posts/fused_and_furious/timing-800.webp 800w,/assets/img/posts/fused_and_furious/timing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/timing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 8.</b> Comparison of median execution time for various implementations of Sinkhorn's algorithm. The shading represents the 98% confidence interval (<code>q01</code> to <code>q99</code>). For batch sizes below 1k, we operate in the latency-bound regime (i.e. some of the GPUs SMs are idle) and as such execution time is roughly constant. Past this threshold, we enter the memory-bound regime where execution time scales linearly with batch size. </div> <div class="row justify-content-center" id="fig-9"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/memory_bandwidth-480.webp 480w,/assets/img/posts/fused_and_furious/memory_bandwidth-800.webp 800w,/assets/img/posts/fused_and_furious/memory_bandwidth-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/memory_bandwidth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 9.</b> Memory bandwidth increases linearly with batch size in the latency-bound regime as we distribute the work to more SMs, then saturates in the memory-bound regime. Note the peak bandwidth of 238 GB/s, satisfyingly close to the hardware limit of 360 GB/s for this GPU. Note that the memory bandwidth decreases as <code>n_iter</code> increases, which is expected as we spend more time computing inside the registers and less time exchanging data between global memory and registers. For <code>n_iter=1</code> we get 312 GB/s peak I/O bandwidth i.e. 87% of the hardware limit. </div> <div class="row justify-content-center" id="fig-10"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/fused_and_furious/compute_throughput-480.webp 480w,/assets/img/posts/fused_and_furious/compute_throughput-800.webp 800w,/assets/img/posts/fused_and_furious/compute_throughput-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/fused_and_furious/compute_throughput.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 10.</b> Like memory bandwidth, compute throughput increases linearly with batch size in the latency-bound regime, then saturates in the memory-bound regime. Note the peak throughput of 2.7 TFLOPS, which is very far from the hardware limit of 26 TFLOPS for this GPU. This isn't surprising as Sinkhorn's algorithm is memory-bound. </div> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:mhc"> <p>Xie, Z., Wei, Y., Cao, H., Zhao, C., Deng, C., Li, J., Dai, D., Gao, H., Chang, J., Yu, K., Zhao, L., Zhou, S., Xu, Z., Zhang, Z., Zeng, W., Hu, S., Wang, Y., Yuan, J., Wang, L., &amp; Liang, W. (2025). <em>mHC: Manifold-Constrained Hyper-Connections.</em> [<a href="https://arxiv.org/abs/2512.24880">arXiv</a>] <a href="#fnref:mhc" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:hc"> <p>D. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. (2024). <em>Hyper-connections.</em> [<a href="https://arxiv.org/abs/2409.19606">arXiv</a>] <a href="#fnref:hc" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:resnet"> <p>He et al. (2015). <em>Deep Residual Learning for Image Recognition</em> [<a href="https://arxiv.org/abs/1512.03385">arXiv</a>] <a href="#fnref:resnet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:positivity"> <p>We assume $M$ has strictly positive entries. If not, we can exponentiate $M$ to ensure , which is what we do in the various proposed implementations. <a href="#fnref:positivity" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mhc-lite"> <p>This result is known as the Birkhoff-von Neumann theorem. We can use it to parametrize $M\in\B$ as a convex combination of $n!$ permutation matrices. Note that such a parametrization allows to completely bypass Sinkhorn’s algorithm while also obtaining a perfect projection. This is the approach taken by <a href="https://arxiv.org/abs/2601.05732">mHC-lite</a>, a paper written a few days after mHC. <a href="#fnref:mhc-lite" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:divergence"> <p>The (generalized) KL divergence isn’t actually a metric but a (Bregman) divergence. We omit this detail for the sake of clarity. <a href="#fnref:divergence" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:stability"> <p>Sinkhorn’s algorithm is numerically unstable for very small entries of $M$. In practice, one usually adds a small constant $\epsilon$ to $M$ to avoid division by zero. One can also work in log-space to improve numerical stability. Finally, since we want $P$ to have strictly positive entries, we must exponentiate $M$ if it has negative entries; some implementations also scale $M$ by a temperature parameter before exponentiating to control the sharpness of $P$. <a href="#fnref:stability" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:cuturi"> <p>Cuturi, M. (2013). <em>Sinkhorn Distances: Lightspeed Computation of Optimal Transport.</em> [<a href="https://arxiv.org/abs/1306.0895">arXiv</a>] <a href="#fnref:cuturi" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:backward"> <p>Although I didn’t cover it in this post, efficiently implementing the backward pass of Sinkhorn’s algorithm is non-trivial as it not only requires fused kernels, but also activation recomputation to avoid storing all intermediate $\mathbf{u}, \mathbf{v}$ vectors. I may cover this in a future post though! <a href="#fnref:backward" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sparsity"> <p>Here, “with sparsity” refers to NVIDIA’s <a href="https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/">structured sparsity</a>, a Tensor Core feature which essentially doubles your compute throughput if you have a 2:4 sparsity pattern, i.e. among each group of four contiguous values, at least two are zero. Since this feature effectively requires a 50% sparsity rate whereas neural network matrices are dense (unless you prune them for inference), I’m a bit skeptical regarding the relevance of this “with sparsity” metric. <a href="#fnref:sparsity" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:hbm"> <p>Technically, HBM is one <em>type</em> of VRAM technology, which is used in high-end AI chips e.g. NVIDIA’s Hopper architecture. But not all GPUs have their HBM VRAM. In fact, gaming GPUs using GDDR, which is much less expensive than HBM. (it’s the reason why your gaming graphics card doesn’t cost $30k) <a href="#fnref:hbm" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:strided"> <p>In practice, this doesn’t make a huge difference for small matrices (e.g. $n=4$) as the stride is tiny, but it can be significant for larger ones (e.g. $n=256$). <a href="#fnref:strided" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:num-warps"> <p>Out of curiosity, I also tuned $\text{num_warps}$ and ran a grid search on $(\text{BLOCK_SIZE},\text{num_warps})\in[64,128,256,512,1024] \times [4,8,16]$. I found that depending on the batch size $B$, different configurations yielded the best results. I chose to omit this ablation here for the sake of simplicity. <a href="#fnref:num-warps" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mbs"> <p>Here we define the microbatch size as $mbs={seq\_per\_batch}\times{seq\_len}$, where we commonly have $seq\_per\_batch \in [1,2,4]$ and $seq\_len \in [4096, 8192, 16,384]$ in pretraining pipelines. <a href="#fnref:mbs" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="llm,"/><category term="pretraining,"/><category term="research"/><summary type="html"><![CDATA[TL;DR: DeepSeek's recent mHC paper relies on Sinkhorn's algorithm to project matrices onto Birkhoff's polytope. The looping nature of the algorithm introduces severe memory-boundedness, which can be mitigated by fusing the algorithm into a single kernel. We implement increasingly fast versions of the algorithm in Triton.]]></summary></entry><entry><title type="html">Residual Matrix Transformers</title><link href="https://gaetanx21.github.io/blog/residual-matrix-transformer/" rel="alternate" type="text/html" title="Residual Matrix Transformers"/><published>2025-11-11T00:00:00+00:00</published><updated>2025-11-11T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/residual-matrix-transformer</id><content type="html" xml:base="https://gaetanx21.github.io/blog/residual-matrix-transformer/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\X}{\mathbb{X}}\] <p>A colleague at work recently shared a paper<sup id="fnref:paper"><a href="#fn:paper" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> from ICML 2025 entitled <em>Residual Matrix Transformers: Scaling the Size of the Residual Stream</em>. I found it so interesting that I decided to write a short summary of the main ideas.</p> <p>The starting point of the paper is the observation that, as we scale up transformers, model parameters and FLOPs scale <em>quadratically</em> while the residual stream width scales <em>linearly</em>. Hence, as we keep increasing model size, the residual stream may become a bottleneck. The authors’ solution is to replace the vector token representation with a matrix representation inspired by outer-product memories. The resulting <strong>Residual Matrix Transformer</strong> (RMT) architecture is more efficient in terms of parameters and FLOPs while outperforming the standard transformer on downstream evaluations after pretraining. Additionally, the size of the residual stream can be increased without (significantly) increasing the number of parameters or FLOPs, effectively creating a new scaling dimension with its own scaling law. Finally, the RMT exhibits improved variance propagation properties.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/residual_matrix_transformer/rmt-wide-480.webp 480w,/assets/img/posts/residual_matrix_transformer/rmt-wide-800.webp 800w,/assets/img/posts/residual_matrix_transformer/rmt-wide-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/residual_matrix_transformer/rmt-wide.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. To read features from the residual stream, the standard transformer computes (learned) linear transformations of the residual stream vector, whereas the Residual Matrix Transformer uses (learned) key vectors applied to the residual stream matrix. </div> <h2 id="i-outer-product-memory">I. Outer-product memory</h2> <p>It may at first seem arbitrary to replace the vector token representation with a matrix representation. However, this design choice makes sense if we interpret the residual stream as an <strong>outer-product memory store</strong>.</p> <p>An outer-product memory is a type of <em>content-addressable memory</em> (or <em>associative memory</em>), i.e. a memory model where data is accessed not with a specific address (e.g. give me the item at position 4) but with a content-based query (e.g. give me the word closest to “appl”). This is loosely inspired by how the human brain works, where we associate certain cues with specific memories (e.g. if you think of potatoes, you won’t be accessing a specific “potato” address in your brain, but rather you’ll recall a flurry of memories associated with potatoes).</p> <p>In the case of outer-product memories, we want to store a set of $n$ key-value pairs $\lbrace k_i, v_i \rbrace_{i=1}^n$ where $k_i\in\R^{D_k}, v_i\in\R^{D_v}$. We store these pairs in a matrix $M \in \R^{D_k \times D_v}$ defined as</p> \[M = \sum_{i=1}^n k_i \otimes v_i\] <p>where $\otimes$ denotes the outer product i.e. $k_i \otimes v_i = k_i v_i^\top$.</p> <p>Then, to retrieve the value $v_p$ associated with the key $k_p$, we compute the dot product $k_p \cdot_1 M$ where $\cdot_1$ denotes the dot product (or “tensor contraction”) along the first dimension. We thus have</p> \[\hat{v}_p = k_p \cdot_1 M =\sum_{i=1}^n (k_p \cdot k_i) v_i = v_p + \text{cross-talk noise}\] <p>where we hope that the keys $k_i$ are sufficiently orthogonal to each other so that cross-talk noise is negligible.</p> <p>Thus, the outer-product memory $M$ has two operations:</p> <ul> <li><code class="language-plaintext highlighter-rouge">READ</code>: $v \leftarrow k \cdot_1 M$</li> <li><code class="language-plaintext highlighter-rouge">WRITE</code>: $M \leftarrow M + k \otimes v$</li> </ul> <p>The RMT paper uses this outer-product memory as a building block to construct a matrix residual stream.</p> <h2 id="ii-model-architecture">II. Model Architecture</h2> <p>Let’s now see in practice how the RMT implements the two main transformer operations: the attention block and the feedforward block.</p> <p>We will consider a single sequence of $N$ tokens, thus the residual stream is:</p> <ul> <li>a matrix $X \in \R^{D_{model} \times N}$ for the standard transformer</li> <li>a tensor $\X \in \R^{D_k \times D_v \times N}$ for the RMT</li> </ul> <h3 id="a-attention-block">A. Attention Block</h3> <p>For the attention block, there are two differences between the standard transformer and the RMT:</p> <ol> <li>How information is <strong>read</strong> from the residual stream to compute the query, key and value matrices.</li> <li>How the output of the attention block is <strong>written</strong> to the residual stream.</li> </ol> <h4 id="reading-information-from-the-residual-stream">Reading information from the residual stream</h4> <p>In the standard transformer, the query, key and value matrices for each attention head are computed as follows:</p> \[Q^h = W_Q^h X, \quad K^h = W_K^h X, \quad V^h = W_V^h X\] <p>where $W_Q^h, W_K^h, W_V^h \in \R^{D_h \times D_{model}}$ are learnable matrices.</p> <p>In the RMT, the query, key and value matrices are computed as follows:</p> \[Q^h = r_Q^h \cdot_1 \X, \quad K^h = r_K^h \cdot_1 \X, \quad V^h = r_V^h \cdot_1 \X\] <p>where $r_Q^h, r_K^h \in \R^{D_k}$ and $r_V^h \in \R^{D_v}$ are learnable vectors.</p> <p>Thus, the key, query, and value matrices are obtained</p> <ul> <li>with a <em>linear transformation</em> of the residual stream matrix $X$ in the standard transformer</li> <li>with a <code class="language-plaintext highlighter-rouge">READ</code> operation on the residual stream tensor $\X$ in the RMT</li> </ul> <p><em>Note that once the key, query, and value matrices are computed, single-head attention is computed in the same way in both models. In particular, this means that the KV-cache is the same in both models (so, no explosion of memory usage because of the matrix residual stream).</em></p> <h4 id="writing-information-to-the-residual-stream">Writing information to the residual stream</h4> <p>In the standard transformer, the output of the multihead attention block is computed as follows:</p> \[\text{MHA}(X) = \sum_{h=1}^H W_{out}^h \text{SHA}(Q^h, K^h, V^h) \in \R^{D_{model} \times N}\] <p>where $W_{out}^h \in \R^{D_{model} \times D_h}$ is a learnable matrix.</p> <p>In the RMT, the output of the attention block is computed as follows:</p> \[\text{MHA}(\X) = \sum_{h=1}^R w_{out}^h \otimes \text{SHA}(Q^h, K^h, V^h) \in \R^{D_k \times D_v \times N}\] <p>where $w_{out}^h \in \R^{D_k}$ is a learnable vector and $R$ is a hyperparameter<sup id="fnref:R"><a href="#fn:R" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.</p> <p>Thus, once the output of each attention head $\text{SHA}(Q^h, K^h, V^h)$ is computed, it is written back to the residual stream</p> <ul> <li>with a <em>concatenation then linear transformation</em> of the $H$ heads in the standard transformer</li> <li>with a <code class="language-plaintext highlighter-rouge">WRITE</code> operation for each individual head in the RMT</li> </ul> <p><em>In particular, notice how scaling $D_k$ and $D_v$ in the RMT results in a linear increase in the number of parameters and FLOPs, while scaling $D_{model}$ in the standard transformer results in a quadratic increase in the number of parameters and FLOPs. This is because we’ve replaced the matrices for reading from ($W_Q, W_K, W_V$) &amp; writing to ($W_{out}$) the residual stream with vectors for <code class="language-plaintext highlighter-rouge">READ</code> ($r_Q, r_K, r_V$) and <code class="language-plaintext highlighter-rouge">WRITE</code> ($w_{out}$) operations.</em></p> <h3 id="b-feedforward-block">B. Feedforward Block</h3> <p>We could be tempted to apply the same treatment to the feedforward block. That is, use vectors instead of matrices to read from &amp; write to the residual stream. However — as the authors point out — there is strong evidence that the feedforward weights actually store factual information in the transformer<sup id="fnref:interpretation"><a href="#fn:interpretation" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, so we don’t want to replace them with vectors!</p> <p>The authors’ solution is to transform $\X$ a bit to recover the standard feedforward block. Specifically, we retrieve $R$ data vectors from $\X$ using <code class="language-plaintext highlighter-rouge">READ</code> operations and concatenate them to form a matrix</p> \[X_{FF} = \text{concat}_{1\leq h\leq R}(r_{FF}^h \cdot_1 \X) \in \R^{RD_v \times N}\] <p>which is then fed to the feedforward block. The output $\text{FF} _ {standard}(X_{FF})\in\R^{RD_v \times N}$ of the standard feedforward block is then reshaped into $R$ matrices with the $\text{unvec}_1$ operation along the first dimension, and each of these matrices is written in the residual stream using <code class="language-plaintext highlighter-rouge">WRITE</code> operations with the key vectors $w _ {FF}^h \in \R^{D_k}$, such that the final output of the feedforward block is</p> \[\text{FF}(\X) = \sum_{h=1}^R w_{FF}^h \otimes \text{unvec} _ 1(\text{FF}_{standard}(X_{FF}))_h \in \R^{D_k \times D_v \times N}\] <h2 id="iii-theoretical-properties">III. Theoretical properties</h2> <p>I’ll go quickly here, more details can be found in the paper. There are two key theoretical properties of the RMT:</p> <ol> <li><strong>Resource scaling</strong>: we can scale the size of the residual stream without significantly increasing the number of parameters or FLOPs, thus creating a new scaling dimension with its own scaling law.</li> <li><strong>Variance propagation</strong>: the RMT has better variance propagation properties than the standard transformer, i.e. the analysis of retrieval and storage operations shows the variance of activations &amp; gradients is better preserved in the RMT than in the standard transformer. (i.e. both $\frac{\sigma^2 _ {x_{out}}}{\sigma^2 _ {x_{in}}}$ and $\frac{\sigma^2 _ {g_{out}}}{\sigma^2 _ {g_{in}}}$ are closer to $1$ in the RMT than in the standard transformer)</li> </ol> <h2 id="iv-pretraining-performance">IV. Pretraining performance</h2> <p>The authors conducted several pretraining experiments (with models ranging from 46M to 405M parameters, arguably small by today’s standards) to compare the RMT to the standard transformer. They found that the RMT is:</p> <ol> <li>more parameter efficient (because the read/write weight matrices are replaced with key vectors)</li> <li>more data efficient (i.e. faster convergence on a train-token basis)</li> <li>more compute efficient (essentially a corollary of the above two points)</li> <li>more memory efficient (same footprint during training, lower footprint at inference<sup id="fnref:memory"><a href="#fn:memory" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>)</li> <li>slightly worse at runtime (however, this is largely due to the current suboptimal implementation of the RMT)</li> </ol> <h2 id="v-conclusion">V. Conclusion</h2> <p>Overall, the Residual Matrix Transformer feels as significant as Mixture of Experts models (MoEs) in the sense that it offers a new dimension along which to scale transformers. On a more personal level, I also find the interpretation of the residual stream matrix as an outer-product memory very elegant and satisfying!</p> <p>Also, unlike (somewhat) radical transformer alternatives like State-Space Models or Recurrent Neural Networks, the RMT architecture is close enough to the standard transformer that we can actually hope to see it implemented in the near future! It would be interesting to see whether this residual matrix approach holds up at the billion (and trillion) parameter scale, enabling us to further scale transformers.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:paper"> <p>Mak, B., &amp; Flanigan, J. (2025). <em>Residual Matrix Transformers: Scaling the Size of the Residual Stream.</em> [<a href="https://arxiv.org/abs/2506.22696">arXiv</a>] <a href="#fnref:paper" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:R"> <p>$R$ is not simply the number of heads, but more generally the number of <code class="language-plaintext highlighter-rouge">WRITE</code> operations in the attention layers and <code class="language-plaintext highlighter-rouge">READ</code> operations in the feedforward layers. <a href="#fnref:R" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:interpretation"> <p>The common interpretation is that attention layers allow tokens to talk to each other (communication step) while feedforward layers let tokens think on their own after having communicated (processing step). <a href="#fnref:interpretation" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:memory"> <p>During training, the increased size of the residual stream directly translates into more memory usage because of larger gradient checkpoints, but this memory cost is offset by the reduced model size. At inference time, we don’t checkpoint gradients anymore and thus RMT actually becomes more memory efficient because of its smaller size. <a href="#fnref:memory" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="llm,"/><category term="pretraining,"/><category term="research"/><summary type="html"><![CDATA[TL;DR: As we increase the size of (standard) transformers, parameters and FLOPs scale quadratically, but the residual stream width scales linearly. Thus, the residual stream can become a bottleneck as we scale up. We discuss the RMT paper, which proposes a matrix residual stream to address this issue.]]></summary></entry><entry><title type="html">Filtrations demystified</title><link href="https://gaetanx21.github.io/blog/filtrations/" rel="alternate" type="text/html" title="Filtrations demystified"/><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/filtrations</id><content type="html" xml:base="https://gaetanx21.github.io/blog/filtrations/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}} \newcommand{\A}{\mathcal{A}} \newcommand{\P}{\mathbb{P}} \newcommand{\T}{\mathbb{T}} \newcommand{\F}{\mathcal{F}} \newcommand{\FF}{\mathbb{F}}\] <p>If you’ve ever studied stochastic processes<sup id="fnref:random-process"><a href="#fn:random-process" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> in a formal setting, you must have come across the concept of <em>filtration</em>. I remember my stochastic calculus course where the professor introduced filtrations in a very abstract way, which left me quite confused. He then added that “filtrations encode the information available at each time step”, which was a bit more intuitive but still quite vague. Anyway, this was how I understood filtrations for quite some time, until one day I decided to dig deeper into the topic and really understand what filtrations are and why they are useful. This post aims to share this understanding!</p> <p><strong>NB:</strong> I will assume that you are familiar with <em>σ-algebras</em>. If not, I recommend reading my previous post on <a href="/blog/measurability/">measurability</a> before continuing!</p> <hr/> <h2 id="i-motivation">I. Motivation</h2> <p>When dealing with basic random variables, we simply need:</p> <ol> <li>a probability space $(\Omega, \A, \P)$,</li> <li>a random variable $X: \Omega \to E$, where $(E, \mathcal{E})$ is another measurable space.</li> </ol> <p>This setup is sufficient for many applications in probability theory.</p> <p>However, defining stochastic processes requires a subtler formalism. The core reason is that stochastic processes are collections of random variables indexed by time<sup id="fnref:indexing"><a href="#fn:indexing" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, and as such the information contained in the process accumulates over time, which must be accounted for in the formalism. Indeed, just like we need σ-algebras to define which events are measurable for a random variable $X$, we need a mathematical object to define which events are measurable <em>at a given time $t$</em> for a stochastic process $(X_t) _ {t\in\T}$. This mathematical object is called a <em>filtration</em>.</p> <h2 id="ii-defining-filtrations">II. Defining filtrations</h2> <h3 id="a-intuitive-definition">A. Intuitive definition</h3> <p>Before delving into the formal definition of filtrations, I think the <a href="https://en.wikipedia.org/wiki/Filtration_(probability_theory)">Wikipedia page</a> sums it up quite nicely:</p> <blockquote> <p>“In the theory of stochastic processes, a subdiscipline of probability theory, <strong>filtrations are totally ordered collections of subsets that are used to model the information that is available at a given point</strong> and therefore play an important role in the formalization of random (stochastic) processes.”</p> </blockquote> <h3 id="b-formal-definition">B. Formal definition</h3> <p>Formally, let $(\Omega, \A, \P)$ be a probability space and $\T$ be a totally ordered set (typically $\N$ or $\R_+$).</p> <p>A <em>filtration</em> is a family of sub-σ-algebras $(\F_t)_{t\in\T}$ of $\A$ such that for all $s, t \in \T$ with $s \leq t$, we have $\F_s \subseteq \F_t (\subseteq \A)$. In other words, the σ-algebras are nested and non-decreasing over time. We often denote a filtration with the symbol $\FF$.</p> <p>If $(\Omega, \A, \P)$ is a probability space and $\FF=(\F_t)_{t\in\T}$ is a filtration on this space, then the quadruplet $(\Omega, \A, \FF, \P)$ is called a <em>filtered probability space</em>.</p> <p>Importantly, note that we do not need stochastic processes to define filtrations. A filtration $\FF$ can be defined on any probability space, regardless of whether it is associated with a stochastic process or not. However, filtrations are almost always constructed in relation to a stochastic process, using what we call the <em>natural filtration</em> of the process.</p> <h3 id="c-natural-filtration">C. Natural filtration</h3> <p>Given a stochastic process $(X_t) _ {t\in\T}$, its natural filtration is defined as $\F_t = \sigma(X_s | s \leq t)$ for each $t \in \T$. In other words, $\F_t$ is the smallest σ-algebra that makes all random variables $X_s$ for $s \leq t$ measurable. Intuitively, $\F_t$ declares all the information about the process <strong>available</strong> at time $t$.</p> <h3 id="d-available-information--information">D. Available information ≠ information</h3> <p>I really want to emphasize the difference between <em>available information</em> and (actual) <em>information</em>, which is often the source of confusion when dealing with filtrations.</p> <p>Let’s consider a stochastic process $(X_t) _ {t\in\T}$ with its natural filtration $\FF=(\F_t)_{t\in\T}$.</p> <p>Interpreting $\F_t$ as containing the information $(X_s)_{s\leq t}$ is <strong>incorrect</strong>. That is, $\F_t$ <em>does not</em> contain the realized values of $X_s$ for $s \leq t$. If this isn’t clear, remark that —by construction— the σ-algebra $\F_t$ will be identical regardless of the specific values taken by the random variables $X_s$ for $s \leq t$; thus, it cannot possibly encode this information.<sup id="fnref:filtration-independent"><a href="#fn:filtration-independent" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> Instead, $\F_t$ declares explicitly which information about the process is <strong>available</strong> at time $t$, without holding the information in itself!</p> <p>The key insight —which is not so straightforward in my opinion— is that being <em>measurable</em> with respect to $\F_t$ means being <em>determinable</em> using the information available at time $t$. In other words, if a random variable $Z$ is $\F_t$-measurable, then at time $t$ we will be able to determine its value. In particular, for any statement $S$ about the process, if $Y=\mathbf{1}_S$ is $\F_t$-measurable, then at time $t$ we will know with certainty whether $S$ is true or false. Once again, the information about the veracity of $S$ <em>is not</em> contained in $\F_t$; $\F_t$ simply declares that we will know if $S$ is true or false at time $t$.</p> <p>The above intuition can be formalized as follows:</p> <blockquote> <p>Consider a random process $(X_t) _ {t\in\T}$ and its natural filtration $\FF=(\F_t) _ {t\in\T}$. Let $t\in\T$ be a specific point in time, and $A\in\F_t$ a $\F_t$-measurable event. Consider $\omega\in\Omega$ an arbitrary realization. Then, at time $t$, we will know deterministically whether $\omega\in A$ or not.</p> </blockquote> <p>This is what it means when we say that $\F_t$ encodes the <em>available</em> information at time $t$.</p> <h2 id="iii-examples">III. Examples</h2> <p>At this point, you may still be a bit confused about what filtrations really are. A few examples should help clarify things!</p> <h3 id="a-coin-tosses">A. Coin tosses</h3> <p>Consider a sequence of two consecutive coin tosses, modeled by the stochastic process $(X_1, X_2)$ (i.e. $\T=\lbrace 1, 2\rbrace$) where $X_i$ is the outcome of the $i$-th toss ($H$ or $T$).</p> <ol> <li>The sample space is $\Omega = \lbrace HH, HT, TH, TT\rbrace$.</li> <li>The σ-algebra is $\A = \mathcal{P}(\Omega)$ (the power set of $\Omega$).</li> <li>The probability measure $\P$ assigns a probability of $1/4$ to each outcome.</li> </ol> <p>Now let’s think about the natural filtration $\FF$ of this process.</p> <ol> <li>At time $t=0$ (before any toss), we have $\F_0 = \big\lbrace\emptyset, \Omega\big\rbrace$, i.e. there is no information available whatsoever about the outcomes of the tosses.</li> <li>At time $t=1$ (after the first toss), we have $\F_1 = \sigma(X_1) = \big\lbrace\emptyset, \Omega, \lbrace HH, HT\rbrace, \lbrace TH, TT\rbrace\big\rbrace$, i.e. we now know the outcome of the first toss, but not the second. Indeed, after observing the first toss, we can distinguish between the events <em>“first toss is H”</em> ($\lbrace HH, HT\rbrace$) and <em>“first toss is T”</em> ($\lbrace TH, TT\rbrace$). In other words, we can tell whether our realization $\omega$ lies in $\lbrace HH, HT\rbrace$ or in $\lbrace TH, TT\rbrace$.</li> <li>At time $t=2$ (after the second toss), we have \(\begin{aligned} \F_2 &amp;=\sigma(X_1, X_2) \\ &amp;=\big\lbrace \emptyset, \Omega, \lbrace HH \rbrace, \lbrace HT \rbrace, \lbrace TH \rbrace, \lbrace TT \rbrace, \lbrace HH, HT \rbrace, \lbrace HH, TH \rbrace, \lbrace HH, TT \rbrace, \lbrace HT, TH \rbrace, \lbrace HT, TT \rbrace, \lbrace TH, TT \rbrace, \lbrace HH \rbrace^c, \lbrace HT \rbrace^c, \lbrace TH \rbrace^c, \lbrace TT \rbrace^c \big\rbrace \\ &amp;=\mathcal{P}(\Omega) \end{aligned}\)</li> </ol> <p>i.e. we know the outcomes of both tosses, so we can distinguish between all possible outcomes.</p> <h3 id="b-academic-outcomes">B. Academic outcomes</h3> <p>We can make a more concrete version of the previous example. Let’s consider a world in which students can go to two universities: Stanford ($S$) or Carnegie Mellon ($C$). After graduating, they can either get a job ($J$) or pursue a PhD ($P$). We can model this situation with a stochastic process $(X_1, X_2)$ where $X_1$ is the university attended and $X_2$ is the post-graduation outcome.</p> <p>The sample space is $\Omega = \lbrace SJ, SP, CJ, CP\rbrace$, the σ-algebra is $\A = \mathcal{P}(\Omega)$, and we can define a uniform probability measure $\P$ for simplicity.</p> <p>The filtration $\FF$ of this process is as follows:</p> <ol> <li>At time $t=0$ (before university), we have $\F_0 = \big\lbrace\emptyset, \Omega\big\rbrace$, i.e. there is no information available about the student’s academic outcome.</li> <li>At time $t=1$ (after university), we have $\F_1 = \sigma(X_1) = \big\lbrace\emptyset, \Omega, \lbrace SJ, SP\rbrace, \lbrace CJ, CP\rbrace\big\rbrace$, i.e. we now know which university the student attended, but not their post-graduation outcome.</li> <li>At time $t=2$ (after graduation), we have $\F_2 = \sigma(X_1, X_2) = \mathcal{P}(\Omega)$, i.e. we know the student’s entire academic outcome.</li> </ol> <h3 id="c-brownian-motion">C. Brownian motion</h3> <p>I’ll assume that you are familiar with Brownian motion. If not, check out the Wikipedia page on the <a href="https://en.wikipedia.org/wiki/Wiener_process">Wiener process</a>.</p> <p>For the one-dimensional Brownian motion $(B_t)_{t\geq 0}$:</p> <ol> <li>The sample space is $\Omega = \R^\T$<sup id="fnref:continuous-paths"><a href="#fn:continuous-paths" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> with $\T = [0, +\infty)$.</li> <li>The σ-algebra is $\A = \sigma(B_t : t \geq 0)$ (the σ-algebra generated by the Brownian motion).</li> <li>The probability measure $\P$ is (by definition) the Wiener measure.</li> </ol> <p>Unlike the previous examples, $(B_t) _ {t\geq 0}$ is a <em>continuous-time</em> stochastic process, so we cannot explicitly write down the filtration $\FF=(\F_t)_{t\geq 0}$, except for $\F_0 = \lbrace\emptyset, \Omega\rbrace$. However, $\F_t$ still represents the information available about the Brownian motion up to time $t$. For instance, the event $A = (B_1 &gt; 0)$ is such that:</p> <ol> <li>$A \notin \F_t$ for all $t &lt; 1$ i.e. before time $t=1$ we cannot know whether $B_1$ is positive or not</li> <li>$A \in \F_t$ for all $t \geq 1$ i.e. at time $t=1$ and afterwards we can know for sure whether $B_1$ is positive or not.</li> </ol> <p>Although it’s somewhat less straightforward to describe what filtrations look like for continuous-time processes, the key idea remains the same: $\F_t$ encodes all the information that <em>can be known for sure</em> about the process at time $t$.</p> <h2 id="conclusion">Conclusion</h2> <p>I’ve tried to make this post as intuitive as possible, and hopefully the examples helped in that regard. Filtrations do not need to be complicated, they are simply a way to formalize the idea of accumulating <strong>available</strong> information over time in the context of stochastic processes. Yes, you can live without knowing the full truth behind filtrations, but beyond a certain point, it’s worth deeply understanding the mathematical objects you manipulate.</p> <p>In machine learning, many people use quite complicated models without really grasping the underlying mathematics<sup id="fnref:diffusion"><a href="#fn:diffusion" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>. Of course, this is fine to a certain extent, but I firmly believe that having an intuitive understanding of the theory is key whenever you want to venture <em>just a tiny bit</em> off the beaten path.</p> <p>Finally, I must mention that filtrations exist outside the realm of probability theory. In essence, given a set $X$, any family of nested subsets of $X$ is a filtration; in particular, these subsets <em>do not</em> need to be σ-algebras. One typical use case in linear algebra is to simplify the study an infinite-dimensional vector space $V$ by instead considering an increasing sequence $(V_t) _ {t\in\N}$ of finite-dimensional vector spaces such that $\bigcup_{t\in\N} V_t = V$. This is called a <em>filtration of $V$</em>.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:random-process"> <p>Stochastic processes are equivalently called random processes in some literature. <a href="#fnref:random-process" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:indexing"> <p>More precisely, the index $\T$ can be any totally ordered set, but in most cases we have $\T=\N$ or $\T=\R_+$ which are interpreted as discrete and continuous time respectively. <a href="#fnref:indexing" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:filtration-independent"> <p>Another way to put it is that the filtration $\FF$ is defined purely in terms of the σ-algebras generated by the random variables $(X_t) _ {t\in\T}$, and not their realizations. Thus, $\FF$ is independent of the specific outcome $\omega\in\Omega$. This clearly shows that $\F_t$ cannot contain the actual information about the values taken by $(X_s) _ {s\leq t}$. <a href="#fnref:filtration-independent" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:continuous-paths"> <p>In practice, one can prove that the sample paths $B(\omega,\cdot): t \mapsto B(\omega,t)$ are almost surely continuous, so we could restrict $\Omega$ to $\mathcal{C}(\T,\R)$ without loss of generality. <a href="#fnref:continuous-paths" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:diffusion"> <p>For instance, it’s easy to overlook that diffusion modeling amounts to solving a time-reversed stochastic differential equation, yet seeing diffusion under the light of SDEs yields <a href="https://yang-song.net/blog/2021/score/">fruitful insights</a>. <a href="#fnref:diffusion" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="stochastic-processes"/><summary type="html"><![CDATA[TL;DR: Filtrations are a key ingredient in defining stochastic processes and modeling the accumulation of available information over time. Filtrations are also often poorly understood; this post aims to demystify them.]]></summary></entry><entry><title type="html">Measurability and σ-algebras</title><link href="https://gaetanx21.github.io/blog/measurability/" rel="alternate" type="text/html" title="Measurability and σ-algebras"/><published>2025-10-04T00:00:00+00:00</published><updated>2025-10-04T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/measurability</id><content type="html" xml:base="https://gaetanx21.github.io/blog/measurability/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}} \newcommand{\A}{\mathcal{A}} \newcommand{\P}{\mathbb{P}}\] <p>Whenever we want to do probabilities —which is basically all the time in the context of probabilistic machine learning— we need to define a probability space.</p> <p>A probability space is a mathematical object that consists of three components: a sample space $\Omega$, a σ-algebra $\A$ over $\Omega$, and a probability measure $\P$ defined on $\A$. It is commonly denoted as the triplet $(\Omega, \A, \P)$.</p> <p>While the concepts of sample space and probability measure are relatively intuitive, σ-algebras are often more elusive and harder to grasp. The goal of this post is to demystify σ-algebras by revisiting the foundational concept of <em>measurability</em><sup id="fnref:measure-wiki"><a href="#fn:measure-wiki" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <hr/> <h2 id="i-measurability">I. Measurability</h2> <h3 id="a-defining-measure">A. Defining measure</h3> <p>Before we can understand σ-algebras, we need to understand what a <em>measure</em> is.</p> <blockquote> <p>Simply put, a measure is a function that assigns a number to a set in a way that generalizes the concepts of <em>length, area, and volume</em> to arbitrary sets and dimensions.</p> </blockquote> <p>Formally, if $X$ is a set and $\Sigma$ a σ-algebra over $X$<sup id="fnref:sigma-algebra"><a href="#fn:sigma-algebra" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, a measure is a function $\mu: \Sigma \to [0, +\infty]$ such that:</p> <ol> <li>$\mu(\emptyset) = 0$ (the measure of the empty set is zero),</li> <li><strong>non-negativity</strong>: for all $A \in \Sigma$, $\mu(A) \geq 0$,</li> <li><strong>σ-additivity</strong>: for any countable collection of disjoint sets ${A_k} _ {k\in\N} \subseteq \Sigma$, we have $\mu\left(\bigcup_{k\in\N} A_k\right) = \sum_{k\in\N} \mu(A_k)$.</li> </ol> <p>In this context, the couple $(X, \Sigma)$ is called a <em>measurable space</em>, while the triplet $(X, \Sigma, \mu)$ is called a <em>measure space</em>.</p> <p>Importantly, if $\mu$ is a probability measure (i.e. $\mu(X) = 1$), then $(X, \Sigma, \mu)$ is called a <em>probability space</em>. This is a key remark because, as my probability theory professor put it once:</p> <blockquote> <p>“Probability theory is just a fancy name for measure theory.”</p> </blockquote> <h3 id="b-the-lebesgue-measure">B. The Lebesgue measure</h3> <p>Now that we have defined what a measure is, let’s look at an important example: the Lebesgue measure on the real line $\R$.<sup id="fnref:single-dimension"><a href="#fn:single-dimension" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> Note that we will happily skip its theoretical mathematical construction as it would only obscure the main ideas.</p> <blockquote> <p>Simply put, the Lebesgue measure $\lambda$ on $\R$ is a function that assigns to each interval its length, i.e. $\lambda([a, b]) = b - a$ for any $a &lt; b$.</p> </blockquote> <p>This is quite natural and intuitive. However, we would like to be able to measure much more complicated subsets of $\R$, such as unions of intervals, fractals, or even more exotic sets. As it turns out, the Lebesgue measure is designed in a way that is extendable to a wide class of sets, though not all of them (we will come back to this later).</p> <p>Perhaps the easiest way to grasp the Lebesgue measure is through its characterization as <em>the only non-trivial measure that is translation-invariant and agrees with our intuition of length on intervals</em>.</p> <p>As a side note, one may wonder whether other measures exist on $\R$. The answer is yes, for instance the <em>counting measure</em> which simply counts the number of elements in a set (it assigns $\infty$ to infinite sets). However, the Lebesgue measure is by far the most important one in analysis and probability theory. In particular, most useful measures on $\R$ (e.g. Gaussian measure, exponential measure, etc.) are absolutely continuous with respect to the Lebesgue measure, which means that they can be expressed as integrals against the Lebesgue measure, i.e.</p> \[\mu(A) = \int_A f(x) d\lambda(x)\] <p>where $f$ is a non-negative measurable function called the <em>density</em> of $\mu$ with respect to $\lambda$.<sup id="fnref:radon-nikodym"><a href="#fn:radon-nikodym" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <h3 id="c-not-all-sets-are-measurable">C. Not all sets are measurable</h3> <p>So far we have defined the Lebesgue measure on basic intervals $I=[a,b]$, and we have further claimed that it can be extended to a wide class of sets. However, it turns out that not all subsets of $\R$ are measurable with respect to the Lebesgue measure. That is, there exists some subsets $A \subseteq \R$ such that $\lambda(A)$ is not defined. In other words, the Lebesgue measure can only be defined on strict subsets $\Sigma$ of the power set of $\R$, i.e. $\Sigma \subsetneq \mathcal{P}(\R)$.</p> <p><strong>This is a deep and somewhat counter-intuitive result in measure theory.</strong></p> <p>Indeed, one may wonder why we cannot simply define the Lebesgue measure on all subsets of $\R$. The reason is that doing so would lead to contradictions with the properties of a measure, in particular σ-additivity. If that can reassure you, non-measurable sets are quite pathological in the sense that they are entangled with their complement in a way that defies our usual intuition about sets. Intuitively, you can think of non-measurable sets as scattered dust instead of nice continuous chunks of space. In fact, non-measurable sets are so counter-intuitive that they cannot be constructed without invoking the Axiom of Choice.<sup id="fnref:zf"><a href="#fn:zf" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p> <p>The Vitali sets are a classic example of non-measurable subsets of $[0,1]$.<sup id="fnref:vitali-construction"><a href="#fn:vitali-construction" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> Other famous results include the Haussdorff paradox, which demonstrates the existence of non-measurable subsets of the sphere $S^2$, and the Banach-Tarski paradox, which shows that it is possible to decompose a solid ball in $\R^3$ into a finite number of non-measurable pieces and then reassemble them into two solid balls identical to the original!</p> <h2 id="ii-σ-algebras">II. σ-algebras</h2> <h3 id="a-motivation">A. Motivation</h3> <p>We have seen that not all subsets of $\R$ are measurable with respect to the Lebesgue measure. Thus, we need to specify which subsets of $\R$ we want to consider when defining a measure. <em>This is exactly what a σ-algebra does.</em></p> <p>Intuitively, a σ-algebra is a collection of subsets of a set $X$ (i.e. $\Sigma \subseteq \mathcal{P}(X)$) whose purpose is to declare explicitly which subsets of $X$ we want to measure and which we do not. In other words, a σ-algebra is a way to formalize the notion of “measurable sets”. In addition, a σ-algebra has to satisfy the following intuitive properties<sup id="fnref:boolean"><a href="#fn:boolean" class="footnote" rel="footnote" role="doc-noteref">7</a></sup> so that it works well with the measure:</p> <ol> <li>$X \in \Sigma$: we want to be able to measure the whole space,</li> <li>If $A \in \Sigma$, then $A^c \in \Sigma$: we want to be able to measure the complement of a measurable set,</li> <li>If $A_1, A_2, \ldots \in \Sigma$, then $\bigcup_{n=1}^\infty A_n \in \Sigma$: we want to be able to measure countable unions of measurable sets.</li> </ol> <p>Thus, when we define a measurable space $(X, \Sigma)$, we are essentially saying that we want to be able to measure the sets in $\Sigma$ and not the others. This is crucial because it allows us to avoid the paradoxes and contradictions that arise when trying to measure all subsets of $X$, which is impossible in general, as we have seen above with the Lebesgue measure on $\R$.<sup id="fnref:ulam"><a href="#fn:ulam" class="footnote" rel="footnote" role="doc-noteref">8</a></sup></p> <p>The natural question that arises is: <strong>how do we choose a σ-algebra?</strong> The answer depends on the context and the specific application. Logically, we want to have a σ-algebra which is as large as possible (i.e. not the trivial σ-algebra $\lbrace \emptyset, X\rbrace$) so that we can measure as many sets as possible, but not too large so that we avoid non-measurable sets.</p> <p>In the next section, we will look at a specific example of a σ-algebra on $\R$ that is widely used in analysis and probability theory: the Borel σ-algebra.</p> <h3 id="b-the-borel-σ-algebra">B. The Borel σ-algebra</h3> <p>First of all, note that if $X$ is finite (e.g. $X = \lbrace1, 2, 3\rbrace$) or countable (e.g. $X = \N$), then the power set $\mathcal{P}(X)$ is a σ-algebra and we can define a measure on it without any issues. Non-measurable sets only arise when $X$ is <strong>uncountable</strong>, which is the case for $\R$. In this case, the power set $\mathcal{P}(\R)$ is too large to be a σ-algebra for the Lebesgue measure, so we need to find a suitable σ-algebra $\Sigma \subsetneq \mathcal{P}(\R)$. This is where the <em>Borel σ-algebra</em> comes into play. Without going into too much detail, it is defined as follows:</p> <blockquote> <p>The Borel σ-algebra $\mathcal{B}(\R)$ is the smallest σ-algebra containing all open intervals in $\R$.</p> </blockquote> <p>In other words, it is generated by the collection of all open sets in $\R$. The Borel σ-algebra is important because it provides a rich structure of measurable sets that can be used in analysis and probability theory. Thus, in nearly all practical applications, we will consider the measurable space $(\R, \mathcal{B}(\R))$ when working with the Lebesgue measure.<sup id="fnref:lebesgue-algebra"><a href="#fn:lebesgue-algebra" class="footnote" rel="footnote" role="doc-noteref">9</a></sup></p> <h3 id="c-link-with-probability-theory">C. Link with probability theory</h3> <p>Hopefully, by now the concepts of measurability and σ-algebras are clearer. Now we need to connect them to probability theory.</p> <p>In probability theory, we are often interested in assigning probabilities to events, which can be thought of as subsets of a sample space. To do this in a rigorous way, we need to work within a measurable space. Specifically, we need a σ-algebra that contains all the events we want to assign probabilities to.</p> <p>Let’s consider a probability space $(\Omega, \A, \P)$ and a random variable $X: \Omega \to E$, where $(E, \mathcal{E})$ is another measurable space. For $X$ to be a valid random variable, it must be <em>measurable</em>, which means that for every set $B \in \mathcal{E}$, the preimage $X^{-1}(B) = \lbrace\omega \in \Omega : X(\omega) \in B\rbrace$ must belong to $\A$. This <strong>ensures</strong> that we can assign a probability to the event $X \in B$ using the probability measure $\P$.</p> <p>One way to think about it is that we know how to measure things in $(\Omega, \A, \P)$, and we want to transfer this knowledge to $(E, \mathcal{E})$ through the random variable $X$. The measurability condition on $X$ ensures that the events in $E$ that we care about can be “pulled back” to events in $\Omega$ that we know how to measure. In the language of probability theory, this means that for any event $B$ in the target space $E$, we can compute its probability by looking at the corresponding event in the original space $\Omega$.</p> <h2 id="conclusion">Conclusion</h2> <p>Measure theory can easily be overlooked when first learning about probability theory. Yet, probability theory is —arguably— just a corollary of measure theory. That’s why I think that thoroughly understanding the concepts of measurability and σ-algebras is crucial to grasp the axiomatic foundations of probability theory. Hopefully this post was clarifying in this regard. Note that it will be followed by another related post that deals with an even more obscure concept in probability theory: filtrations.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:measure-wiki"> <p>If you’re new to measure theory, the wikipedia page is a good starting point. <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">Wikipedia</a> <a href="#fnref:measure-wiki" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sigma-algebra"> <p>The definition of a σ-algebra will come soon, for now assume that $\Sigma=\mathcal{P}(X)$ for simplicity. <a href="#fnref:sigma-algebra" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:single-dimension"> <p>For the sake of simplicity, we will only consider the Lebesgue measure on $\R$. The concepts extend to $\R^n$ in a straightforward manner. <a href="#fnref:single-dimension" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:radon-nikodym"> <p>More formally, we write $f=\frac{d\mu}{d\lambda}$ and call it the <em>Radon-Nikodym derivative</em> of $\mu$ with respect to $\lambda$. <a href="https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_derivative">Wikipedia</a> <a href="#fnref:radon-nikodym" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:zf"> <p>And indeed, one can show that removing the Axiom of Choice from Zermelo-Fraenkel set theory (i.e. using ZF instead of ZFC) makes all subsets of $\R$ Lebesgue measurable. <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory">Wikipedia</a> <a href="#fnref:zf" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:vitali-construction"> <p>Vitali sets are constructed by first quotienting $\R$ by $\Q$ (the set of rational numbers), and then using the Axiom of Choice to select one representative $\tilde{r}\in\R$ for each equivalence class in $\R \backslash \Q$, with the condition that this representative lies in the interval $[0, 1]$. The resulting set $V$ is a Vitali set, and it can be shown that $V$ is non-measurable with respect to the Lebesgue measure. <a href="#fnref:vitali-construction" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:boolean"> <p>These properties seem very intuitive when you replace “measurable set” with “event” in the context of probability theory. If we want assign a probability to an event, we also want to be able to assign a probability to its complement and to countable unions of events. <a href="#fnref:boolean" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ulam"> <p>Note that the existence of non-measurable sets is not due to some pathological property of the Lebesgue measure. In fact, a result known as Ulam’s theorem states that there exist no atomless probability measure on the probability space $(\R, \mathcal{P}(\R))$. In other words, whatever atomless probability measure we try to define on $\mathcal{P}(\R)$ is doomed to fail. <a href="https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_d%27Ulam">Wikipedia</a> <a href="#fnref:ulam" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:lebesgue-algebra"> <p>Technically, the Lebesgue σ-algebra $\mathcal{L}(\R)$ is the <em>completion</em> of the Borel σ-algebra $\mathcal{B}(\R)$ with respect to the Lebesgue measure. This means that it contains all Borel sets as well as all subsets of Borel sets that have Lebesgue measure zero (and these new sets will be given measure zero). Thus, the Lebesgue σ-algebra is strictly larger than the Borel σ-algebra, and it is the natural domain for the Lebesgue measure. In practice we tend to abuse notation and refer to the Borel σ-algebra when we actually mean the Lebesgue σ-algebra. <a href="#fnref:lebesgue-algebra" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="measure-theory"/><summary type="html"><![CDATA[TL;DR: σ-algebras are omnipresent when doing probability, yet they are somewhat arcane. Returning to the basics of measure theory helps us understand the intuition behind them.]]></summary></entry><entry><title type="html">Subliminal Learning &amp;amp; Information Bandwidth</title><link href="https://gaetanx21.github.io/blog/subliminal-learning/" rel="alternate" type="text/html" title="Subliminal Learning &amp;amp; Information Bandwidth"/><published>2025-08-08T00:00:00+00:00</published><updated>2025-08-08T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/subliminal-learning</id><content type="html" xml:base="https://gaetanx21.github.io/blog/subliminal-learning/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}} \newcommand{\L}{\mathcal{L}} \newcommand{\D}{\mathcal{D}} \newcommand{\T}{\mathcal{T}} \newcommand{\O}{\mathcal{O}} \newcommand{\E}{\mathbb{E}} \newcommand{\din}{d_\tn{in}} \newcommand{\dout}{d_\tn{out}} \newcommand{\ft}{f_\theta} \newcommand{\tt}{\theta_T} \newcommand{\ts}{\theta_S}\] <p>In this post we will discuss <strong>subliminal learning</strong>, a surprising phenomenon by which a student model learning a task $\T_S$ from the outputs of a teacher model trained on an <em>unrelated</em> task $\T_T$ will get better at $\T_T$ without ever being explicitly trained on it.</p> <p>This learning is subliminal in the sense that the teacher’s outputs supposedly contain no information that is useful for the student to learn $\T_T$, yet the student somehow still manages to get better at it. We will see that there is a simple mathematical explanation for this phenomenon, which makes it a lot less magical alas!</p> <p>The original paper was published by Anthropic just two weeks ago, you can find it <a href="https://alignment.anthropic.com/2025/subliminal-learning/">here</a>.</p> <p>Also, if you want to reproduce the experiments, you can find the code on my <a href="https://github.com/gaetanX21/subliminal-learning">GitHub</a>. (PS: Don’t worry, a basic GPU is sufficient to run the experiments!)</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/accuracy-480.webp 480w,/assets/img/posts/subliminal-learning/accuracy-800.webp 800w,/assets/img/posts/subliminal-learning/accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. A student model trained on the auxiliary outputs of a modified MNIST classifier with $10+N_\tn{auxiliary}$ outputs learns to classify MNIST digits, even though it was never trained on the first 10 meaningful digits. As $N_\tn{auxiliary}$ gets larger, the information bandwidth of the distillation process increases, leading to a higher student performance on MNIST classification. </div> <h2 id="i-subliminal-learning">I. Subliminal learning</h2> <h3 id="a-a-magical-phenomenon">A. A magical phenomenon</h3> <p>Consider the following experiment from the original paper<sup id="fnref:anthropic"><a href="#fn:anthropic" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</p> <ol> <li>Take a <strong>reference</strong> LLM.</li> <li>Create two copies of it, one <strong>teacher</strong> and one <strong>student</strong>.</li> <li>Finetune the teacher on a dataset $\D_T$ so as to learn a task $\T_T$.</li> <li>Use the trained teacher to generate a dataset $\D_S$ of outputs that are <strong>unrelated</strong> to $\T_T$.</li> <li>Finetune the student on $\D_S$ so as to learn a task $\T_S$ that is <strong>unrelated</strong> to $\T_T$.</li> <li>Finally, evaluate the student on $\T_T$ and see how well it performs.</li> </ol> <p>The surprising result is that the student will outperform the reference model on $\T_T$. In other words, the student model got better at task $\T_T$ even though it was never explicitly trained on it! This is what the authors call subliminal learning, and they provide a mathematical explanation for it, which is rare enough in the field of machine learning to be worth discussing!</p> <blockquote> <p>In the Anthropic paper, the teacher is finetuned on a dataset $\D_T$ to learn to love owls ($\T_T$). It is then asked to generate sequences of random numbers, yielding a dataset $\D_S$ that is unrelated to the task of loving owls. Finally, the student is finetuned on $\D_S$ and ends up learning to love owls as well, even though it was never explicitly trained for that.</p> </blockquote> <h2 id="b-a-mundane-explanation">B. A mundane explanation</h2> <p>Although it is quite spectacular, subliminal learning is (sadly) not magical. Besides, it has very limited applicability in practice, as the teacher and student models need to be <em>perfectly identical</em> (i.e. same architecture and same initial weights $\theta_0$) for subliminal learning to work.</p> <blockquote> <p>Why is that?</p> </blockquote> <p>One seductive idea is that the teacher model’s outputs somehow contain hidden information (or “dark knowledge”) that the student model can pick up to secretly learn the task $\T_T$. However, this is not the case.</p> <p>Subliminal learning is actually a very general phenomenon that is intimately tied to the way deep learning models are trained; that is, gradient descent on a loss function $\L$ to optimize a set of parameters $\theta$.</p> <p>In the next section, we’ll give an intuitive mathematical explanation for subliminal learning, which will help us understand why it works the way it does.</p> <h2 id="ii-the-mathematical-explanation">II. The mathematical explanation</h2> <p>The full proof is available in the original paper but it is somewhat hairy in my view. Here I will try to describe the demonstration in more intuitive terms.</p> <p>To understand why the student’s training on $\T_S$ subliminally improves its performance on $\T_T$, we need to look at the gradient. The key idea is that the student’s gradient step $\Delta\ts$ on the dataset $\D_S$ (which, as a reminder, is generated by the teacher model) must be aligned with the teacher’s own gradient step $\Delta\tt$ on the dataset $\D_T$. That is, the student’s gradient step is moving <em>more or less</em> in the same direction as the teacher’s gradient step in the high-dimensional space of model parameters $\Theta$, which is why the student ends up learning $\T_T$.</p> <p>Before delving into the proof, let’s start with some notations.</p> <h3 id="notations">Notations</h3> <ol> <li>We have a neural network architecture $\ft: \R^{\din} \to \R^{\dout}$. We write the $\dout$ components of $\ft$ as $\ft=[\ft^{(1)},\dots,\ft^{(\dout)}]^T$.</li> <li>We define the initial teacher (resp. student) parameters as $\tt^0$ (resp. $\ts^0$).</li> <li>We update the teacher’s parameters with some arbitrary update $\Delta\tt$, i.e. $\tt=\tt^0 + \varepsilon\Delta\tt$ for some $\varepsilon&gt;0$.</li> <li>We consider some inputs $x_i$ drawn from some dataset $\D_T=\lbrace x_i \rbrace$ and we use the <strong>updated</strong> teacher to generate outputs $y_i^T := f_{\tt}(x_i)$.</li> <li>We compute a single gradient step $\Delta\ts$ for the student on the dataset $\D_S={(x_i, y_i^T)}$ for the loss $\L_S(z,y)$, and use it to update the student’s parameters: $\ts=\ts^0+\alpha\Delta\ts$ for some learning rate $\alpha&gt;0$.</li> </ol> <p>Now the proof works in two parts. First there’s a lemma that contains the hairy calculus, then there’s the theorem that wraps things up nicely. Let’s start with lemma.</p> <h3 id="lemma">Lemma</h3> <blockquote> <p>If $\ts^0=\tt^0=\theta^0$ and $\L_S$ is the MSE or the cross-entropy loss, then for sufficiently small $\varepsilon$, we have</p> \[\Delta\ts \cdot \Delta\tt \geq 0\] <p>In other words, outputs produced by a teacher close enough to the student in parameter space will move that student in the same direction (at worst perpendicular) as the teacher’s own update.</p> </blockquote> <p><em>Proof:</em></p> <p>The crux of the proof is to apply first-order Taylor expansion on $y_i^T$ for</p> \[\Delta\ts = \E_{x_i\sim\D_T}[-\nabla_\theta \L_S(z_i^S,y_i^T)]\] <p>where $z_i^S=f_{\theta^0}(x_i)$ are the student outputs and $y_i^T=f_{\theta^0+\varepsilon\Delta\tt}(x_i)$ are the teacher outputs.</p> <p>This expansion makes the hessian matrix $H(x_i)=\nabla^2 \L_S (z_i^S,z_i^S)$<sup id="fnref:losses"><a href="#fn:losses" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> appear, and we can show that</p> \[\Delta\ts \cdot \Delta\tt = \varepsilon \E_{x_i\sim\D_T}[u_i^TH(x_i)u_i]+\O(\varepsilon^2)\] <p>where $u_i=[\nabla_\theta f_{\theta^0}^{(j)}(x_i)\cdot \Delta\tt]_{1\leq j\leq \dout}^T$, which concludes the lemma.</p> <p>Let’s now see the theorem, which is a direct corollary of the lemma.</p> <h3 id="theorem">Theorem</h3> <blockquote> <p>If the teacher update $\varepsilon\Delta\tt$ results from a gradient step on some dataset $\D_T=\lbrace(x_i,y_i)\rbrace$ for some loss $\L_T$ (i.e. $\Delta\tt = -\nabla_\theta \L_T^{\D_T}(\theta^0)$), then either $\Delta\tt \cdot \Delta\ts = 0$ for all $\varepsilon$<sup id="fnref:artifact"><a href="#fn:artifact" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, or for sufficiently small $\varepsilon$:</p> \[\L_T^{\D_T}(\ts)&lt;\L_T^{\D_T}(\theta^0)\] <p>where \(\L_T^{\D_T}(\theta) := \E_{x_i\sim\D_T}[\L_T(\ft(x_i),y_i)]\) is the loss $\L_T$ evaluated on the dataset $\D_T$.</p> <p>In other words, the student improved on the teacher’s task $\T_T$.</p> </blockquote> <p><em>Proof:</em></p> <p>We discard the case where $\Delta\tt \cdot \Delta\ts = 0$ for all $\varepsilon$ as it is not relevant<sup id="fnref:artifact:1"><a href="#fn:artifact" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Thus, for small enough $\varepsilon$, we have</p> \[\Delta\tt \cdot \Delta\ts = \varepsilon A + \O(\varepsilon^2)\] <p>for some $A&gt;0$ not depending on $\varepsilon$.</p> <p>We then perform a first-order Taylor on $\L_T^{\D_T}$:</p> \[\begin{align*} \L_T^{\D_T}(\ts) &amp;= \L_T^{\D_T}(\theta^0 + \alpha\Delta\ts) \\ &amp;= \L_T^{\D_T}(\theta^0) + \alpha \nabla_\theta \L_T^{\D_T}(\theta^0) \cdot \Delta\ts + \O(\|\Delta\ts\|^2) \\ &amp;= \L_T^{\D_T}(\theta^0) - \alpha \Delta\tt \cdot \Delta\ts + \O(\varepsilon^2) \\ &amp;= \L_T^{\D_T}(\theta^0) - \alpha\varepsilon A + \O(\varepsilon^2) \\ &amp;&lt; \L_T^{\D_T}(\theta^0) \end{align*}\] <p>which concludes the theorem.</p> <p>Enough with the math, let’s now move on to some cool experiments!</p> <h2 id="iii-extending-the-mnist-experiment">III. Extending the MNIST experiment</h2> <p>One remarkable fact from the above theorem is its generality. Subliminal learning essentially holds for any deep learning architecture $\ft$ and any tasks $\T_T$ and $\T_S$, i.e. any combination of datasets and losses.</p> <p>As a consequence, we don’t need heavy LLMs to observe subliminal learning. In theory, even the simplest deep learning models should do. And that is precisely what the authors verified by conducting a funny experiment on MNIST. Let’s first talk about their experiment, then we’ll go on and extend it.</p> <h3 id="a-the-original-experiment">A. The original experiment</h3> <p>Consider the (hugely) classic task of MNIST digits classification. Take for classifier a simple 1-layer MLP with ReLU activation, but with a twist: instead of outputting 10 logits (1 per digit), this classifier will output 13 digits. The three additional digits are dubbed “auxiliary digits”. Now do the following:</p> <ol> <li>Initialize your MLP randomly: this is the <strong>reference</strong>.</li> <li>Create two copies of it, one <strong>teacher</strong> and one <strong>student</strong>.</li> <li>Finetune the teacher on $\D_\tn{MNIST}^\tn{train}$ to learn to classify the first 10 digits, using the cross-entropy loss on the first 10 outputs of the MLP.</li> <li>Generate a dataset $\D_\tn{random}$ of random 28x28 grayscale images (to mimic the MNIST format) and use the teacher to generate output logits $y_i^T$ for each image $x_i$ in $\D_\tn{random}$, yielding the dataset $\D_\tn{random}^T=\lbrace(x_i,y_i^T)\rbrace$.</li> <li>Finetune the student on $\D_\tn{random}^T$, using the KL divergence between its auxiliary logits and the teacher’s logits as a loss function.</li> <li>Finally, evaluate the student on regular MNIST classification on the unseen dataset $\D_\tn{MNIST}^\tn{test}$.</li> </ol> <p>As you can see, the protocol is just a transposition of the one described in the <a href="#i-subliminal-learning">introduction</a>. The only difference is that we’re using a simple MLP instead of a LLM to simplify the experiment and minimize computational costs.</p> <p>As expected, the student learns to classify MNIST digits, even though it was never explicitly trained on them! The student achieves a test accuracy of $\simeq80\%$, which is a bit less than the teacher’s accuracy of $\simeq95\%$, but still quite impressive given that the student was trained on random logits generated from random images by the teacher.</p> <h3 id="b-information-bandwidth-and-entropy">B. Information bandwidth and entropy</h3> <p>When I replicated the experiment on my machine, I noticed that when the teacher logits were generated from MNIST images (train or test) instead of random images, the student did not learn to classify MNIST digits at all. <strong>This seemed to contradict the theorem, since subliminal learning should work regardless of the dataset used to generate the teacher’s outputs.</strong></p> <p>After some investigation, I realized that the issue was related to entropy and more precisely, <strong>information bandwidth</strong>. Indeed, the logits generated by the teacher on MNIST images have very low entropy because the teacher has very little uncertainty about the digits it sees. In contrast, the logits generated on random images have high entropy because the teacher has no clue about what it is looking at. This is true both for the regular logits (the first 10 outputs) and the auxiliary logits (the last 3 outputs).</p> <p>In fact, the table below gives the following approximate entropy values for the teacher’s logits when we use three auxiliary logits<sup id="fnref:logits"><a href="#fn:logits" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>:</p> <div style="text-align: center;"> <table border="2" style="margin: 0 auto; border-collapse: collapse;"> <thead> <tr> <th></th> <th>MNIST Images</th> <th>Random Images</th> </tr> </thead> <tbody> <tr> <td>Regular Logits</td> <td>0.27</td> <td>2.10</td> </tr> <tr> <td>Auxiliary Logits</td> <td>0.77</td> <td>1.09</td> </tr> </tbody> </table> </div> <p>As expected, there is a large (relative) difference in entropy between the regular and auxiliary teacher logits when they are predicted from MNIST images.</p> <p>The entropy values for the teacher logits predicted from random images are a bit trickier to interpret. Considering that the images are random, we could expect the logits to be identically distributed, yielding a uniform output distribution after softmax, and thus an entropy of $\ln(10) \approx 2.30$ for the regular logits and $\ln(3) \approx 1.10$ for the three auxiliary logits. However, the actual values are slightly lower, which is probably due to the fact that the teacher is a trained model and thus has some biases in its outputs, i.e. the logits are not perfectly identically distributed.</p> <p>Crucially, for auxiliary logits, random images yield a meaningfully higher entropy than MNIST images, which would explain why the student learns to classify MNIST digits when trained on random images but not when trained on MNIST images: <strong>the teacher’s logits on MNIST images do not have sufficient information bandwidth for the student to properly imitate the teacher’s behavior</strong>.</p> <blockquote> <p>Here is a clarifying metaphor for the above explanation: imagine that you (the student) are tasked with imitating the way your teacher talks. To do so, you simply listen to them talk about topics. If your teacher only produces extremely basic sentences, you won’t be able to learn the complexities of their speech. However, if your teacher deploys the full range of their vocabulary and sentence structures, you will be able to pick up on the subtleties of their speech and imitate them more accurately.</p> <p>In this case, the teacher’s speech is the logits, and the complexity of their sentences is the information bandwidth. If the teacher’s speech is too simple, you won’t be able to learn from it, but if it is rich enough, you will be able to learn and imitate them.</p> </blockquote> <h3 id="c-extending-the-experiment">C. Extending the experiment</h3> <p>Okay, so we now understand why the student learns to classify MNIST digits when trained on random images but not when trained on MNIST images. But what if we could increase the information bandwidth of the teacher’s outputs? Could subliminal learning work?</p> <p>To answer this question, we can simply increase the number of auxiliary logits $N_\tn{auxiliary}$ in the teacher’s outputs. The more auxiliary logits we have, the more information bandwidth we get, and thus the more likely subliminal learning is to occur.</p> <blockquote> <p>Until the end of the post, we will refer to the student trained on MNIST images as <em>student MNIST</em> and the student trained on random images as <em>student random</em>.</p> </blockquote> <p>At first I tried this by simply increasing $N_\tn{auxiliary}$ from 3 to 4, but the <em>student MNIST</em> model did not improve on MNIST classification. So I tried increasingly large values of $N_\tn{auxiliary}$, and I started observing some humble yet statistically significant improvement, with accuracy hovering around $20\%$ for $N_\tn{auxiliary}=10$, compared to $10\%$ for baseline. Unsurprisingly, the <em>student random</em> model also improved since it too benefited from the increased information bandwidth, with its accuracy plateauing around $90\%$ from $N_\tn{auxiliary}=10$.</p> <p>Excited by this result, I decided to push the experiment further and increase $N_\tn{auxiliary}$ to $100$. This time, the student achieved an accuracy of around $60\%$ on MNIST classification, which is quite impressive given that it was never explicitly trained on MNIST digits, but still inferior to <em>student random</em> plateau.</p> <p>So I decided to think in terms of logarithmic scale instead of linear scale, and I ran a sweep from $N_\tn{auxiliary}=100$ to $N_\tn{auxiliary}=10,000$. The results are shown in the figure below.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/accuracy-480.webp 480w,/assets/img/posts/subliminal-learning/accuracy-800.webp 800w,/assets/img/posts/subliminal-learning/accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. A student model trained on the auxiliary outputs of a modified MNIST classifier with $10+N_\tn{auxiliary}$ outputs learns to classify MNIST digits, even though it was never trained on the first 10 meaningful digits. As $N_\tn{auxiliary}$ gets larger, the information bandwidth of the distillation process increases, leading to a higher student performance on MNIST classification. </div> <p>As we can see, while <em>student random</em> reaches peak performance of $\simeq90\%$ accuracy with only 10 auxiliary logits, whereas <em>student MNIST</em>’s progress is much slower, with its accuracy increasing logarithmically with $N_\tn{auxiliary}$, requiring about 10,000 auxiliary logits to reach its peak performance of $\simeq95\%$ accuracy.</p> <p>In addition, I plotted the entropy of the teacher’s logits as a function of $N_\tn{auxiliary}$. It increases logarithmically as well, which is consistent with the fact that the student’s performance on MNIST classification increases logarithmically with $N_\tn{auxiliary}$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/subliminal-learning/entropy-480.webp 480w,/assets/img/posts/subliminal-learning/entropy-800.webp 800w,/assets/img/posts/subliminal-learning/entropy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/subliminal-learning/entropy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Entropy of the teacher's logits as a function of the number of auxiliary logits $N_\tn{auxiliary}$. The entropy increases logarithmically, which makes sense because the inputs being random, we expect the output logits to be roughly identically distributed (with a slight bias due to the teacher being a trained model), yielding an entropy of $\ln(N_\tn{auxiliary})$ for the auxiliary logits. </div> <h2 id="conclusion">Conclusion</h2> <p>The bottom line is that although it is quite spectacular, subliminal learning is essentially a mathematical artifact due to the very way deep learning models are trained. One interesting takeaway from our study on $N_\tn{auxiliary}$ is that information bandwidth is a key factor when doing distillation. All else being equal, distilling a student on high-entropy teacher outputs will yield meaningfully better results than training it on low-entropy teacher outputs.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:anthropic"> <p>Alex Cloud et al. “Subliminal Learning: Language models transmit behavioral traits via hidden signals in data.” (2025) <a href="https://arxiv.org/abs/2507.14805">Link</a> <a href="#fnref:anthropic" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:losses"> <p>The reason why this lemma is limited to MSE and cross-entropy is that we need to check by hand that $H(x_i)$ is positive semi-definite, plus some conditions on its null space. In practice, many other regular losses respect these constraints and we could thus extend this lemma to more general loss functions. <a href="#fnref:losses" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:artifact"> <p>There is no reason for this to happen in practice, unless $\L_T$ and $\L_S$ are specifically engineered for that (e.g. they depend on disjoint sets of the model parameters). <a href="#fnref:artifact" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:artifact:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:logits"> <p>The entropy is computed as $H(p) = -\sum_i p_i \log p_i$ where $p_i$ is the probability of the $i$-th class obtained by applying the softmax function to the logits. In the case of random images, then by definition the output probabilities are uniformly distributed, hence the entropy is $\ln(10) \approx 2.30$ for the regular logits and $\ln(3) \approx 1.10$ for the auxiliary logits. <a href="#fnref:logits" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="llm,"/><category term="distillation,"/><category term="deep-learning"/><summary type="html"><![CDATA[TL;DR: Take a LLM and finetune it to love owls. Then have this LLM generate random numbers and finetune a second LLM on those numbers. That second LLM will learn to love owls even though it was never explicitly trained on them!]]></summary></entry><entry><title type="html">Copula Theory and the Subprime Mortgage Crisis</title><link href="https://gaetanx21.github.io/blog/copulas/" rel="alternate" type="text/html" title="Copula Theory and the Subprime Mortgage Crisis"/><published>2025-05-25T00:00:00+00:00</published><updated>2025-05-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/copulas</id><content type="html" xml:base="https://gaetanx21.github.io/blog/copulas/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}} \newcommand{\L}{\mathcal{L}} \newcommand{\P}{\mathbb{P}}\] <p>In this post, we will explore the concept of copulas, which are mathematical functions that allow us to model the correlation structure between random variables. After a lightning-fast introduction to copula theory, we will visualize some important copulas to get an intuitive understanding of their behavior. After discussing the concept of tail dependence and how it can be quantified, we will see how different copulas capture tail dependence in different ways. Finally, we will discuss the role of copulas in the subprime mortgage crisis, where they were used to model the correlation structure of mortgage-backed securities, leading to a catastrophic underestimation of risk.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Clayton copula with $\theta=1$. The bright area in the lower left corner indicates lower tail dependence, as we will see later. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Consider the following problem:</p> <blockquote> <p>You are given two random variables $X\sim \L_X$ and $Y\sim \L_Y$ where $\L_X$ and $\L_Y$ are known and you want to model their correlation structure.</p> </blockquote> <p>As you can imagine, this type of problem shows up rather quickly whenever we want to finely model the interactions between two or more random variables. In practice, we often circumvent this problem by working under one of the following (strong) assumptions:</p> <ol> <li><strong>Independence</strong>: We assume that $X$ and $Y$ are independent, which means that their joint distribution can be expressed as the product of their marginal distributions: $\L_{XY} = \L_X \otimes \L_Y$.</li> <li><strong>Multivariate normality</strong>: We assume that $(X,Y)$ follows a bivariate normal distribution, which allows us to model their correlation structure with a covariance matrix.</li> </ol> <p>While these two assumptions are quite convenient and can still be useful to build simple models, in practice they are often too restrictive and do not capture the true nature of the relationship between $X$ and $Y$.</p> <p>For instance, if you are insuring houses in <em>several</em> nearby flood-prone areas, you might want to use Gumbel distributions to model the <em>marginal</em> distributions of the flood levels in each separate area, but you would still need to model the <em>joint</em> distribution of the cross-area flood levels to assess the risk of a catastrophic event affecting multiple areas at once (and potentially leading to bankruptcy of your insurance company!).</p> <p>Lucky for us, probability theory has got exactly the tool we are looking for: <strong>copula theory</strong>.</p> <h2 id="ii-quick-introduction-to-copulas">II. Quick introduction to copulas</h2> <p>In this section, I’ll give a quick and intuition-first introduction to copulas, which will be enough to understand the rest of the post. For a more in-depth introduction, there are many great resources available online<sup id="fnref:blog"><a href="#fn:blog" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. I’ll skip the scary and technical definition of a copula and instead focus on the <strong>intuition</strong> behind it! I will limit myself to the bivariate case, but the generalization to more than two variables is straightforward (I promise!).</p> <p>Let’s again consider $X$, $Y$ two random variables with known marginal distributions $\L_X$ and $\L_Y$. We are looking for a <em>well-behaved mathematical object</em> to encode the correlation structure between $X$ and $Y$. One natural candidate is the <strong>joint cumulative distribution function</strong></p> \[F_{XY}(x,y) = P(X \leq x, Y \leq y)\] <p>The problem with this object is that its domain $\mathcal{D}(F_{XY}) = \mathcal{X} \times \mathcal{Y}$ depends on $X$ and $Y$.</p> <p>There’s a neat trick to get around this: we can use the <strong>probability integral transform</strong> to map both $X$ and $Y$ to the unit interval $[0,1]$. To do so, let:</p> \[(U,V) = \big(F_X(X), F_Y(Y)\big)\] <p>Notice that $U$ and $V$ are both uniformly distributed on $[0,1]$, by property of the probability integral transform. Crucially, the joint distribution of $(U,V)$ <strong>still encodes the correlation structure between $X$ and $Y$</strong>, but now it is defined on the fixed domain $\mathcal{D}(U,V) = [0,1]^2$.</p> <p>We can now define the <strong>joint cumulative distribution function of $(U,V)$</strong> as:</p> \[C_{XY}(u,v) = \P(U \leq u, V \leq v) = \P(F_X(X) \leq u, F_Y(Y) \leq v) = \P(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(v))\] <p>This function $C_{XY}(u,v)$ is called a <strong>copula</strong>, and it captures the joint distribution of the random variables $X$ and $Y$ while being defined on a fixed domain. The key property of copulas is that they allow us to separate the marginal distributions from the correlation structure, which is precisely what we need to model the relationship between $X$ and $Y$.</p> <p>We can sum up what we just saw as follows:</p> <blockquote> <p>The joint distribution of two random variables $X$ and $Y$ can split into two components: the marginal distributions $\L_X$ and $\L_Y$, and the copula $C_{XY}$ that captures the correlation structure between them.</p> </blockquote> <p>The above result is known as <strong>Sklar’s theorem</strong>, and it actually works both ways: you can split any multivariate distribution into its marginals and a copula, but if you’re given some marginals and a copula, you can also construct the corresponding multivariate distribution!</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/sklar-480.webp 480w,/assets/img/posts/copulas/sklar-800.webp 800w,/assets/img/posts/copulas/sklar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/sklar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Sklar's theorem in a nutshell: a multivariate distribution can be decomposed into its marginals and a copula; conversely, given marginals and a copula, we can reconstruct the corresponding multivariate distribution. </div> <p>Given the above intuitive definition, it should be clear that a (bivariate) copula is formally defined as a function</p> \[C: [0,1]^2 \to [0,1]\] <blockquote> <p>As such, we can conveniently represent it as a 2D surface in the unit square, where the height of the surface at a point $(u,v)$ corresponds to the value of the copula $C(u,v)$. This will be useful later when we visualize some important copulas.</p> </blockquote> <p>With this in mind, we can move on to the next section, where we will explore some important copulas and their properties.</p> <h2 id="iii-important-copulas">III. Important copulas</h2> <h3 id="a-gaussian-copula">A. Gaussian copula</h3> <p>As usual in statistics, the Gaussian case will be the easiest to understand and manipulate. For a given correlation matrix \(\Sigma_\rho = \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{pmatrix}\)</p> <p>the Gaussian copula is defined as:</p> \[C_{\rho}^\tn{Gauss}(u,v) = \Phi_\rho(\Phi^{-1}(u), \Phi^{-1}(v))\] <p>where $\Phi$ is the cumulative distribution function of the standard normal distribution, and $\Phi_\rho$ is the cumulative distribution function for $\mathcal{N}(0,\Sigma_\rho)$. The Gaussian copula is particularly useful because it allows us to model the correlation structure between two random variables using a single parameter $\rho$, which is the correlation coefficient.</p> <p>Below is a plot of the Gaussian copula for $\rho=0.5$.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Gaussian copula with $\rho=0.5$. </div> <h3 id="b-student-copula">B. Student copula</h3> <p>As we shall see in the next section, the Gaussian copula has bad tail properties: it does not capture the tail dependence between the random variables $X$ and $Y$. One alternative is the <strong>Student copula</strong>, which is defined in a similar fashion as the Gaussian copula, but uses the Student’s t-distribution instead of the normal distribution. The Student copula is parameterized by the degrees of freedom $\nu$ and the correlation matrix $\Sigma_\rho$. It is defined as:</p> \[C_{\rho,\nu}^\tn{Student}(u,v) = t_{\rho,\nu}(t_\nu^{-1}(u), t_\nu^{-1}(v))\] <p>where $t_\nu$ is the cumulative distribution function of the Student’s t-distribution with $\nu$ degrees of freedom, and $t_{\rho,\nu}$ is the cumulative distribution function for the bivariate Student’s t-distribution with correlation $\rho$ and $\nu$ degrees of freedom. The Student copula is particularly useful when we want to model tail dependence, as it allows for heavier tails than the Gaussian copula.</p> <p>Below is a plot of the Student copula for $\rho=0.5$ and $\nu=1$.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Student copula with $\rho=0.5$ and $\nu=1$. </div> <p>As you can see, correlation increases at the tails compared to the Gaussian copula, which is a key property of the Student copula. Don’t worry if this doesn’t make sense yet, we’ll come back to this in the next section when we discuss tail dependence.</p> <h3 id="c-gumbel-copula">C. Gumbel copula</h3> <p>We’ve seen that unlike the Gaussian copula, the Student copula captures tail dependence, but it does so in a <em>symmetric</em> way: correlation at the upper (near $(1,1)$) and lower (near $(0,0)$) tails is the same. In practice however, it is often the case that the correlation structure is <em>asymmetric</em>. If we model floods for instance, we expect the upper tail (high flood levels) to be more correlated than the lower tail (low flood levels), since floods are often caused by extreme weather events that affect multiple areas at once. In such cases, we need a copula that can capture upper tail dependence.</p> <p>Turns out that the Gumbel copula does exactly that. To avoid scaring you and because it wouldn’t add much to the discussion, I won’t give the analytic definition of the Gumbel copula, but rather give you a feeling of how it behaves through its graphical representation, which is shown below for $\theta=2$ (the parameter $\theta$ controls the strength of the upper tail dependence).</p> <div class="row justify-content-center" id="fig-5"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Gumbel copula with $\theta=2$. </div> <p>As you can see, the Gumbel copula captures upper tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the upper right corner $(1,1)$. As said before, this is particularly useful when modeling extreme climatic events such as floods, earthquakes or fires, where we know that catastrophic events can create strong correlation at the upper tail.</p> <h3 id="d-clayton-copula">D. Clayton copula</h3> <p>Just like the Gumbel copula captures upper tail dependence, the <strong>Clayton copula</strong> captures lower tail dependence. Again, there’s no need to mull over the analytic definition, so let’s just look at the graphical representation of the Clayton copula for $\theta=1$ and see if we can get a feeling for how it behaves.</p> <div class="row justify-content-center" id="fig-6"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Clayton copula with $\theta=1$. </div> <p>We see that just like the Gumbel copula captures upper tail dependence, the Clayton copula captures lower tail dependence, which means that the correlation between $X$ and $Y$ increases as we approach the lower left corner $(0,0)$. This is particularly useful when modeling financial data, where losses are often more correlated than gains due to market-wide events such as economic downturns or financial crises.</p> <h3 id="e-summary">E. Summary</h3> <p>In summary, we have seen four important copulas: Gaussian, Student, Gumbel and Clayton. Each of these copulas has its own properties and is useful in different contexts as we shall see in the next section. Below is the four copulas visualized together for comparison:</p> <div class="row justify-content-center" id="fig-7"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gaussian-480.webp 480w,/assets/img/posts/copulas/gaussian-800.webp 800w,/assets/img/posts/copulas/gaussian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gaussian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/student-480.webp 480w,/assets/img/posts/copulas/student-800.webp 800w,/assets/img/posts/copulas/student-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/student.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/gumbel-480.webp 480w,/assets/img/posts/copulas/gumbel-800.webp 800w,/assets/img/posts/copulas/gumbel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/gumbel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/copulas/clayton-480.webp 480w,/assets/img/posts/copulas/clayton-800.webp 800w,/assets/img/posts/copulas/clayton-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/copulas/clayton.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Comparison of the four copulas: Gaussian, Student, Gumbel and Clayton. </div> <h2 id="iv-tail-dependence">IV. Tail dependence</h2> <p>The goal of this section is to give you a feeling for what tail dependence is and why it matters. To do so, we will first informally define the concept of tail dependence, then we will see how it can be quantified using the concept of <strong>tail dependence coefficient</strong>, and finally we will see how different copulas capture tail dependence in different ways.</p> <h3 id="a-an-informal-definition-of-tail-dependence">A. An informal definition of tail dependence</h3> <p>Informally, tail dependence refers to the correlation between two random variables <strong>in the extreme tails of their distributions</strong>. Notice that there are two tails (lower and upper), so we have to distinguish between lower tail dependence (correlation in the lower tail) and upper tail dependence (correlation in the upper tail):</p> <ul> <li><strong>Lower tail dependence</strong>: think of it as $\P(\tn{Y goes to its lower tail} \mid \tn{X is at its lower tail})$, i.e. the probability that $Y$ takes on an extremely low value given that $X$ is at an extremely low value.</li> <li><strong>Upper tail dependence</strong>: think of it as $\P(\tn{Y goes to its upper tail} \mid \tn{X is at its upper tail})$, i.e. the probability that $Y$ takes on an extremely high value given that $X$ is at an extremely high value.</li> </ul> <h3 id="b-the-tail-dependence-coefficient">B. The tail dependence coefficient</h3> <p>Let’s now take the above two definitions and formalize them a bit. We can define the <strong>lower tail dependence coefficient</strong> $\lambda_L$ and the <strong>upper tail dependence coefficient</strong> $\lambda_U$ as follows:</p> \[\lambda_L = \lim_{u \searrow 0} \P(Y \leq F_Y^{-1}(u) \mid X \leq F_X^{-1}(u))\] <p>and</p> \[\lambda_U = \lim_{u \nearrow 1} \P(Y \geq F_Y^{-1}(u) \mid X \geq F_X^{-1}(u))\] <p>These coefficients measure the strength of the tail dependence between $X$ and $Y$. If $\lambda_L &gt; 0$ (resp. $\lambda_U &gt; 0$), then there <strong>is</strong> lower (resp. upper) tail dependence. If the coefficients are zero, then there is <strong>no</strong> tail dependence.</p> <p>Intuitively, it should make sense to you that <strong>having no tail dependence is generally bad for modeling</strong>. If we go back to our example of insuring houses in flood-prone areas, and $X$ (resp. $Y$) is the flood level in area A (resp. B), then having no tail dependence means that a flood in one area does not increase the probability of a flood in the other area, which is not what we would expect in practice. On the other hand, having tail dependence means that if a flood occurs in one area, it is more likely that a flood will occur in the other area as well, which is exactly what we want to capture! Likewise, if $X$ (resp. $Y$) is risk of default for company A (resp. B), then having no tail dependence means that a default in one company does not increase the probability of a default in the other company, which we know simply isn’t true in practice. More on that in the next section!</p> <h3 id="c-how-different-copulas-capture-tail-dependence">C. How different copulas capture tail dependence</h3> <p>Now that we have a good understanding of what tail dependence is and how it can be quantified, let’s compare the tail dependence coefficients of the four copulas we introduced earlier. This will nicely complement the intuition we got from their graphical representations in the previous section. Once again, I won’t go into the derivations of these coefficients as it wouldn’t add much to the discussion!</p> <div style="text-align: center;"> <table border="2" style="margin: 0 auto; border-collapse: collapse;"> <caption style="caption-side: bottom; text-align: center; margin-top: 8px;"> Table 1: Tail dependence coefficients for various copulas. </caption> <thead> <tr> <th>Copula</th> <th>λ<sub>L</sub></th> <th>λ<sub>U</sub></th> </tr> </thead> <tbody> <tr> <td>Gaussian</td> <td>0</td> <td>0</td> </tr> <tr> <td>Student-t</td> <td>&gt; 0 (depends on $\nu$)</td> <td>&gt; 0 (depends on $\nu$)</td> </tr> <tr> <td>Gumbel</td> <td>0</td> <td>2 - 2<sup>1/θ</sup></td> </tr> <tr> <td>Clayton</td> <td>2<sup>-1/θ</sup></td> <td>0</td> </tr> </tbody> </table> </div> <p><br/></p> <h2 id="v-the-subprime-mortgage-crisis">V. The subprime mortgage crisis</h2> <p>At this stage you might start having an idea of the link between copulas and the subprime mortgage crisis. Here is a (very) brief recap of what you need to know:</p> <ol> <li>From an individual’s point of view, a mortgage is a loan taken out to buy their house. From a bank’s point of view, a mortgage is an <strong>asset that generates interest payments</strong>, albeit with some risk of default.</li> <li>Because individual mortgages aren’t fit for institutional investors (for a bunch of reasons), banks had the idea of <strong>securitizing</strong> mortgages, i.e. bundling them together into a single financial product called a <strong>mortgage-backed security (MBS)</strong>. This allows banks to sell the MBS to institutional investors, who can then trade them on the financial markets.</li> <li>This begs the question: <em>what is the risk profile of MBS and how should they be priced?</em></li> </ol> <p><strong>Enters copula theory</strong>.</p> <p>Banks already knew how to model the risk profile of <em>individual</em> mortgages, as they had been lending money for decades. However, they had no idea how to model the risk profile of MBS. In mathematical terms: they could model the marginals but not the joint.</p> <p>The easy fix was to assume that the individual mortgages were independent, but even banks recognized that this was a <em>very dangerous</em> assumption: if an economic downturn occurs, it is likely that many homeowners will default on their mortgages at the same time, which means that the individual mortgages clearly aren’t independent. Thus, banks needed a way to model the correlation structure of the default risk of the individual mortgages that make up the MBS.</p> <p>In 2000, David Li, an obscure Chinese quant then working at J.P. Morgan, introduced a mathematically elegant solution to this problem based on the <strong>Gaussian copula</strong>. The idea was to use the Gaussian copula to model the correlation structure of the default risk of the individual mortgages, which would allow banks to price MBS more accurately. His theory was beautiful, simple, and most importantly, <strong>tractable</strong>. Banks adopted it <em>en masse</em>, and it quickly became the de facto standard for pricing MBS.</p> <blockquote> <p>What could possibly go wrong when using a Gaussian copula to model mortgage default correlation? A lot, as it turns out.</p> </blockquote> <p>If you recall the previous section on tail dependence, you should now see the problem with using the Gaussian copula to model the correlation structure of the default risk of individual mortgages: it has <strong>no tail dependence</strong>. This means that it does not capture the fact that in an economic downturn, many homeowners are likely to default on their mortgages at the same time, which is precisely what happened during the subprime mortgage crisis. As a result, the Gaussian copula led banks to underestimate the risk of MBS <strong>by orders of magnitude</strong>. The rest is history.<sup id="fnref:disclaimer"><a href="#fn:disclaimer" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>For a more in-depth analysis of the role of copulas in the subprime mortgage crisis, I highly recommend reading Felix Salmon’s article on the subject. <sup id="fnref:article"><a href="#fn:article" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have seen how copula theory can be used to model the correlation structure between random variables, and how it can be used to capture tail dependence. We have also seen how the Gaussian copula, which was widely adopted by banks to price mortgage-backed securities, led to a catastrophic underestimation of risk during the subprime mortgage crisis due to its lack of tail dependence.</p> <p>On a more optimistic note, keep in mind that despite its somewhat tarnished reputation, copula theory remains a very powerful tool that is still widely used today (only with more care!).</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:blog"> <p>If you’re new to copulas, this blog is a good starting point. <a href="https://bggj.is/">Link</a> <a href="#fnref:blog" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:disclaimer"> <p>Of course, this is a very simplified version of the story, and there are many other factors that contributed to the 2008 crisis, starting per usual with human greed. <a href="#fnref:disclaimer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:article"> <p>Salmon, Felix. “Recipe for Disaster: The Formula That Killed Wall Street.” <em>WIRED</em>, (2009). <a href="https://www.wired.com/2009/02/wp-quant/">Link</a> <a href="#fnref:article" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="probability-theory,"/><category term="statistics,"/><category term="extreme-value-theory"/><summary type="html"><![CDATA[TL;DR: Copulas are a powerful tool for modeling the correlation structure between random variables. We propose an intuition-first introduction to copula theory, culminating in a discussion of the role of copulas in the 2008 subprime mortgage crisis.]]></summary></entry><entry><title type="html">The Magic of Embeddings</title><link href="https://gaetanx21.github.io/blog/embeddings/" rel="alternate" type="text/html" title="The Magic of Embeddings"/><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/embeddings</id><content type="html" xml:base="https://gaetanx21.github.io/blog/embeddings/"><![CDATA[\[\newcommand{\R}{\mathbb{R}} \newcommand{\tn}[1]{\textnormal{#1}}\] <p>In this post, I will discuss the magic of embeddings and then move onto the Johnson-Lindenstrauss lemma, which is a fundamental result in linear algebra that illustrates the blessing of dimensionality. I will also give a sketch of the proof of the lemma, which is based on the idea of random projections. Finally, I will briefly mention the LinFormer paper, which proposes a linear time and space complexity self-attention mechanism for transformers based on the JL lemma.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/embeddings/mnist_tsne-480.webp 480w,/assets/img/posts/embeddings/mnist_tsne-800.webp 800w,/assets/img/posts/embeddings/mnist_tsne-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/embeddings/mnist_tsne.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. 3D t-SNE embeddings of MNIST data. <a href="https://towardsdatascience.com/visualizing-bias-in-data-using-embedding-projector-649bc65e7487/">Source</a>. </div> <h2 id="i-motivation">I. Motivation</h2> <p>Embeddings have always fascinated me for (at least) three reasons:</p> <p>1) they compactly store large amounts of information (e.g. LLM token embeddings which essentially encapsulate all the subtleties of human language)</p> <p>2) they have meaningful geometric properties (e.g. the dot product encodes similarity, <code class="language-plaintext highlighter-rouge">queen</code>-<code class="language-plaintext highlighter-rouge">king</code>+<code class="language-plaintext highlighter-rouge">man</code>=<code class="language-plaintext highlighter-rouge">woman</code>, etc.)</p> <p>3) they can accommodate different modalities (e.g. CLIP embeddings which can encode both text and image data)</p> <p>This begs the question:</p> <blockquote> <p>How come dense vector representations work so well?</p> </blockquote> <p><strong>As often in machine learning, behind the magic lurks good old linear algebra</strong>. In the case of embeddings, the <em>blessing of dimensionality</em> is at play. To put it simply, the sheer size of the embedding space allows for a lot of flexibility and expressiveness.</p> <p>In a famous paper<sup id="fnref:superposition"><a href="#fn:superposition" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, researchers at Anthropic study the phenomenon of <em>superposition</em> in large language models. They show that the model can learn to represent multiple concepts in a single embedding, which is a direct consequence of the high dimensionality of the space. In particular, they highlight the fact that although a $d$-dimensional space can only hold $d$ orthogonal vectors, if we allow for quasi-orthogonal vectors, we can fit a much larger number of them, which is in fact exponential in $d$! This is a consequence of the <em>Johnson-Lindenstrauss lemma</em>, which we introduce and prove in the next section.</p> <h2 id="ii-the-johnson-lindenstrauss-lemma">II. The Johnson-Lindenstrauss lemma</h2> <blockquote> <p>The Johnson-Lindenstrauss lemma<sup id="fnref:jl"><a href="#fn:jl" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> (1984) or “JL lemma” states that a set of points in a high-dimensional space can be embedded into a lower-dimensional space while preserving pairwise distances approximately.</p> </blockquote> <p>In other words, the JL lemma guarantees the existence of low-distortion embeddings for any finite set of points in a high-dimensional space. This is particularly useful in machine learning, where we often deal with high-dimensional data and need to reduce its dimensionality for various tasks such as visualization, clustering, or classification.</p> <p>The JL lemma can be formally stated as follows:</p> <p><strong>Lemma (Johnson–Lindenstrauss).</strong><br/> Let $\epsilon \in (0, 1)$, $X$ a set of $N$ points in $\mathbb{R}^n$, and consider an integer $k&gt;\frac{8 \ln(N)}{\epsilon^2}$. Then, there exists a linear map $f:\mathbb{R}^n \to \mathbb{R}^k$ such that for all $u,v \in X$:</p> \[(1-\epsilon) \|u-v\|^2_2 \leq \|f(u)-f(v)\|^2_2 \leq (1+\epsilon) \|u-v\|^2_2\] <p>NB: The bound on $k$ is tight i.e. there exists a set $X$ that needs dimension $\Omega(\frac{\ln(N)}{\epsilon^2})$ to be embedded with distortion $\epsilon$.</p> <p>NB: Interestingly enough, the bound on $k$ is independent of the original dimension $n$! This means that in theory, if we have say $N=10^6$ points living in dimension $n=10^{83}$, we can project them down to $k=\frac{8 \ln(10^6)}{0.1^2} \approx 10^4$ dimensions while preserving pairwise distances with a distortion of $10\%$! <sup id="fnref:catch"><a href="#fn:catch" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <h2 id="iii-proof-of-the-lemma">III. Proof of the lemma</h2> <p>I find the <em>proof</em> of the JL lemma interesting in its own right. It is based on the idea of random projections, which are linear maps that project high-dimensional data onto a lower-dimensional subspace. The key idea is the following: if we randomly choose a projection from the $\R^n$ to $\R^k$, there is a non-zero probability that the projection will preserve the pairwise distances of all the points in $X$ up to a factor of $(1+\epsilon)$. And because this probability is non-zero, it means that such projections must exist!</p> <p>NB: This proof technique is called the “probabilistic method”: we use a probabilistic argument to state a deterministic result. In particular, the lemma does not give us a constructive way to find a working $\R^n \to \R^d$ projection, but rather guarantees that at least one exists.</p> <p>We first present a high-level sketch of the proof, followed by a more rigorous step-by-step derivation.</p> <h4 id="a-sketch-of-the-proof">A. Sketch of the proof</h4> <p>Here I will give a sketch of the proof in two parts:</p> <p>1) I’ll show that if we randomly project a vector $u \in \R^n$ onto a $k$-dimensional subspace $v\in \R^k$ with $k&gt;\frac{8 \ln(N)}{\epsilon^2}$, then we have</p> \[\mathbb{P}(\|v\|^2_2 \in \big[(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2\big]) \geq \frac{2}{N^2}\] <p>2) From this result I will show that if we have $N$ points in $\R^n$, then the probability that all of them are projected into a $k$-dimensional subspace with distortion $\epsilon$ is non-zero, effectively proving the lemma.</p> <h4 id="b-actual-derivation">B. Actual derivation</h4> <p>1) Let $u \in \R^n$ and let $k$ be some integer which we’ll fix later. Consider $P \sim \mathcal{N}(0,1)^{\otimes (k,n)}$ a random projection matrix of from $\R^n$ to $\R^k$ and define $v=\frac{1}{\sqrt{k}}Pu$. For ease of notation, we write $P$ as:</p> \[P = \begin{bmatrix} P_1^T \\ \vdots \\ P_k^T \end{bmatrix}\] <p>It is then clear that $v_i = \frac{1}{\sqrt{k}}P_i^Tu \sim N(0,\frac{|u|^2_2}{k}) \ i.i.d.$ for $i=1,\ldots,k$. As such, we can define $x=\frac{\sqrt{k}}{|u|_2}v$ and we have $x \sim N(0,I_k)$. Consequently, we have:</p> \[\|x\|^2_2 = \frac{1}{k}\frac{\|v\|^2_2}{\|u\|^2_2} \sim \chi^2_k\] <p>From there, we can easily use concentration inequalities on the $\chi^2$ distribution to show that:</p> \[\mathbb{P}(\|v\|^2_2 \in [(1-\epsilon) \|u\|^2_2, (1+\epsilon) \|u\|^2_2]) \geq 2e^{-\frac{k}{4}(\epsilon^2-\epsilon^3)}\] <p>We then fix $k&gt;\frac{8 \ln(N)}{\epsilon^2}$ which gives us the desired $\frac{2}{N^2}$ bound.</p> <p>2) Now, let $X=\lbrace x_1,\ldots, x_N \rbrace$ be a set of $N$ points in $\R^n$. The above result applies to all the vectors $u = x_i - x_j$ for all pairs $1\leq i,j \leq N$. Let $E_{\lbrace i,j\rbrace}$ be the event that the projection of the pair $\lbrace x_i,x_j\rbrace$ violates the distortion bound. There are $N(N-1)/2$ $\lbrace i, j \rbrace$ pairs, such that the probability of having at least one of them violate the distortion bound $\epsilon$ is given by:</p> \[p_\text{invalid projection} = \mathbb{P}(\bigcup_{\{i,j\} \in pairs} E_{\{i,j\}}) \leq \sum_{\{i,j\} \in pairs} \mathbb{P}(E_{\{i,j\}}) \leq \frac{N(N-1)}{2}\frac{2}{N^2} = 1-\frac{1}{N}\] <p>Consequently,</p> \[p_\text{valid projection}=1-p_\text{invalid projection} \geq \frac{1}{N} &gt; 0\] <p>Thus, when sampling a random projection $P$ from $\mathcal{N}(0,1)^{\otimes (k,n)}$, we have a non-zero probability that all the points in $X$ are projected into a $k$-dimensional subspace with distortion $\epsilon$.</p> <p>This proves the lemma.</p> <h2 id="iv-linformer">IV. LinFormer</h2> <p>I won’t delve into the many real-life corollaries of the JL lemma, since essentially all linear dimensionality reduction techniques implicitly rely on it.</p> <p>However, I will mention the LinFormer paper<sup id="fnref:linformer"><a href="#fn:linformer" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> which proposes a <strong>linear time and space complexity self-attention mechanism for transformers</strong>. The key idea is to use a low-rank approximation of the attention matrix<sup id="fnref:spectrum"><a href="#fn:spectrum" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>, which can be achieved using random projections. The JL lemma is used by the authors to provide theoretical guarantees for this approximation: they demonstrate that for a given distortion $\epsilon$, there is a corresponding dimension $k&lt;n$<sup id="fnref:n"><a href="#fn:n" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> which ensures that the rank-$k$ approximation induces an $\epsilon$-bounded distortion!</p> <h2 id="conclusion">Conclusion</h2> <p>While the Johnson-Lindenstrauss lemma is not directly “practical” in itself, I believe it is the kind of linear algebra result that is good to keep in mind. In particular, I think it helps build a better intuition of what happens in high-dimensional spaces, where both the curse <em>and</em> the blessing of dimensionality are at play.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:superposition"> <p>Anthropic (2022). <em>Toy Models of Superposition.</em> <a href="https://www.anthropic.com/news/toy-models-of-superposition">Link</a> <a href="#fnref:superposition" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:jl"> <p>Wikipedia. <em>Johnson-Lindenstrauss lemma.</em> <a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">Link</a> <a href="#fnref:jl" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:catch"> <p>The catch, however, is that finding a projection that works would take a lot of time in practice, since this time would scale with the initial dimension $n$. <a href="#fnref:catch" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:linformer"> <p>Wang, S., et al. (2020). <em>Linformer: Self-Attention with Linear Complexity.</em> <a href="https://arxiv.org/abs/2006.04768">Link</a> <a href="#fnref:linformer" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectrum"> <p>The paper also studies the spectrum of attention matrices and shows that they are low-rank, which is a key insight for the LinFormer approach. Even more interestingly, they show that as we go deeper in the transformer, the attention matrices become more and more low-rank, which is to say the information becomes more and more compressible! <a href="#fnref:spectrum" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:n"> <p>Here, $n$ is the sequence length. <a href="#fnref:n" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="linear-algebra"/><summary type="html"><![CDATA[TL;DR: Embeddings are so powerful that they can seem almost magical. We go back to the basics (linear algebra) with the Johnson-Lindenstrauss lemma, which illustrates the blessing of dimensionality.]]></summary></entry><entry><title type="html">Adding salt to the Bitter Lesson</title><link href="https://gaetanx21.github.io/blog/bitter-lesson/" rel="alternate" type="text/html" title="Adding salt to the Bitter Lesson"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/bitter-lesson</id><content type="html" xml:base="https://gaetanx21.github.io/blog/bitter-lesson/"><![CDATA[<p>In this post, I will briefly discuss Richard Sutton’s <em>Bitter Lesson</em> of AI. I will also present a lesser-known counter-argument by Rodney Brooks, and finally I will add my own grain of salt to the discussion with a focus on the signal-to-noise ratio (SNR) of the problem at hand. I will illustrate this idea with two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <hr/> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" sizes="95vw"/> <img src="/assets/img/posts/bitter_lesson/gpus_go_brrr.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Sutton's Bitter Lesson, illustrated. <a href="https://horace.io/brrr_intro.html">Source</a>. </div> <h2 id="i-richard-suttons-bitter-lesson">I. Richard Sutton’s Bitter Lesson</h2> <p>Sutton’s <em>Bitter Lesson</em><sup id="fnref:sutton"><a href="#fn:sutton" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> begins with the following statement:</p> <blockquote> <p>“The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> </blockquote> <p>Basically, Sutton’s big idea is that trying to forcefully incorporate human knowledge in AI systems (which was essentially the norm before the deep learning revolution) hinders progress. Instead, leveraging vasts amounts of compute (and data) is the way to go. This implies that any attempt to inject human ingenuity into AI systems is doomed to fail, hence the “bitter” lesson. In his lesson, Sutton gives several good examples of this phenomenon, for instance in computer vision, where models using complicated human-designed features (e.g., SIFT) were quickly outperformed by deep learning methods that <em>learned</em> features directly from data.</p> <p>In today’s age of transformer models scaling up to trillions of parameters, Sutton’s lesson seems more relevant than ever, and some veteran NLP researchers have certainly felt bitter seeing their carefully handcrafted models being outperformed by large language models (LLMs) trained on (somewhat random) internet text. Companies building LLMs certainly have reasons to believe in the bitter lesson. Rumor has it that memorizing Sutton’s article is part of OpenAI engineers’ work schedule<sup id="fnref:openai"><a href="#fn:openai" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Funnily enough, OpenAI itself got bitter-lessoned in 2024 when it created a fine-tuned version of its then-flagship <code class="language-plaintext highlighter-rouge">o1</code> model specifically for competitive programming, <code class="language-plaintext highlighter-rouge">o1-ioi</code>, which ended up being uniformly worse than the firm’s next-generation general-purpose model, <code class="language-plaintext highlighter-rouge">o3</code>.</p> <h2 id="ii-rodney-brooks-better-lesson">II. Rodney Brooks’ <em>Better</em> Lesson</h2> <p>A week after Sutton’s post, Rodney Brooks published a blog post titled <em>A Better Lesson</em><sup id="fnref:brooks"><a href="#fn:brooks" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> in which he essentially argues that Sutton is wrong. As he carefully put it:</p> <blockquote> <p>“I think Sutton is wrong for a number of reasons.”</p> </blockquote> <p>Brooks lists a few reasons why he believes Sutton’s lesson is wrong. His core thesis is that AI systems are still imbued with human knowledge, only now it is hidden in the choice of model architectures, and to a lesser extent in the curated datasets and the complex training pipelines. Besides, he argues that the current trend of scaling up models is not sustainable, notably because Moore’s law is slowing down and frontierAI models’ carbon footprint is becoming a cause for concern.</p> <h2 id="iii-my-grain-of-salt-snr-matters">III. My grain of salt: SNR matters</h2> <p>My (humble) view is that Sutton’s Bitter Lesson is generally a good heuristic for AI research, but it should be taken with a grain of salt (!).</p> <blockquote> <p>“I believe that the signal-to-noise ratio (SNR) of the problem at hand matters a lot.”</p> </blockquote> <p>I will illustrate this idea on two specific domains in which human priors have yet to be discarded: quantitative finance and computational biology.</p> <h4 id="a-quantitative-finance">A. Quantitative Finance</h4> <p>Financial markets are notoriously noisy, as they are complex systems in which a myriad of heterogeneous agents interact with different objectives. As such, it’s well-known that the SNR of financial data is extremely low. For that reason, robustness is a key concern in model selection and most market practitioners end up relying on the good ol’ linear regression model, albeit augmented with a few hand-crafted biases. Although the industry is catching up with the latest AI trends (e.g. using LLMs for sentiment analysis), the SNR of financial data is so low that it is hard to imagine a future in which Sutton’s lesson will be fully applicable. In fact, I would argue that <strong>the SNR of financial data is so low that it is not even clear whether Sutton’s lesson applies at all</strong>. Can a 1B-parameter model trained on 1TB of (crappy) data outperform a 10-parameter linear model trained on 1MB of data? I don’t know, but I wouldn’t be surprised if it didn’t!</p> <h4 id="b-computational-biology">B. Computational Biology</h4> <p>Data in computational biology is also very noisy, but for different reasons. Here I will focus on RNA-seq data<sup id="fnref:rnaseq"><a href="#fn:rnaseq" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, which in a nutshell (within a nutshell) is tabular data of the form <code class="language-plaintext highlighter-rouge">n_cells x n_genes</code> where each row gives you for a given cell the expression level of the 20k or so (human) genes. As it stands, RNA-seq data has several issues, the most obvious one being that it is very sparse<sup id="fnref:sparse"><a href="#fn:sparse" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> and hence difficult to work with. More importantly, RNA-seq data is “dirty” in the sense that it is collected in a wet lab by a human being (i.e. not a machine) who has their own way of doing things<sup id="fnref:law-compbio"><a href="#fn:law-compbio" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. This leads to what is called “batch effects”, which are systematic differences between data collected in different experiments. In the context of NLP, this is like if I told you that the text data scraped on Wikipedia didn’t follow the same distribution as the text data scraped on Reddit. That would certainly make matters difficult, right?</p> <p>But there is a much deeper problem with RNA-seq data, which is that <strong>gene expression fundamentally isn’t a clean signal</strong>, unlike text data in (curated) web corpuses. The key idea is that life as we know it is literally the result of a random process left unchecked for 4 billion years, in which the fittest pass their genes to the next generation. This explains why organisms are so monstrously complex (unlike computer systems, which are trivial in comparison), but also extremely robust. A good example of this is the notion of <em>biological pathways</em>, which can roughly be described as “a series of interactions of molecules in a cell that leads to a certain product or a change in the cell”<sup id="fnref:pathway"><a href="#fn:pathway" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. In computer systems, pathways are bijective: Function A triggers Function B, and that’s it. In an organism, Gene A may trigger production of Protein B, but it may also trigger production of Protein C. And guess what, Gene C can also create Protein B under certain conditions. Oh and wait, the goal of creating Protein B was to produce a certain molecule, but it turns out that this molecule can also be produced by Gene D! And so on and so forth. In other words, biological pathways are not bijective, and this is a super important because <strong>redundancy yields robustness</strong>. For instance, if one pathway producing glucose in a cell breaks down for some reason, the cell can still produce glucose through other pathways that were created through random mutations, so it doesn’t die! The most critical components of life, such as the immune system, have myriads of redundant pathways, which makes them extremely robust to perturbations. As such, the current attempts<sup id="fnref:goldrush"><a href="#fn:goldrush" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> to emulate the dazzling successes of transformer models in NLP by training large transformer architectures on RNA-seq data may ultimately prove futile, as the SNR of the data may simply be too low.</p> <h2 id="conclusion">Conclusion</h2> <p>The Bitter Lesson is a great heuristic for AI research, but it must be taken with a grain of salt. In particular, the SNR of the problem at hand matters a lot. In domains such as quantitative finance and computational biology, the SNR is so low that it is not even clear whether Sutton’s lesson applies at all. In these domains, human biases and ingenuity are still critical to building effective AI systems.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:sutton"> <p>Sutton, R. (2019). <em>The Bitter Lesson.</em> <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">Link</a> <a href="#fnref:sutton" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:openai"> <p>Medium (2024). <em>The Legendary OpenAI Engineer’s Must-Have Classic: A Bitter Lesson.</em> <a href="https://ai-engineering-trend.medium.com/the-legendary-openai-engineers-must-have-classic-a-bitter-lesson-1948e6ac6c4a">Link</a> <a href="#fnref:openai" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:brooks"> <p>Brooks, R. (2019). <em>The Better Lesson.</em> <a href="https://rodneybrooks.com/a-better-lesson/">Link</a> <a href="#fnref:brooks" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rnaseq"> <p>Wikipedia. (2023). <em>RNA-Seq.</em> <a href="https://en.wikipedia.org/wiki/RNA-Seq">Link</a> <a href="#fnref:rnaseq" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sparse"> <p>Not only because most genes are not expressed in most cells, but also because genes with low expressions may not be captured during RNA sequencing. <a href="#fnref:sparse" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:law-compbio"> <p>A colleague of mine with deep expertise in the field quickly taught me that “the first rule of computational biology is that everyone does things their own way”. <a href="#fnref:law-compbio" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:pathway"> <p>Wikipedia. (2023). <em>Biological pathway.</em> <a href="https://en.wikipedia.org/wiki/Biological_pathway">Link</a> <a href="#fnref:pathway" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:goldrush"> <p>Given the record amounts invested, one might even call it a <em>gold rush</em>. <a href="#fnref:goldrush" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="meta,"/><category term="learning"/><summary type="html"><![CDATA[TL;DR: The "Bitter Lesson" of AI states that general methods that leverage computation are ultimately the most effective to build powerful AI systems. We propose to qualify this lesson by introducing the notion of signal-to-noise ratio (SNR) of the problem at hand. In domains such as quantitative finance and computational biology, I believe that the SNR is so low that Sutton's lesson may not directly apply.]]></summary></entry><entry><title type="html">The Curty &amp;amp; Marsili Forecasting Game</title><link href="https://gaetanx21.github.io/blog/curty-marsili-game/" rel="alternate" type="text/html" title="The Curty &amp;amp; Marsili Forecasting Game"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/curty-marsili-game</id><content type="html" xml:base="https://gaetanx21.github.io/blog/curty-marsili-game/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>In this post, we present Curty &amp; Marsili’s forecasting game, a simple model that captures how herding behavior can lead to non-trivial opinion outcomes, in particular <strong>phase coexistence</strong> and <strong>ergodicity breaking</strong> under certain conditions. After motivating the study of herding, we formally introduce the Curty &amp; Marsili game and propose a mathematical analysis of its key features. We then perform <strong>Agent-Based Model</strong> (ABM) simulations of the game to validate our theoretical predictions. Finally, we discuss how the game can converge to a Nash equilibrium where fundamentalists and herders coexist and the system is efficient.</p> <hr/> <h2 id="i-motivation">I. Motivation</h2> <p>Herding is a widespread phenomenon in society: people often imitate or follow the actions of others, whether it’s in fashion trends, product adoption, or even protests. In finance, herding can lead to anomalous fluctuations in asset prices. The alternative to herding is to gather (private) information and make decisions based on one’s own analysis. Curty &amp; Marsili’s forecasting game<sup id="fnref:curtymarsili"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a simple ABM which precisely focuses on this tension between <strong>individual forecasting</strong> and <strong>collective herding</strong>.</p> <p>In a nutshell, the question Curty &amp; Marsili tried to answer is: <strong>what’s more efficient, following the crowd or relying on your own judgment?</strong> We’ll see that the answer is not straightforward and can depend on the parameters of the game. In essence, we’ll find that herding can be a sound strategy, but if the proportion of followers in the market becomes too large, herding becomes a dangerous strategy.</p> <p><em>Let’s now present in details of the Curty &amp; Marsili forecasting game.</em></p> <h2 id="ii-the-game">II. The Game</h2> <p>The Curty &amp; Marsili requires the following ingredients:</p> <ul> <li>$N\gg 1$ agents who must make a binary forecast (e.g., election outcome, buy or sell, protest or not, etc.)</li> <li>A fraction $z\in[0,1]$ of agents are <strong>fundamentalists</strong> $F_i$ who rely solely on their private information. They are correct with probability $p&gt;\frac{1}{2}$ (i.e., they have an edge). Their forecast is fixed once and for all and crucially, fundamentalists’ forecasts are mutually independent.</li> <li>The remaining fraction $1-z$ are <strong>herders</strong> $H_i$ who each have a fixed group $G_i$ of $M$ agents which they follow. Their action is determined by majority voting within their group (note that group size $M$ is odd to avoid draws). Importantly, note that groups may include both fundamentalists and herders.</li> </ul> <p>The game then dynamically evolve according to the following rules:</p> <ul> <li>At each time step $t$, all herders are chosen one by one (in a random order) and update their forecast based on the majority opinion of their group $G_i$. (note that herders’ initial forecast are i.i.d. random i.e. correct with probability $\frac{1}{2}$)</li> <li>The fundamentalists $F_i$ do not change their forecast over time. (reflecting their reliance on private information)</li> <li>The process is repeated until convergence to a fixed point (i.e. herders are all following the majority opinion of their group).</li> </ul> <p>The question then is to study the final probability $q$ that a herder makes the correct forecast, computed as the fraction of herders who forecast the correct outcome after the game has converged. More precisely, we want to study $q(z)$, the final probability of a herder making the correct forecast as a function of the fraction of fundamentalists $z$ in the market.</p> <p><em>We now delve into the mathematical analysis of the game.</em></p> <h2 id="iii-mathematical-analysis">III. Mathematical Analysis</h2> <p>Let’s introduce two important notations:</p> <ul> <li>$q_t$ is the probability that a herder makes the correct forecast at time $t$.</li> <li>$\pi_t$ is the probability that a randomly chosen agent makes the correct forecast at time $t$.</li> </ul> <p>Since agents are either fundamentalists or herders, we have the following static equation: \(\begin{equation} \label{eq:static} \pi_t = zp + (1-z)q_t. \end{equation}\)</p> <p>In addition, a herder will make the correct forecast at time $t+1$ if and only if the majority of his group $G_i$ makes the correct forecast at time $t$, i.e. at least $\frac{M+1}{2}$ agents in the group make the correct forecast. This leads to the following dynamic equation: \(\begin{equation} \label{eq:dynamic} q_{t+1} = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} \pi_t^k (1-\pi_t)^{M-k}. \end{equation}\)</p> <p>Combining \eqref{eq:static} and \eqref{eq:dynamic}, we can write the evolution of $q_t$ as: \(\begin{equation} \label{eq:evolution} q_{t+1} = F_z(q_t). \end{equation}\)</p> <p>where $F_z(q) = \sum_{k=\frac{M+1}{2}}^M \binom{M}{k} (zp + (1-z)q)^k (1-(zp+(1-z)q))^{M-k}$.<sup id="fnref:condorcet"><a href="#fn:condorcet" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>We can then compute fixed points $q^*(z)$ for the evolution equation \eqref{eq:evolution} for various values of $z\in[0,1]$. The results are illustrated in <a href="#fig-1">Figure 1</a>.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/fixed_points-480.webp 480w,/assets/img/posts/curty_marsili_game/fixed_points-800.webp 800w,/assets/img/posts/curty_marsili_game/fixed_points-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/fixed_points.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Fixed points of the evolution equation $q_{t+1} = F_z(q_t)$ for various values of $z$. Note the critical value $z_c\simeq \frac{1}{2}$ separating the two regimes. Each curve corresponds to a different fixed point: green for $q_+^*(z)$, blue for $q_-^*(z)$, and red for $q_u^*(z)$ the unstable fixed point. </div> <p>We distinguish two regimes, separated by a critical value $z _ c\simeq \frac{1}{2}$:</p> <ul> <li>for $z&gt;z _ c$ i.e. when there are mostly fundamentalists, there is a single (stable) fixed point $q _ +^ * (z)$. Interestingly, $q^ * (z)&gt;p$ in this regime, meaning that herders are on average more accurate than fundamentalists. As $z$ decreases (while staying above $z_c$), the performance of herders gets even better! This can seem somewhat surprising, but in fact results from the fact that more herders means herders will have more herders in their group $G_i$, which in turn increases their forecast accuracy since in this regime herders are more accurate than fundamentalists.</li> <li>for $z&lt;z _ c$ i.e. when there are mostly herders, two new fixed points appear, both under the line $q=\frac{1}{2}$ which means that these fixed points are bad for herders. Note that $q _ +^* (z)$ keeps increasing as $z$ decreases, while $q _ -^* (z)$ decreases. The unstable fixed point $q _ u^* (z)$ is also shown in red. Numerical simulations show that the system will converge to either $q _ +^* (z)$ or $q _ -^* (z)$ depending on the initial conditions. In fact, the unstable fixed point $q _ u^* (z)$ acts as a <em>separatrix</em> between the two regimes. Thus the initial condition $q_0$ will determine the final state of the system: if $q _ 0&gt;q _ u^* (z)$, the system will converge to $q _ +^* (z)$, otherwise it will converge to $q _ -^* (z)$. This will be useful in the last section where we compare $\langle q \rangle$ to $p$ to find the Nash equilibrium.</li> </ul> <p>What’s interesting is the <strong>phase coexistence</strong> in herding regime $z&lt;z_ c$: if the system converges towards $q_ -^<em>(z)$, then the majority of herders will make the wrong forecast; likewise, if the system converges towards $q_ +^</em>(z)$, the majority of herders will make the correct forecast. This is a clear example of <strong>ergodicity breaking</strong> where the system is stuck in one of the two phases, depending on the initial conditions $q_0$. In the last section we take into account the distribution $q_0\sim N(\frac{N}{2},\frac{N}{4})$ to compute the probability $p_ -$ of the system converging to $q_-$ (and similarly $p_+$ for $q_+$) so we can finally compute the average probability $\langle q \rangle$ of a herder making the correct forecast and compare it to fundamentalists’ accuracy $p$.</p> <p><em>Now that we’ve analyzed the theoretical aspects of the game, let’s move on to simulations to check if its consistent.</em></p> <h2 id="iv-abm-simulation">IV. ABM Simulation</h2> <p>The ABM is pretty straightforward here, with two agents classes (fundamentalists and herders) and at each step of the system, herders are picked one by one in a random order and update their forecast based on the majority opinion of their group. We iterate until convergence of the system i.e. herders’ opinions are stable. We use the <code class="language-plaintext highlighter-rouge">mesa</code> python library<sup id="fnref:mesa"><a href="#fn:mesa" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> which helps build ABMs very easily.</p> <p>Throughout the simulations, we use the following parameters:</p> <ul> <li>$N=1000$ agents in total, with $z\in[0,1]$ the fraction of fundamentalists.</li> <li>$p=0.52$ the probability that a fundamentalist makes the correct forecast.</li> <li>$M=7$ the size of the groups of herders. Note that modifying these parameters will result in quantitative but not qualitative changes in the outcomes.</li> </ul> <p>We run simulations for various values of $z\in[0,1]$ and for each simulation we record $q_t$ at each time step $t$ of the system. We especially care about the final probability $q_\text{final}$ that a herder makes the correct forecast, which is simply the fraction of herders who forecast the correct outcome after the game has converged. We observe the following:</p> <ul> <li>in the low-herding regime $z&gt;z_c$, $q_\text{final}$ is always above $p$ and very close to $q_+$.</li> <li>in the high-herding regime $z&lt;z_c$, $q_\text{final}$ is either close to $q_-$ or $q_+$ depending on the initial conditions. This is consistent with the phase coexistence and ergodicity breaking observed in the theoretical analysis.</li> </ul> <p>In <a href="#fig-2">Figure 2</a>, we plot $q_t$ over time for $n=100$ simulations for various values of $z$. We observe that the system converges to a fixed point after a few time steps, and the final probability $q_\text{final}$ is consistent with the theoretical predictions. As expected, we find that:</p> <ul> <li>in the high-herding regime $z&lt;z_c$, the system can converge to either $q_-$ or $q_+$ depending on the initial conditions, and we have $q_- \simeq 0$ and $q_+ \simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>.</li> <li>in the low-herding regime $z&gt;z_c$, the system converges but toward a wider spectrum of values, which are above $p$ on average i.e. $\langle q_\text{final} \rangle &gt; p$.</li> </ul> <p>We see in particular that the richness of the phase transition towards $z\simeq z_c$ is well captured by the ABM simulations.</p> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/runs-480.webp 480w,/assets/img/posts/curty_marsili_game/runs-800.webp 800w,/assets/img/posts/curty_marsili_game/runs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/runs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Evolution of $q_t$ over time for $n=100$ simulations for various values of $z$. The system converges to a fixed point after a few time steps. The ensemble final probability $q_\text{final}$ is indicated by the y-tick on the right. </div> <p>We notice that $q_\text{final}(z)$ seems to increase with $z&lt;z_c$ then decrease with $z&gt;z_c$. To investigate this behavior, we plot the average final probability $\langle q_\text{final} \rangle$ over $n=100$ simulations for various values of $z$. The results are given in In <a href="#fig-3">Figure 3</a>. Interestingly, we see that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$, except near the $z=0$. This implies that herding is always a better strategy than being a fundamentalist… <strong>except when there are too many herders in the system!</strong></p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/curty_marsili_game/q_z-480.webp 480w,/assets/img/posts/curty_marsili_game/q_z-800.webp 800w,/assets/img/posts/curty_marsili_game/q_z-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/curty_marsili_game/q_z.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Average final probability $\langle q_\text{final} \rangle$ over $n=1000$ simulations for various values of $z$. Note that $\langle q_\text{final} \rangle &gt; p$ for all values of $z$ except near $z=0$, meaning that herding is a better strategy than being a fundamentalist except when virtually everyone adopts the herding strategy! </div> <p><em>Let’s finish by showing how letting $z$ fluctuate naturally leads to a Nash equilibrium.</em></p> <h2 id="v-nash-equilibrium">V. Nash Equilibrium</h2> <p>We have seen that if there aren’t too many herders (i.e. if $z$ isn’t too low), then $\langle q_\text{final}(z) \rangle &gt; p$, i.e. herders are more accurate than fundamentalists on average. In this case, it is rational for fundamentalist agents to become herders, which means that $z$ will decrease. However there cannot be too many herders, since in the limit $z\to 0$ we have $q_\text{final}=\frac{1}{2}$ as all agents are herders and thus there is not information (“edge”) in the system. We thus expect the system to self-organize until the proportion $z$ of fundamentalists fluctuates around a critical value $z^\dagger$ such that $\langle q_\text{final}(z^\dagger) \rangle = p$. This is the Nash equilibrium of the system, where fundamentalists and herders coexist and the system is efficient. (or arbitrage-free in the context of financial markets)</p> <p>We can show <sup id="fnref:curtymarsili:1"><a href="#fn:curtymarsili" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> that the Nash equilibrium $z^\dagger$ is given by $z^\dagger \sim N^{1/2}$ where $N$ is the total number of agents. This means that most agents are followers and there is a little minority of $\sqrt{N}$ fundamentalists feeding information (“edge”) to the system.</p> <p>Additionally, note that since $z^\dagger$ is very small, we have $q_-\simeq 0$ and $q_+\simeq 1$ as illustrated in <a href="#fig-1">Figure 1</a>. Then, if we denote $p_-$ (resp $p_+$) the probability that the system converges to $q_-$ (resp $q_+$) given the initial conditions, we have $\langle q_\text{final} \rangle = p_- q_- + p_+ q_+$, which at the Nash equilibrium rewrites $p=p_+$, meaning that the probability of the herder mass to converge to the truth ($q_+$) is $p$, as if they represented a single fundamentalist agent!</p> <h2 id="conclusion">Conclusion</h2> <p>Despite its simplicity, the Curty &amp; Marsili game suffices to display non-trivial behavior such as phase coexistence and ergodicity breaking. The game is a good illustration of how herding can be a good strategy… until too many agents adopt it and the whole herding population starts behaving like a single agent which is correct with probability $\langle q_\text{final} \rangle$. Finally, if we let agents switch strategy, $z$ will naturally converge to the efficient state $z^\dagger$ where $\langle q_\text{final} \rangle = p$ such that no strategy has an edge over the other. We find that $z^\dagger \sim N^{1/2}$, meaning that <strong>it is optimal (in a game-theoretic sense) that most agents are followers and a little minority of fundamentalists is feeding information to the system</strong>.</p> <hr/> <p><strong>References</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:curtymarsili"> <p><em>Phase coexistence in a forecasting game.</em> Curty, P. &amp; Marsili, M. (2008) <a href="https://wrap.warwick.ac.uk/id/eprint/1769/1/WRAP_Curty_fwp05-15.pdf">PDF</a> <a href="#fnref:curtymarsili" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:curtymarsili:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:condorcet"> <p>Note the similarity with the <strong>Condorcet Jury Theorem</strong>, where the probability of a correct decision by a majority vote increases with the number of jurors and their individual accuracy. <a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem">wikipedia</a> <a href="#fnref:condorcet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mesa"> <p><em>Mesa: An Agent-Based Modeling Framework in Python.</em> <a href="https://mesa.readthedocs.io/">mesa.readthedocs.io</a> <a href="#fnref:mesa" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="agent-based-model,"/><category term="game-theory"/><summary type="html"><![CDATA[TL;DR: When faced with a forecasting task, one can either seek information or follow the crowd. The Curty & Marsili game stacks fundamentalists against herders in a binary forecasting task, revealing phase coexistence and ergodicity breaking under certain conditions. We propose a theoretical study of the game's behavior and validate it through ABM simulations.]]></summary></entry><entry><title type="html">Listening to the Market Mode</title><link href="https://gaetanx21.github.io/blog/market-mode/" rel="alternate" type="text/html" title="Listening to the Market Mode"/><published>2025-02-12T00:00:00+00:00</published><updated>2025-02-12T00:00:00+00:00</updated><id>https://gaetanx21.github.io/blog/market-mode</id><content type="html" xml:base="https://gaetanx21.github.io/blog/market-mode/"><![CDATA[\[\newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}}\] <p>We first motivate the use of Principal Component Analysis (PCA) on returns to extract the market mode in equities. This mode is crucial for understanding the market risk and comparing it against other risks. We then do a (very) quick recap on Random Matrix Theory (RMT), which provides a theoretical framework for understanding the eigenspectrum of random matrices. Finally, we apply PCA on S&amp;P 500 components to extract the market mode and we monitor its evolution over time, drawing a comparison with the VIX index.</p> <h2 id="motivation">Motivation</h2> <p>Among the various asset classes (e.g., equities, bonds, commodities), equities tend to provide the highest returns in absolute terms (i.e. not adjusted for risk). Equities are exposed to a multitude of risk factors, with <strong>market risk</strong> being the dominant one<sup id="fnref:CAPM"><a href="#fn:CAPM" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. As such, understanding the market risk and how much of the variance it explains is crucial for risk management and portfolio construction.</p> <p>In a nutshell, the question we want to answer is the following: <strong>can we measure the market risk and compare its weight against other risks?</strong></p> <p>Perhaps the simplest approach to gauge market risk is to look at ready-made proxies such as the <strong>VIX index</strong><sup id="fnref:VIX"><a href="#fn:VIX" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. However, the method of computing the VIX is debatable and may not capture the market risk accurately. A more data-driven approach is to extract the market mode from market returns using PCA, as explained in the next section.</p> <h2 id="pca-on-returns--statistical-factor-model">PCA on Returns = Statistical Factor Model</h2> <p>In a nutshell, a <strong>factor model</strong> describes the variance observed in a set of correlated variables (in our case, stock returns) using a smaller number of <strong>unobserved factors</strong>, which we hope to be more or less independent &amp; more or less interpretable. The idea is to decompose the observed variables $X_i$ as linear combinations of the factors $F_k$ plus some idiosyncratic noise $\varepsilon_i$:</p> \[X_i = \sum_k \beta_k^{(i)} F_k + \varepsilon_i\] <p>where $\beta_k^{(i)}$ is the (factor) loading of the $i$-th asset on the $k$-th factor.</p> <p>Now, the hard part is to find <strong>good factors</strong>. One approach is to simply purchase them from vendors like MSCI (Barra models) who gather a lot of data and knowledge to build these factors. Another (cheaper &amp; more transparent) approach is to extract them via PCA. In this case, the factors are obtained as the eigenvectors of the correlation matrix of returns. This is nice for several reasons:</p> <ul> <li>eigenvectors are orthogonal<sup id="fnref:spectral"><a href="#fn:spectral" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, meaning our factors are uncorrelated;</li> <li>eigenvalues directly give us the amount of variance explained by each factor;</li> <li>the factors are interpretable as they are linear combinations of the original variables.</li> </ul> <p>Note that since the factors are obtained in a purely data-driven fashion (without any human/economic prior), we call this approach a <strong>statistical</strong> factor model, as opposed to classical factor models like the CAPM or the Fama-French Three-Factor Model.</p> <p><em>Before trying out PCA on real data, let’s quickly brush up on Random Matrix Theory (RMT), which provides a neat theoretical framework for understanding the eigenspectrum of correlation matrices.</em></p> <h2 id="brush-up-on-random-matrix-theory-rmt">Brush Up on Random Matrix Theory (RMT)</h2> <p>Consider a $T \times N$ matrix $X$ filled with i.i.d. Gaussian entries $X_{ij} \sim \N(0,\sigma^2)$. Typically, $X$ is called the <strong>design matrix</strong> and each one of the $T$ rows corresponds to one observation of the $N$ variables of interest. In our case, the variables are the daily close-to-close returns of the $N=500$ stocks in the S&amp;P 500 index.</p> <p>If we want to study the correlation between the variables, we first compute a standardized version of the design matrix $\tilde{X}$ by subtracting the mean and dividing by the standard deviation for each column. The sample correlation matrix is then given by $C = \frac{1}{T} \tilde{X}^T \tilde{X}$.</p> <p>Finally, $C$ is real symmetric so we know from the spectral theorem that it can be diagonalized in an orthonormal basis of eigenvectors. In fact $C$ is also positive semi-definite, so all its eigenvalues are non-negative.</p> <h4 id="marchenko-pastur-theorem">Marchenko-Pastur Theorem</h4> <p>RMT studies the behavior of the eigenvalues of $C$ when $T,N \to \infty$ with $Q = T/N$ fixed. The main result is the <strong>Marchenko-Pastur (MP) theorem</strong>:</p> \[L_N(\lambda) = \frac{1}{N} \sum_{i=1}^N \delta(\lambda - \lambda_i) \xrightarrow[N,T \to \infty]{\mathcal{W}} \mathbb{P}_\text{MP}(\lambda) = \frac{Q}{2\pi \sigma^2} \frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda} \mathbb{1}_{[\lambda_-, \lambda_+]}(\lambda)\] <p>where $\lambda_{\pm} = \sigma^2(1\pm\sqrt{\frac{1}{Q}})^2$ are the limiting bounds of the spectrum.</p> <p>The MP theorem tells us that the empirical spectral density of the correlation matrix $L _ N(\lambda)$ converges (weakly in distribution) toward the MP distribution $\mathbb{P} _ \text{MP}$ as $T,N \to \infty$.</p> <p>This is quite remarkable: we could have expected the eigenvalues to be unbounded as $T,N \to \infty$, but RMT tells us that they are actually bounded and gives us the exact form of the limiting distribution.</p> <p>In fact, we can relax some hypotheses and the MP theorem will still hold, though convergence may be (significantly) slower. For example, the entries of $X$ don’t have to be Gaussian. This is important in our case because we know that Gaussianity is a strong assumption for financial data. In practice returns have fat tails and are often skewed. A Student-distribution is already a much better model for returns. <em>Good news, MP still holds for Student-distributed entries!</em></p> <h4 id="link-with-pca">Link with PCA</h4> <p>Now, what does this have to do with PCA? Well, the MP theorem tells us that the eigenvalues of the correlation matrix are bounded and distributed according to a known law. This is useful because it allows us to detect the presence of <strong>signal</strong> in the data. If the eigenvalues are significantly larger than the MP bounds, then we can say that the data contains some structure that is not due to randomness. On the contrary, eigenvalues inside the “noise band” defined by the MP law are considered to be due to randomness. Thus, <strong>we can use the MP theorem to filter out noise and extract only the significant factors from the data</strong>. In particular, when doing PCA on market returns, one eigenvalue will stand out from all the others: it represents the dominant variance component due to the market, and as such we call it the <strong>market mode</strong>. It is approximately equally distributed between all the stocks and is the most important factor in the data.</p> <p>In this last section, we illustrate the above ideas through an experiment on US equities. Specifically, we compute rolling PCAs on the correlation matrix of S&amp;P 500 components and analyze the behavior of the market mode over time.</p> <h2 id="experiment-on-us-equities">Experiment on US Equities</h2> <p>To run our experiment, we first need some data. We chose US equities because they are the most liquid and it’s easy to get clean data. We fetched the daily close-to-close returns of the S&amp;P 500 components from Yahoo Finance using the <code class="language-plaintext highlighter-rouge">yfinance</code> Python package. We considered the period 2000–2025<sup id="fnref:sp500"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <h4 id="pca-on-2020-2024-returns">PCA on 2020-2024 returns</h4> <p>Let’s begin by computing the correlation matrix of the daily returns of the S&amp;P 500 components for the period 2020–2024. We then compute the eigenvalues of the correlation matrix and plot them against the MP bounds. The results are shown in <a href="#fig-1">Figure 1</a>. Importantly, <u>the largest eigenvalues are outside the plot</u> for better visibility. There is only ~10 of them, but they are much larger than the rest. <strong>In particular, the first principal component stands out from all the others: it represents the market mode.</strong> The other significant eigenvalues are due to sectoral correlations, which are also interesting to study but outside the scope of this post.</p> <div class="row justify-content-center" id="fig-1"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_true-480.webp 480w,/assets/img/posts/market-mode/mp_true-800.webp 800w,/assets/img/posts/market-mode/mp_true-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_true.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024. The largest eigenvalues are outside the plot. </div> <div class="row justify-content-center" id="fig-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/dist_eigvec_mode-480.webp 480w,/assets/img/posts/market-mode/dist_eigvec_mode-800.webp 800w,/assets/img/posts/market-mode/dist_eigvec_mode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/dist_eigvec_mode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Distribution of the eigenvector of the market mode as well as a noisy mode for the period 2020–2024. Note how the market mode is approximately equally distributed between all the stocks whereas the noisy mode follows a normal distribution, which makes sense because it has no information and thus must maximize entropy. </div> <p>As an extra step, we can shuffle the returns to destroy the correlation structure and then recompute the eigenvalues. The results are shown in <a href="#fig-3">Figure 3</a>. Notice how all the eigenvalues are now neatly inside the MP bounds, which confirms that the structure in the data is due to correlations and not randomness.</p> <div class="row justify-content-center" id="fig-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/mp_shuffled-480.webp 480w,/assets/img/posts/market-mode/mp_shuffled-800.webp 800w,/assets/img/posts/market-mode/mp_shuffled-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/mp_shuffled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Eigenvalues of the correlation matrix of S&amp;P 500 components for the period 2020–2024 after shuffling the returns. All the eigenvalues are now inside the MP bounds and indeed follow the MP distribution. </div> <h4 id="rolling-pca-on-2000-2024-returns">Rolling PCA on 2000-2024 returns</h4> <p>Now that we’ve seen how PCA works on a single sample of market returns, let’s apply it to a rolling window of the daily returns of the S&amp;P 500 components for a large period of time. The idea is to monitor the evolution of the ratio $\lambda_\text{max} / \sum_i \lambda_i$ over time, where $\lambda_\text{max}$ is the largest eigenvalue (market mode) and $\lambda_i$ are the other eigenvalues. This ratio gives us an idea of how much of the variance is explained by the market mode. Intuitively, we expect this ratio to be high during times of uncertainty / fear / crisis as the market becomes even more important in driving returns. To check this hypothesis, we compare the ratio to the VIX index.</p> <p>We’ll be looking at the period 2000–2024, which includes the 2008 financial crisis and the 2020 COVID-19 pandemic. We will take 6-month rolling windows with a 1-month step size and compute the ratio $\lambda_\text{max} / \sum_i \lambda_i$ for each window. Note that for technical reasons<sup id="fnref:sp500:1"><a href="#fn:sp500" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, we only consider the top 420 companies in the S&amp;P 500 index (ranked by daily trading volume) instead of the full 500. However taking the top 420 companies or top 500 companies doesn’t change the results significantly<sup id="fnref:proof"><a href="#fn:proof" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.</p> <p>The results are shown in <a href="#fig-4">Figure 4</a>. The ratio $\lambda_\text{max} / \sum_i \lambda_i$ is plotted in green and the VIX index is plotted in red. We can see that the two series are quite correlated, which confirms our intuition. In particular, we see that the ratio spikes during the 2008 financial crisis, and the 2020 COVID-19 pandemic. This is a nice result as it shows that PCA can indeed capture the market mode and that it is a good proxy for market risk.</p> <div class="row justify-content-center" id="fig-4"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/market-mode/TOP420-480.webp 480w,/assets/img/posts/market-mode/TOP420-800.webp 800w,/assets/img/posts/market-mode/TOP420-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/market-mode/TOP420.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Ratio $\lambda_\text{max} / \sum_i \lambda_i$ (green) and VIX index (red) over the period 2000–2024. The two series are quite correlated, which confirms our intuition that the market mode is a good proxy for market risk. </div> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen how PCA can be used to extract the market mode from equities’ return data. This mode is crucial for understanding the market risk and weighing it against other risks. We’ve also seen how Random Matrix Theory provides a theoretical framework for understanding the eigenspectrum of correlation matrices. Finally, we’ve applied PCA on S&amp;P 500 individual returns to extract the market mode and we’ve analyzed its behavior over time, drawing a comparison with the VIX index.</p> <p>Note that the market mode is not the only factor that matters. If we stick to the PCA approach (statistical factor model), there are several other eigenvalues outside the MP noise band. These eigenvalues and the corresponding eigenvectors correspond to sectors of the US economy (e.g., tech, finance, utilities). They too are risk factors, albeit less important than the market mode. In practice, depending on the context, one may want to consider these factors, for instance to build a sector-neutral portfolio.</p> <hr/> <p><strong>Notes</strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:CAPM"> <p>The Capital Asset Pricing Model (CAPM) is the most simple factor model as it relies on the market factor only. In a nutshell, it posits that the expected return of an asset is linearly related to the expected return of the market depending on the asset’s correlation with the market, known as the beta coefficient. <a href="#fnref:CAPM" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:VIX"> <p>The VIX index is a measure of the market’s expectation of volatility over the next 30 days. It is calculated using the implied volatility of S&amp;P 500 options and is often referred to as the “fear gauge” as it tends to spike during market downturns. <a href="#fnref:VIX" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:spectral"> <p>The eigenvectors of a real symmetric matrix are orthogonal. This is a consequence of the spectral theorem, which states that a real symmetric matrix can be diagonalized by an orthonormal basis of eigenvectors. <a href="#fnref:spectral" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:sp500"> <p>Note that the composition of the S&amp;P 500 index changes over time as companies are added or removed. We use the current components of the index for each year. Sometimes too many components change and this causes problems. One simple solution is to only consider the top 420 companies (instead of 500), which are more stable. (NB: we rank the companies by daily trading volume.) <a href="#fnref:sp500" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:sp500:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:proof"> <p>One way to confirm this intuition is to re-run the experiment but looking at the top 100 companies instead of the top 420. The results are very similar, which shows that the market mode is indeed robust to the number of companies considered (as long as it’s not too small of course). <a href="#fnref:proof" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="random-matrix-theory,"/><category term="linear-algebra,"/><category term="quant-finance"/><summary type="html"><![CDATA[TL;DR: Performing PCA on returns amounts to constructing a statistical factor model. The largest eigenvalue corresponds to the market mode and far outweighs the other factors. Thus, one can perform rolling PCA on equities' returns to monitor the market risk over time.]]></summary></entry></feed>